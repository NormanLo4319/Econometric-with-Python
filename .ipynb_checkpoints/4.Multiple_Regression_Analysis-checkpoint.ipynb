{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "protective-vertex",
   "metadata": {},
   "source": [
    "## Multiple Regression Analysis\n",
    "\n",
    "Running a multiple regression in *Python* is as straightforward as running a simple regression using the **ols()** command in **statsmodels**. In the following example, we show how it is done in coding. In the later section, we will open the black box and replicaes the main calculations using matrix algebra. This is not required for the remaining notebook, so it can be skipped by readers who prefer to keep black boxes closed.\n",
    "\n",
    "We will also discuss the interpretation of regression results and the prevalent omitted variable problems. Finally, we will cover standard errors and multicollinearity for multiple regression by the end of this notebook.\n",
    "\n",
    "**Topics:**\n",
    "1. Multiple Regression in Practice\n",
    "2. OLS in Matrix Form\n",
    "3. Ceteris Paribus Interpretation and Omitted Variable Bias\n",
    "4. Standard Errors, Multicollinearity, and VIF\n",
    "\n",
    "### 1. Multiple Regression in Practice\n",
    "\n",
    "Consider the population regression model\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + ... + \\beta_k x_k + u$$\n",
    "\n",
    "and suppose the data set **sample** contains variables **y, x1, x2, x3**, with respective data of our sample. We estimate the model parameters by OLS using the commands\n",
    "\n",
    "``` Python\n",
    "reg = smf.ols(formula = 'y ~ x1 + x2 + x3', data = sample)\n",
    "results = reg.fit()\n",
    "```\n",
    "\n",
    "The tilde \"$\\tilde$\" again separates the dependent variable from the regressors which are now separated using \"**+**\" sign. We can add options as before. The constant is again automatically added unless it is explicitly suppressed using **'y ~ x1 + x2 + x3 + ...'**.\n",
    "\n",
    "WE already familar with the working of **smf.ols()** and **fit()**: The first command creates an object which contains all relevant information and the estimation is performed in a second step. The estimation results are stored in a variable **results** using the code **results = reg.fit()**. We can use this variable for further analyses. For a typical regression output including a coefficent table, call **results.summary()** in one step. Further analyses involving residuals, fitted values and the like can be used exactly as presented in the previous notebook.\n",
    "\n",
    "The output of **summary()** includes parameter estimates, standard errors according to Theorem 3.2 of Wooldgridge (2019), the coefficient of determination $R^2$, and many more useful results we cannot interpret yet before we have worked through the next notebook file.\n",
    "\n",
    "#### Wooldridge, Example 3.1: Determinants of College GPA\n",
    "\n",
    "This example from Wooldridge (2019) relates the college GPA (*colGPA*) to the high school GPA (*hsGPA*) and the achievement test score (*ACT*) for a sample of 141 students. The OLS regression function is\n",
    "\n",
    "$$\\hat{colGPA} = 1.286 + 0.453 \\cdot hsGPA + 0.0094 \\cdot ACT$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "material-benjamin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import wooldridge as woo\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "local-fleet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import gpa1 data set\n",
    "gpa1 = woo.dataWoo('gpa1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "centered-acrylic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression Summary Output: \n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                 colGPA   R-squared:                       0.176\n",
      "Model:                            OLS   Adj. R-squared:                  0.164\n",
      "Method:                 Least Squares   F-statistic:                     14.78\n",
      "Date:                Tue, 11 May 2021   Prob (F-statistic):           1.53e-06\n",
      "Time:                        16:46:17   Log-Likelihood:                -46.573\n",
      "No. Observations:                 141   AIC:                             99.15\n",
      "Df Residuals:                     138   BIC:                             108.0\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      1.2863      0.341      3.774      0.000       0.612       1.960\n",
      "hsGPA          0.4535      0.096      4.733      0.000       0.264       0.643\n",
      "ACT            0.0094      0.011      0.875      0.383      -0.012       0.031\n",
      "==============================================================================\n",
      "Omnibus:                        3.056   Durbin-Watson:                   1.885\n",
      "Prob(Omnibus):                  0.217   Jarque-Bera (JB):                2.469\n",
      "Skew:                           0.199   Prob(JB):                        0.291\n",
      "Kurtosis:                       2.488   Cond. No.                         298.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the model and print the summary output\n",
    "reg = smf.ols(formula = 'colGPA ~ hsGPA + ACT', data = gpa1)\n",
    "results = reg.fit()\n",
    "print(f'Regression Summary Output: \\n{results.summary()}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convinced-vulnerability",
   "metadata": {},
   "source": [
    "#### Wooldridge, Example 3.4: Determinants of College GPA\n",
    "\n",
    "For the regression run in Example 3.1, the output reports $R^2$ = 0.176, so about 17.6% of the variance in college GPA is explained by the two regressors.\n",
    "\n",
    "#### Examples 3.2, 3.3, 3.5, 3.6: Further Multiple Regression Examples\n",
    "\n",
    "In order ot get a feeling of the methods and results, we present the analyses including the full regression tables of the mentioned Examples from Wooldridge (2019). See Wooldridge (2019) for descriptions of the data sets and variables and for comments on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "controlled-passenger",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import wooldridge as woo\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "generous-senate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression Summary: \n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:           np.log(wage)   R-squared:                       0.316\n",
      "Model:                            OLS   Adj. R-squared:                  0.312\n",
      "Method:                 Least Squares   F-statistic:                     80.39\n",
      "Date:                Tue, 11 May 2021   Prob (F-statistic):           9.13e-43\n",
      "Time:                        16:46:17   Log-Likelihood:                -313.55\n",
      "No. Observations:                 526   AIC:                             635.1\n",
      "Df Residuals:                     522   BIC:                             652.2\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      0.2844      0.104      2.729      0.007       0.080       0.489\n",
      "educ           0.0920      0.007     12.555      0.000       0.078       0.106\n",
      "exper          0.0041      0.002      2.391      0.017       0.001       0.008\n",
      "tenure         0.0221      0.003      7.133      0.000       0.016       0.028\n",
      "==============================================================================\n",
      "Omnibus:                       11.534   Durbin-Watson:                   1.769\n",
      "Prob(Omnibus):                  0.003   Jarque-Bera (JB):               20.941\n",
      "Skew:                           0.021   Prob(JB):                     2.84e-05\n",
      "Kurtosis:                       3.977   Cond. No.                         135.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 3.2\n",
    "\n",
    "# Import wage1 data set\n",
    "wage1 = woo.dataWoo('wage1')\n",
    "\n",
    "# Build the OLS regression model and print the summary output\n",
    "reg = smf.ols(formula = 'np.log(wage) ~ educ + exper + tenure', data = wage1)\n",
    "results = reg.fit()\n",
    "print(f'Regression Summary: \\n{results.summary()}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "included-inspiration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression Summary: \n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                  prate   R-squared:                       0.092\n",
      "Model:                            OLS   Adj. R-squared:                  0.091\n",
      "Method:                 Least Squares   F-statistic:                     77.79\n",
      "Date:                Tue, 11 May 2021   Prob (F-statistic):           6.67e-33\n",
      "Time:                        16:46:17   Log-Likelihood:                -6422.3\n",
      "No. Observations:                1534   AIC:                         1.285e+04\n",
      "Df Residuals:                    1531   BIC:                         1.287e+04\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept     80.1190      0.779    102.846      0.000      78.591      81.647\n",
      "mrate          5.5213      0.526     10.499      0.000       4.490       6.553\n",
      "age            0.2431      0.045      5.440      0.000       0.155       0.331\n",
      "==============================================================================\n",
      "Omnibus:                      375.579   Durbin-Watson:                   1.910\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              805.992\n",
      "Skew:                          -1.387   Prob(JB):                    9.57e-176\n",
      "Kurtosis:                       5.217   Cond. No.                         32.5\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 3.3\n",
    "\n",
    "# Import 401k data set\n",
    "k401k = woo.dataWoo('401k')\n",
    "\n",
    "# Build the OLS regression model and print the summary output\n",
    "reg = smf.ols(formula = 'prate ~ mrate + age', data = k401k)\n",
    "results = reg.fit()\n",
    "print(f'Regression Summary: \\n{results.summary()}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "indonesian-genetics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression Summary: \n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                 narr86   R-squared:                       0.041\n",
      "Model:                            OLS   Adj. R-squared:                  0.040\n",
      "Method:                 Least Squares   F-statistic:                     39.10\n",
      "Date:                Tue, 11 May 2021   Prob (F-statistic):           9.91e-25\n",
      "Time:                        16:46:17   Log-Likelihood:                -3394.7\n",
      "No. Observations:                2725   AIC:                             6797.\n",
      "Df Residuals:                    2721   BIC:                             6821.\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      0.7118      0.033     21.565      0.000       0.647       0.776\n",
      "pcnv          -0.1499      0.041     -3.669      0.000      -0.230      -0.070\n",
      "ptime86       -0.0344      0.009     -4.007      0.000      -0.051      -0.018\n",
      "qemp86        -0.1041      0.010    -10.023      0.000      -0.124      -0.084\n",
      "==============================================================================\n",
      "Omnibus:                     2394.860   Durbin-Watson:                   1.836\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           106169.153\n",
      "Skew:                           4.002   Prob(JB):                         0.00\n",
      "Kurtosis:                      32.513   Cond. No.                         8.27\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 3.5a\n",
    "\n",
    "# Import crime1 data set\n",
    "crime1 = woo.dataWoo('crime1')\n",
    "\n",
    "# Build the OLS regression model and print the summary output\n",
    "reg = smf.ols(formula = 'narr86 ~ pcnv + ptime86 + qemp86', data = crime1)\n",
    "results = reg.fit()\n",
    "print(f'Regression Summary: \\n{results.summary()}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "actual-addiction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression Summary: \n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                 narr86   R-squared:                       0.042\n",
      "Model:                            OLS   Adj. R-squared:                  0.041\n",
      "Method:                 Least Squares   F-statistic:                     29.96\n",
      "Date:                Tue, 11 May 2021   Prob (F-statistic):           2.01e-24\n",
      "Time:                        16:46:17   Log-Likelihood:                -3393.5\n",
      "No. Observations:                2725   AIC:                             6797.\n",
      "Df Residuals:                    2720   BIC:                             6826.\n",
      "Df Model:                           4                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      0.7068      0.033     21.319      0.000       0.642       0.772\n",
      "pcnv          -0.1508      0.041     -3.692      0.000      -0.231      -0.071\n",
      "avgsen         0.0074      0.005      1.572      0.116      -0.002       0.017\n",
      "ptime86       -0.0374      0.009     -4.252      0.000      -0.055      -0.020\n",
      "qemp86        -0.1033      0.010     -9.940      0.000      -0.124      -0.083\n",
      "==============================================================================\n",
      "Omnibus:                     2396.990   Durbin-Watson:                   1.837\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           106841.658\n",
      "Skew:                           4.006   Prob(JB):                         0.00\n",
      "Kurtosis:                      32.611   Cond. No.                         10.2\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 3.5b\n",
    "\n",
    "# Import crime1 data set\n",
    "crime1 = woo.dataWoo('crime1')\n",
    "\n",
    "# Build the OLS regression model and print the summary output\n",
    "reg = smf.ols(formula = 'narr86 ~ pcnv + avgsen + ptime86 + qemp86', data = crime1)\n",
    "results = reg.fit()\n",
    "print(f'Regression Summary: \\n{results.summary()}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "spread-final",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression Summary: \n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:           np.log(wage)   R-squared:                       0.186\n",
      "Model:                            OLS   Adj. R-squared:                  0.184\n",
      "Method:                 Least Squares   F-statistic:                     119.6\n",
      "Date:                Tue, 11 May 2021   Prob (F-statistic):           3.27e-25\n",
      "Time:                        16:46:17   Log-Likelihood:                -359.38\n",
      "No. Observations:                 526   AIC:                             722.8\n",
      "Df Residuals:                     524   BIC:                             731.3\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      0.5838      0.097      5.998      0.000       0.393       0.775\n",
      "educ           0.0827      0.008     10.935      0.000       0.068       0.098\n",
      "==============================================================================\n",
      "Omnibus:                       11.804   Durbin-Watson:                   1.801\n",
      "Prob(Omnibus):                  0.003   Jarque-Bera (JB):               13.811\n",
      "Skew:                           0.268   Prob(JB):                      0.00100\n",
      "Kurtosis:                       3.586   Cond. No.                         60.2\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example 3.6\n",
    "\n",
    "# Import wage1 data set\n",
    "wage1 = woo.dataWoo('wage1')\n",
    "\n",
    "# Build the OLS regression model and print the summary output\n",
    "reg = smf.ols(formula = 'np.log(wage) ~ educ', data = wage1)\n",
    "results = reg.fit()\n",
    "print(f'Regression Summary: \\n{results.summary()}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rough-wyoming",
   "metadata": {},
   "source": [
    "### 2. OLS in Matrix Form\n",
    "\n",
    "For applying regression methods to empirical problems, we do not actually need to know the formulas our software uses. In multiple regression, we need to resort to matrix algebra in order to find an explicity expression for the OLS parameter estimates. Wooldridge (2019) defers this discussion to Appendix E and we folow the notation used there. Going through this material is not required for applying multiple regression to real-world problems but is useful for a deeper understanding of the methods and their black-box implementations in software packages. In the following chapters, we will rely on the comfort of the canned routine **fit()**, so this section may be skipped.\n",
    "\n",
    "In matrix form, we store the regressors in a *n $\\cdot$ (k + 1)* matrix **X** which has a column for each regressor plus a column of ones for the constant. The sample values of the dependent variable are stored in a *n $\\cdot$ 1* column vector **y**.  Wooldridge (2019) derives the OLS estimator $\\hat{\\beta} = (\\hat{\\beta}_0, \\hat{\\beta}_1, \\hat{\\beta}_2, \\hat{\\beta}_3, ..., \\hat{\\beta}_k)$ to be\n",
    "\n",
    "$$\\hat{\\beta} = (X'X)^{-1}X'y$$\n",
    "\n",
    "This equation involves three matrix operations which we know how to implement in *Python*:\n",
    "\n",
    "- Transpose: The expression $X'$ is **X.T** in **numpy**\n",
    "- Matrix multiplication: The expression $X'X$ is translated as **X.T @ X**\n",
    "- Inverse: $(X'X)^{-1}$ is written as **np.linalg.inv(X.T @ X)**\n",
    "\n",
    "So we can collect everything and translate the matrix equation into the somewhat unsightly expression\n",
    "\n",
    "``` Python\n",
    "b = np.linalg.inv(X.T @ X) @ X.T @y\n",
    "```\n",
    "\n",
    "The vector of residuals can be manually calculated as \n",
    "\n",
    "$$\\hat{u} = y - X\\hat{\\beta}$$\n",
    "\n",
    "or translated into the **numpy** matrix language\n",
    "\n",
    "``` Python\n",
    "u_hat = y - X @ b\n",
    "```\n",
    "\n",
    "The formula for the estimated variance of the error term is\n",
    "\n",
    "$$\\hat{\\sigma}^2 = \\frac{1}{n - k - 1} \\hat{u}' \\hat{u}$$\n",
    "\n",
    "which is equivalent to\n",
    "\n",
    "``` Python\n",
    "sigsq_hat = (u_hat.T @ u_hat) / (n - k - 1)\n",
    "```\n",
    "\n",
    "The standard error of the regression (SER) is its square root $\\hat{\\sigma} = \\sqrt{\\hat{\\sigma}^2}$. The estimated OLS variance-covariance matrix according to Wooldridge (2019, Theorem E.2) is then\n",
    "\n",
    "$$\\hat{Var(\\hat{\\beta})} = \\hat{\\sigma}^2 (X'X)^{-1}$$\n",
    "\n",
    "``` Python\n",
    "Vb_hat = sigsq_hat * np.linalg.inv(X.T @ X)\n",
    "```\n",
    "\n",
    "Finally, the standard error of the parameter estimates are the square roots of the main diagonal of Var($\\hat{\\beta}$) which can be expressed in **numpy** as\n",
    "\n",
    "``` Python\n",
    "se = np.sqrt(np.diagonal(Vb_hat))\n",
    "```\n",
    "\n",
    "Below example implements this for the GPA regression from Example 3.1. Comparing the results to the built-in function, it is reassuring that we get exactly the same numbers for the parameter estimates and standard errors of the coefficients. We also demonstrate another way of generating **y** and **X** by using the module **patsy**. It includes the command **dmatrices()**, which allows to conveniently create the matrices by formula syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "floppy-marine",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import wooldridge as woo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import patsy as pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "seventh-doubt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import gpa1 data set\n",
    "gpa1 = woo.dataWoo('gpa1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "capital-cooper",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine sample size and number of regressors\n",
    "n = len(gpa1)\n",
    "k = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "english-judges",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Rows of X: \n",
      "   const  hsGPA  ACT\n",
      "0      1    3.0   21\n",
      "1      1    3.2   24\n",
      "2      1    3.6   26\n",
      "3      1    3.5   27\n",
      "4      1    3.9   28\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract dependent variable y, 'colGPA'\n",
    "y = gpa1['colGPA']\n",
    "\n",
    "# Extract independent variables X and add a column of ones\n",
    "X = pd.DataFrame({'const': 1, 'hsGPA': gpa1['hsGPA'], 'ACT': gpa1['ACT']})\n",
    "\n",
    "# Alternative with patsy:\n",
    "# y2, X2 = pt.dmatrices('colGPA ~ hsGPA + ACT', data = gpa1, return_type = 'dataframe')\n",
    "\n",
    "# Print the first rows of X\n",
    "print(f'First Rows of X: \\n{X.head()}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "static-binary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Parameters (Betas): \n",
      "[[1.28632777]\n",
      " [0.45345589]\n",
      " [0.00942601]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parameter estimates\n",
    "X = np.array(X)\n",
    "y = np.array(y).reshape(n, 1) # Create a row vector\n",
    "b = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "print(f'Estimated Parameters (Betas): \\n{b}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "lesbian-copyright",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SER: [[0.34031576]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Residuals, estimated variance of u and SER\n",
    "u_hat = y - X @ b\n",
    "sigsq_hat = (u_hat.T @ u_hat) / (n - k - 1)\n",
    "SER = np.sqrt(sigsq_hat)\n",
    "print(f'SER: {SER}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "handy-community",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Errors of the Estimated Parameters: \n",
      "[0.34082212 0.09581292 0.01077719]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Estimated variance of the parameter estimators and SE\n",
    "Vbeta_hat = sigsq_hat * np.linalg.inv(X.T @ X)\n",
    "se = np.sqrt(np.diagonal(Vbeta_hat))\n",
    "print(f'Standard Errors of the Estimated Parameters: \\n{se}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thorough-stereo",
   "metadata": {},
   "source": [
    "### 3. Ceteris Paribus Interpretation and Omitted Variable Bias\n",
    "\n",
    "The parameters in a multiple regression can be interpreted as partial effects. In a general model with *k* regressors, the estimated slope parameter $\\beta_j$ associated with the variable $x_j$ is the change of $\\hat{y}$ as $x_j$ increases by one unit and *the other variable are held fixed*.\n",
    "\n",
    "Wooldridge (2019) discusses this interpretation in Section 3.2 and offers a useful formula for interpreting the difference between simple regression results and the *ceteris paribus* interpretation of multiple regression: Consider a regression with two explanatory variables:\n",
    "\n",
    "(3.1)\n",
    "$$\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2$$\n",
    "\n",
    "The parameter $\\hat{\\beta}_1$ is the estimated effect of increasing $x_1$ by one unit while keeping $x_2$ fixed. In contrast, consider the simple regression including only $x_1$ as a regressor:\n",
    "\n",
    "(3.2)\n",
    "$$\\tilde{y} = \\tilde{\\beta}_0 + \\tilde{\\beta}_1 x_1$$\n",
    "\n",
    "The parameter $\\tilde{\\beta}_1$ is the estimated effect of increasing $x_1$ by one unit (and NOT keeping $x_2$ fixed). It can be related to $\\hat{\\beta}_1$ using the formula\n",
    "\n",
    "(3.3)\n",
    "$$\\tilde{\\beta}_1 = \\hat{\\beta}_1 + \\hat{\\beta}_2 \\tilde{\\delta}_1$$\n",
    "\n",
    "where $\\tilde{\\delta}_1$ is the slope paramter of the linear regresion of $x_2$ on $x_1$\n",
    "\n",
    "(3.4)\n",
    "$$x_2 = \\tilde{\\delta}_0 + \\tilde{\\delta}_1 x_1$$\n",
    "\n",
    "This equation is actually quite intuitive: As $x_1$ increases by one unit,\n",
    "\n",
    "- Predicted *y* directly increases by $\\hat{\\beta}_1$ units (*ceteris paribus* effect)\n",
    "- Predicted $x_2$ increases by $\\tilde{\\delta}_1$ units\n",
    "- Each of these $\\tilde{delta}_1$ units leads to an increase of predicted *y* by $\\hat{\\beta}_2$ units, giving a total indirect effect of $\\tilde{\\delta}_1 \\hat{\\beta}_2$\n",
    "- The overall effect $\\tilde{\\beta}_1$ is the sum of the direct and indirect effects\n",
    "\n",
    "We revisit Example 3.1 to see whether we can demonstrate this relationship in *Python*. First, we repeat the regression of the college GPA (*colGPA*) on the achievement test score (*ACT*) and the high school GPA (*hsGPA*). We study the *ceteris paribus* effect of *ACT* on *colGPA* which has an estimated value of $\\hat{\\beta}_1$ = 0.0094. The estimated effect of *hsGPA* is $\\hat{\\beta}_2$ = 0.453. The slope parameter of the regression corresponding to Equation 3.4 is $\\hat{\\delta}_1$ = 0.0389. Plugging these values into Equation 3.3 gives a total effect of $\\tilde{\\beta}_1$ = 0.0271 which is exactly what the simple regression at the end of the output delivers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aggregate-learning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import wooldridge as woo\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "standard-georgia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import gpa1 data set\n",
    "gpa1 = woo.dataWoo('gpa1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ready-realtor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Parameters (betas): \n",
      "Intercept    1.286328\n",
      "hsGPA        0.453456\n",
      "ACT          0.009426\n",
      "dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the model and print the summary output\n",
    "reg = smf.ols(formula = 'colGPA ~ hsGPA + ACT', data = gpa1)\n",
    "results = reg.fit()\n",
    "b = results.params\n",
    "print(f'Estimated Parameters (betas): \\n{b}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "angry-postcard",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Parameters (delta): \n",
      "Intercept    2.462537\n",
      "ACT          0.038897\n",
      "dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Relation between regressors\n",
    "reg_delta = smf.ols(formula = 'hsGPA ~ ACT', data = gpa1)\n",
    "results_delta = reg_delta.fit()\n",
    "delta_tilde = results_delta.params\n",
    "print(f'Estimated Parameters (delta): \\n{delta_tilde}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "lucky-advocacy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Omitted Variable Effect: 0.027063973943178537\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Omitted variables formula for b1_tilde\n",
    "b1_tilde = b['ACT'] + b['hsGPA'] * delta_tilde['ACT']\n",
    "print(f'Omitted Variable Effect: {b1_tilde}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "guilty-joshua",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Parameters of ACT: \n",
      "Intercept    2.402979\n",
      "ACT          0.027064\n",
      "dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Actual regression with hsGPA omitted\n",
    "reg_om = smf.ols(formula = 'colGPA ~ ACT', data = gpa1)\n",
    "results_om = reg_om.fit()\n",
    "b_om = results_om.params\n",
    "print(f'Estimated Parameters of ACT: \\n{b_om}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controlling-munich",
   "metadata": {},
   "source": [
    "In this example, the indirect effect is actually stronger than the direct effect. *ACT* predicts *colGPA* mainly because it is related to *hsGPA* which in turn is strongly related to *colGPA*.\n",
    "\n",
    "These relations hold for the estimates from a given sample. In Section 3.3, Wooldridge (2019) discusses how to apply the same sort of arguments to the OLS estimators which are random variables varying over different samples. Omitting relevant regressors causes bias if we are interested in estimating partial effects. In practice, it is difficult to include *all* relevant regressors making of omitted variables a prevalent problem. It is important enough to have motivated a vast amount of methodological and applied research. More advance techniques like instrumental variables or panel data methods try to solve the problem in cases where we cannot add al relevant regressors, for example becasue they are unobservable. We will come back to this in the later sections.\n",
    "\n",
    "### 4. Standard Errors, Multicollinearity, and VIF\n",
    "\n",
    "We have already seen the matrix formula for the conditional variance-covariance matrix under the usual assumptions including homoscedasticity (MLR.5). Theorem 3.2 provides another useful formula for the variance of a single parameter $\\beta_j$, i.e. for a single element on the main diagonal of he variance-coverance matrix:\n",
    "\n",
    "$$Var(\\hat{\\beta}_j) = \\frac{\\sigma^2}{SST_j (1 - R_j^2)} = \\frac{1}{n} \\cdot \\frac{\\sigma^2}{Var(x_j)} \\cdot \\frac{1}{1 - R_j^2}$$\n",
    "\n",
    "where $SST_j = \\sum_{i = 1}^n (x_ji - \\bar{x}_j)^2 = (n - 1) \\cdot Var(x_j)$ is the total sum of squares and $R_j^2$ is the usual coefficient of determination from a regression of $x_j$ on all of the other regressors.\n",
    "\n",
    "The variance of $\\hat{\\beta}_j$ consists of four parts:\n",
    "\n",
    "- $\\frac{1}{n}$: The variance is smaller for larger samples.\n",
    "- $\\sigma^2$: The variance is larger if the error term varies a lot, since it introduces randomness into the relationship between the variables of interest.\n",
    "- $\\frac{1}{Var(x_j)}$: The variance is smaller if the regressor $x_j$ varies a lot since this provides relevent information about the relationship.\n",
    "- $\\frac{1}{1 - R_j^2}$: This variance inflation factor (VIF) accounts for (imperfect) multicollinearity. If $x_j$ is highly related to the other regressors, $R_j^2$ and therefore also $VIF_j$ and the variance of $\\hat{\\beta}_j$ are large.\n",
    "\n",
    "Since the error variance $\\sigma^2$ is unknown, we replace it with an estimate to come up with an estimated variance of the parameter estimate. Its square root is the standard error\n",
    "\n",
    "$$se(\\hat{\\beta}_j) = \\frac{1}{\\sqrt{n}} \\cdot \\frac{\\hat{\\sigma}}{sd(x_j)} \\cdot \\frac{1}{\\sqrt{1 - R_j^2}}$$\n",
    "\n",
    "It is not directly obvious that this formula leads to the same results as the matrix formula. We will validate this formula by replicating Example 3.1 which we also used for manually calculating the SE using teh matrix formula above. \n",
    "\n",
    "We also use this example to demonstrate how to extract results which are included in the object returned by the **fit()** method. Given its results are stored in variable **results** using the results of **results = smf.ols(...).fit()**, we can easily access the information using **results.resultname** where the **resultname** can be any of hte following:\n",
    "\n",
    "- **params** for the regressor coefficients\n",
    "- **resid** for the residuals\n",
    "- **mse_resid** for the (squared) SER\n",
    "- **rsquared** for $R^2$\n",
    "- and more\n",
    "\n",
    "We use this to extract the SER of the main regression and the $R_j^2$ from the regression of *hsGPA* on *ACT* which is needed for calculating the VIF for the coefficient of *hsGPA*. The other statistics are straightforward. The standard error calculated this way is exactly the same as the one of the built-in command and the matrix formula used in the previous examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "later-former",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import wooldridge as woo\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "adjusted-bible",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import gpa1 data set\n",
    "gpa1 = woo.dataWoo('gpa1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "strategic-voice",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model and print the summary output\n",
    "reg = smf.ols(formula = 'colGPA ~ hsGPA + ACT', data = gpa1)\n",
    "results = reg.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "international-elder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract SER (instead of calculation via residual)\n",
    "SER = np.sqrt(results.mse_resid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "warming-howard",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VIF: 1.1358234481972789\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Regressing hsGPA on ACT for calculation of R^2 and VIF\n",
    "reg_hsGPA = smf.ols(formula = 'hsGPA ~ ACT', data = gpa1)\n",
    "results_hsGPA = reg_hsGPA.fit()\n",
    "R2_hsGPA = results_hsGPA.rsquared\n",
    "VIF_hsGPA = 1 / (1 - R2_hsGPA)\n",
    "print(f'VIF: {VIF_hsGPA}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "sensitive-bubble",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Error of hsGPA Coefficient: 0.09581291608057603\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Manual Calculation of SE of hsGPA coefficient\n",
    "n = results.nobs\n",
    "sdx = np.std(gpa1['hsGPA'], ddof = 1) * np.sqrt((n - 1) / n)\n",
    "SE_hsGPA = 1 / np.sqrt(n) * SER / sdx * np.sqrt(VIF_hsGPA)\n",
    "print(f'Standard Error of hsGPA Coefficient: {SE_hsGPA}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sought-strike",
   "metadata": {},
   "source": [
    "A convenient way to automatically calculate variance inflation factors (VIF) is provided by the module **statsmodels** in **stats.outliers_influence**. The command and the number of a given regressor (starting with the constant as the regressor with number 0). The calculation for each of the regressor is performed in a loop.\n",
    "\n",
    "We extend Example 3.6 and regress individual log wage on education (*educ*), potential overall work experience (*exper*), and the number of years with current employer (*tenure*). We could imagine that these three variables are correlated with each other, but the results show no big VIF. The largest one is for the coefficient of *exper*. Its variance is higher by a factor of (only) 1.478 than in a world in which it were uncorrelated with the other regressors. So we don't have to worry about multicollinearity here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "national-strap",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import wooldridge as woo\n",
    "import numpy as np\n",
    "import statsmodels.stats.outliers_influence as smo\n",
    "import patsy as pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "statewide-expert",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import wage1 data set\n",
    "wage1 = woo.dataWoo('wage1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "vocal-windsor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract matrices using patsy\n",
    "y, X = pt.dmatrices('np.log(wage) ~ educ + exper + tenure',\n",
    "                   data = wage1, return_type = 'dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "operating-carnival",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VIF: \n",
      "[29.37890286  1.11277075  1.47761777  1.34929556]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate VIF\n",
    "K = X.shape[1]\n",
    "VIF = np.empty(K)\n",
    "for i in range(K):\n",
    "    VIF[i] = smo.variance_inflation_factor(X.values, i)\n",
    "print(f'VIF: \\n{VIF}\\n')"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
