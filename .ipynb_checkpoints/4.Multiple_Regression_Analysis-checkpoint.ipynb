{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "protective-vertex",
   "metadata": {},
   "source": [
    "## Multiple Regression Analysis\n",
    "\n",
    "Running a multiple regression in *Python* is as straightforward as running a simple regression using the **ols()** command in **statsmodels**. In the following example, we show how it is done in coding. In the later section, we will open the black box and replicaes the main calculations using matrix algebra. This is not required for the remaining notebook, so it can be skipped by readers who prefer to keep black boxes closed.\n",
    "\n",
    "We will also discuss the interpretation of regression results and the prevalent omitted variable problems. Finally, we will cover standard errors and multicollinearity for multiple regression by the end of this notebook.\n",
    "\n",
    "### Multiple Regression in Practice\n",
    "\n",
    "Consider the population regression model\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + ... + \\beta_k x_k + u$$\n",
    "\n",
    "and suppose the data set **sample** contains variables **y, x1, x2, x3**, with respective data of our sample. We estimate the model parameters by OLS using the commands\n",
    "\n",
    "``` Python\n",
    "reg = smf.ols(formula = 'y ~ x1 + x2 + x3', data = sample)\n",
    "results = reg.fit()\n",
    "```\n",
    "\n",
    "The tilde \"$\\tilde$\" again separates the dependent variable from the regressors which are now separated using \"**+**\" sign. We can add options as before. The constant is again automatically added unless it is explicitly suppressed using **'y ~ x1 + x2 + x3 + ...'**.\n",
    "\n",
    "WE already familar with the working of **smf.ols()** and **fit()**: The first command creates an object which contains all relevant information and the estimation is performed in a second step. The estimation results are stored in a variable **results** using the code **results = reg.fit()**. We can use this variable for further analyses. For a typical regression output including a coefficent table, call **results.summary()** in one step. Further analyses involving residuals, fitted values and the like can be used exactly as presented in the previous notebook.\n",
    "\n",
    "The output of **summary()** includes parameter estimates, standard errors according to Theorem 3.2 of Wooldgridge (2019), the coefficient of determination $R^2$, and many more useful results we cannot interpret yet before we have worked through the next notebook file.\n",
    "\n",
    "#### Wooldridge, Example 3.1: Determinants of College GPA\n",
    "\n",
    "This example from Wooldridge (2019) relates the college GPA (*colGPA*) to the high school GPA (*hsGPA*) and the achievement test score (*ACT*) for a sample of 141 students. The OLS regression function is\n",
    "\n",
    "$$\\hat{colGPA} = 1.286 + 0.453 \\cdot hsGPA + 0.0094 \\cdot ACT$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "material-benjamin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import wooldridge as woo\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "local-fleet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import gpa1 data set\n",
    "gpa1 = woo.dataWoo('gpa1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "centered-acrylic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression Summary Output: \n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                 colGPA   R-squared:                       0.176\n",
      "Model:                            OLS   Adj. R-squared:                  0.164\n",
      "Method:                 Least Squares   F-statistic:                     14.78\n",
      "Date:                Sun, 09 May 2021   Prob (F-statistic):           1.53e-06\n",
      "Time:                        21:38:29   Log-Likelihood:                -46.573\n",
      "No. Observations:                 141   AIC:                             99.15\n",
      "Df Residuals:                     138   BIC:                             108.0\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      1.2863      0.341      3.774      0.000       0.612       1.960\n",
      "hsGPA          0.4535      0.096      4.733      0.000       0.264       0.643\n",
      "ACT            0.0094      0.011      0.875      0.383      -0.012       0.031\n",
      "==============================================================================\n",
      "Omnibus:                        3.056   Durbin-Watson:                   1.885\n",
      "Prob(Omnibus):                  0.217   Jarque-Bera (JB):                2.469\n",
      "Skew:                           0.199   Prob(JB):                        0.291\n",
      "Kurtosis:                       2.488   Cond. No.                         298.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the model and print the summary output\n",
    "reg = smf.ols(formula = 'colGPA ~ hsGPA + ACT', data = gpa1)\n",
    "results = reg.fit()\n",
    "print(f'Regression Summary Output: \\n{results.summary()}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convinced-vulnerability",
   "metadata": {},
   "source": [
    "#### Wooldridge, Example 3.4: Determinants of College GPA\n",
    "\n",
    "For the regression run in Example 3.1, the output reports $R^2$ = 0.176, so about 17.6% of the variance in college GPA is explained by the two regressors.\n",
    "\n",
    "#### Examples 3.2, 3.3, 3.5, 3.6: Further Multiple Regression Examples\n",
    "\n",
    "In order ot get a feeling of the methods and results, we present the analyses including the full regression tables of the mentioned Examples from Wooldridge (2019). See Wooldridge (2019) for descriptions of the data sets and variables and for comments on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlled-passenger",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import wooldridge as woo\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generous-senate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3.2\n",
    "\n",
    "# Import wage1 data set\n",
    "wage1 = woo.dataWoo('wage1')\n",
    "\n",
    "# Build the OLS regression model and print the summary output\n",
    "reg = smf.ols(formula = 'np.log(wage) ~ educ + exper + tenure', data = wage1)\n",
    "results = reg.fit()\n",
    "print(f'Regression Summary: \\n{results.summary()}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "included-inspiration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3.3\n",
    "\n",
    "# Import 401k data set\n",
    "k401k = woo.dataWoo('401k')\n",
    "\n",
    "# Build the OLS regression model and print the summary output\n",
    "reg = smf.ols(formula = 'prate ~ mrate + age', data = k401k)\n",
    "results = reg.fit()\n",
    "print(f'Regression Summary: \\n{result.summary()}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indonesian-genetics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3.5a\n",
    "\n",
    "# Import crime1 data set\n",
    "crime1 = woo.dataWoo('crime1')\n",
    "\n",
    "# Build the OLS regression model and print the summary output\n",
    "reg = smf.ols(formula = 'narr86 ~ pcnv + ptime86 + qemp86', data = crime1)\n",
    "results = reg.fit()\n",
    "print(f'Regression Summary: \\n{result.summary()}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "actual-addiction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3.5b\n",
    "\n",
    "# Import crime1 data set\n",
    "crime1 = woo.dataWoo('crime1')\n",
    "\n",
    "# Build the OLS regression model and print the summary output\n",
    "reg = smf.ols(formula = 'narr86 ~ pcnv + avgsen + ptime86 + qemp86', data = crime1)\n",
    "results = reg.fit()\n",
    "print(f'Regression Summary: \\n{result.summary()}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spread-final",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3.6\n",
    "\n",
    "# Import wage1 data set\n",
    "wage1 = woo.dataWoo('wage1')\n",
    "\n",
    "# Build the OLS regression model and print the summary output\n",
    "reg = smf.ols(formula = 'np.log(wage) ~ edu', data = crime1)\n",
    "results = reg.fit()\n",
    "print(f'Regression Summary: \\n{result.summary()}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rough-wyoming",
   "metadata": {},
   "source": [
    "### OLS in Matrix Form\n",
    "\n",
    "For applying regression methods to empirical problems, we do not actually need to know the formulas our software uses. In multiple regression, we need to resort to matrix algebra in order to find an explicity expression for the OLS parameter estimates. Wooldridge (2019) defers this discussion to Appendix E and we folow the notation used there. Going through this material is not required for applying multiple regression to real-world problems but is useful for a deeper understanding of the methods and their black-box implementations in software packages. In the following chapters, we will rely on the comfort of the canned routine **fit()**, so this section may be skipped.\n",
    "\n",
    "In matrix form, we store the regressors in a *n $\\cdot$ (k + 1)* matrix **X** which has a column for each regressor plus a column of ones for the constant. The sample values of the dependent variable are stored in a *n $\\cdot$ 1* column vector **y**.  Wooldridge (2019) derives the OLS estimator $\\hat{\\beta} = (\\hat{\\beta}_0, \\hat{\\beta}_1, \\hat{\\beta}_2, \\hat{\\beta}_3, ..., \\hat{\\beta}_k)$ to be\n",
    "\n",
    "$$\\hat{\\beta} = (X'X)^{-1}X'y$$\n",
    "\n",
    "This equation involves three matrix operations which we know how to implement in *Python*:\n",
    "\n",
    "- Transpose: The expression $X'$ is **X.T** in **numpy**\n",
    "- Matrix multiplication: The expression $X'X$ is translated as **X.T @ X**\n",
    "- Inverse: $(X'X)^{-1}$ is written as **np.linalg.inv(X.T @ X)**\n",
    "\n",
    "So we can collect everything and translate the matrix equation into the somewhat unsightly expression\n",
    "\n",
    "``` Python\n",
    "b = np.linalg.inv(X.T @ X) @ X.T @y\n",
    "```\n",
    "\n",
    "The vector of residuals can be manually calculated as \n",
    "\n",
    "$$\\hat{u} = y - X\\hat{\\beta}$$\n",
    "\n",
    "or translated into the **numpy** matrix language\n",
    "\n",
    "``` Python\n",
    "u_hat = y - X @ b\n",
    "```\n",
    "\n",
    "The formula for the estimated variance of the error term is\n",
    "\n",
    "$$\\hat{\\sigma}^2 = \\frac{1}{n - k - 1} \\hat{u}' \\hat{u}$$\n",
    "\n",
    "which is equivalent to\n",
    "\n",
    "``` Python\n",
    "sigsq_hat = (u_hat.T @ u_hat) / (n - k - 1)\n",
    "```\n",
    "\n",
    "The standard error of the regression (SER) is its square root $\\hat{\\sigma} = \\sqrt{\\hat{\\sigma}^2}$. The estimated OLS variance-covariance matrix according to Wooldridge (2019, Theorem E.2) is then\n",
    "\n",
    "$$\\hat{Var(\\hat{\\beta})} = \\hat{\\sigma}^2 (X'X)^{-1}$$\n",
    "\n",
    "``` Python\n",
    "Vb_hat = sigsq_hat * np.linalg.inv(X.T @ X)\n",
    "```\n",
    "\n",
    "Finally, the standard error of the parameter estimates are the square roots of the main diagonal of Var($\\hat{\\beta}$) which can be expressed in **numpy** as\n",
    "\n",
    "``` Python\n",
    "se = np.sqrt(np.diagonal(Vb_hat))\n",
    "```\n",
    "\n",
    "Below example implements this for the GPA regression from Example 3.1. Comparing the results to the built-in function, it is reassuring that we get exactly the same numbers for the parameter estimates and standard errors of the coefficients. We also demonstrate another way of generating **y** and **X** by using the module **patsy**. It includes the command **dmatrices()**, which allows to conveniently create the matrices by formula syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floppy-marine",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import wooldridge as woo\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import patsy as pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seventh-doubt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import gpa1 data set\n",
    "gpa1 = woo.dataWoo('gpa1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capital-cooper",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine sample size and number of regressors\n",
    "n = len(gpa1)\n",
    "k = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "english-judges",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract dependent variable y, 'colGPA'\n",
    "y = gpa1['colGPA']\n",
    "\n",
    "# Extract independent variables X and add a column of ones\n",
    "X = pd.DataFrame({'const': 1, 'hsGPA': gpa1['hsGPA'], 'ACT': gpa1['ACT']})\n",
    "\n",
    "# Alternative with patsy:\n",
    "# y2, X2 = pt.dmatrices('colGPA ~ hsGPA + ACT', data = gpa1, return_type = 'dataframe')\n",
    "\n",
    "# Print the first rows of X\n",
    "print(f'First Rows of X: \\n{X.head()}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "static-binary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter estimates\n",
    "X = np.array(X)\n",
    "y = np.array(y).reshape(n, 1) # Create a row vector\n",
    "b = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "print(f'Estimated Parameters (Betas): \\n{b}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesbian-copyright",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residuals, estimated variance of u and SER\n",
    "u_hat = y - X @ b\n",
    "sigsq_hat = (u_hat.T @ u_hat) / (n - k - 1)\n",
    "SER = np.sqrt(sigsq_hat)\n",
    "print(f'SER: {SER}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handy-community",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimated variance of the parameter estimators and SE\n",
    "Vbeta_hat = sigsq_hat * np.linalg.inv(X.T @ X)\n",
    "se = np.sqrt(np.diagonal(Vbeta_hat))\n",
    "print(f'Standard Errors of the Estimated Parameters: \\n{se}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thorough-stereo",
   "metadata": {},
   "source": [
    "### Ceteris Paribus Interpretation and Omitted Variable Bias\n",
    "\n",
    "The parameters in a multiple regression can be interpreted as partial effects. In a general model with *k* regressors, the estimated slope parameter $\\beta_j$ associated with the variable $x_j$ is the change of $\\hat{y}$ as $x_j$ increases by one unit and *the other variable are held fixed*.\n",
    "\n",
    "Wooldridge (2019) discusses this interpretation in Section 3.2 and offers a useful formula for interpreting the difference between simple regression results and the *ceteris paribus* interpretation of multiple regression: Consider a regression with two explanatory variables:\n",
    "\n",
    "(3.1)\n",
    "$$\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2$$\n",
    "\n",
    "The parameter $\\hat{\\beta}_1$ is the estimated effect of increasing $x_1$ by one unit while keeping $x_2$ fixed. In contrast, consider the simple regression including only $x_1$ as a regressor:\n",
    "\n",
    "(3.2)\n",
    "$$\\tilde{y} = \\tilde{\\beta}_0 + \\tilde{\\beta}_1 x_1$$\n",
    "\n",
    "The parameter $\\tilde{\\beta}_1$ is the estimated effect of increasing $x_1$ by one unit (and NOT keeping $x_2$ fixed). It can be related to $\\hat{\\beta}_1$ using the formula\n",
    "\n",
    "(3.3)\n",
    "$$\\tilde{\\beta}_1 = \\hat{\\beta}_1 + \\hat{\\beta}_2 \\tilde{\\delta}_1$$\n",
    "\n",
    "where $\\tilde{\\delta}_1$ is the slope paramter of the linear regresion of $x_2$ on $x_1$\n",
    "\n",
    "(3.4)\n",
    "$$x_2 = \\tilde{\\delta}_0 + \\tilde{\\delta}_1 x_1$$\n",
    "\n",
    "This equation is actually quite intuitive: As $x_1$ increases by one unit,\n",
    "\n",
    "- Predicted *y* directly increases by $\\hat{\\beta}_1$ units (*ceteris paribus* effect)\n",
    "- Predicted $x_2$ increases by $\\tilde{\\delta}_1$ units\n",
    "- Each of these $\\tilde{delta}_1$ units leads to an increase of predicted *y* by $\\hat{\\beta}_2$ units, giving a total indirect effect of $\\tilde{\\delta}_1 \\hat{\\beta}_2$\n",
    "- The overall effect $\\tilde{\\beta}_1$ is the sum of the direct and indirect effects\n",
    "\n",
    "We revisit Example 3.1 to see whether we can demonstrate this relationship in *Python*. First, we repeat the regression of the college GPA (*colGPA*) on the achievement test score (*ACT*) and the high school GPA (*hsGPA*). We study the *ceteris paribus* effect of *ACT* on *colGPA* which has an estimated value of $\\hat{\\beta}_1$ = 0.0094. The estimated effect of *hsGPA* is $\\hat{\\beta}_2$ = 0.453. The slope parameter of the regression corresponding to Equation 3.4 is $\\hat{\\delta}_1$ = 0.0389. Plugging these values into Equation 3.3 gives a total effect of $\\tilde{\\beta}_1$ = 0.0271 which is exactly what the simple regression at the end of the output delivers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggregate-learning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import wooldridge as woo\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standard-georgia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import gpa1 data set\n",
    "gpa1 = woo.dataWoo('gpa1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ready-realtor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model and print the summary output\n",
    "reg = smf.ols(formula = 'colGPA ~ hsGPA + ACT', data = gpa1)\n",
    "results = reg.fit()\n",
    "b = results.params\n",
    "print(f'Estimated Parameters (betas): \\n{b}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "angry-postcard",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relation between regressors\n",
    "reg_delta = smf.ols(formula = 'hsGPA ~ ACT', data = gpa1)\n",
    "results_delta = reg_delta.fit()\n",
    "delta_tilde = results_delta.params\n",
    "print(f'Estimated Parameters (delta): \\n{delta_tilde}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lucky-advocacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Omitted variables formula for b1_tilde\n",
    "b1_tilde = b['ACT'] + b['hsGPA'] * delta_tilde['ACT']\n",
    "print(f'Omitted Variable Effect: {b1_tilde}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guilty-joshua",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual regression with hsGPA omitted\n",
    "reg_om = smf.ols(formula = 'colGPA ~ ACT', data = gpa1)\n",
    "results_om = reg_om.fit()\n",
    "b_om = results_om.params\n",
    "print(f'Estimated Parameters of ACT: \\n{b_om}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controlling-munich",
   "metadata": {},
   "source": [
    "In this example, the indirect effect is actually stronger than the direct effect. *ACT* predicts *colGPA* mainly because it is related to *hsGPA* which in turn is strongly related to *colGPA*.\n",
    "\n",
    "These relations hold for the estimates from a given sample. In Section 3.3, Wooldridge (2019) discusses how to apply the same sort of arguments to the OLS estimators which are random variables varying over different samples. Omitting relevant regressors causes bias if we are interested in estimating partial effects. In practice, it is difficult to include *all* relevant regressors making of omitted variables a prevalent problem. It is important enough to have motivated a vast amount of methodological and applied research. More advance techniques like instrumental variables or panel data methods try to solve the problem in cases where we cannot add al relevant regressors, for example becasue they are unobservable. We will come back to this in the later sections.\n",
    "\n",
    "### Standard Errors, Multicollinearity, and VIF\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "later-former",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
