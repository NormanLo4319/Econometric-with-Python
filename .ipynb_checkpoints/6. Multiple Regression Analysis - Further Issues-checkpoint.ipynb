{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "consistent-timothy",
   "metadata": {},
   "source": [
    "## Multiple Regression Analysis: Further Issues\n",
    "\n",
    "In this notebook, we cover some issues regarding the implementation of regression analyses. We first discuss more flexible specification of regression equations such as variable scaling, standardization, polynomials, and interactions. They can be conveinently included in the **formula** and used in the **statsmodels** OLS estimation. Then, we discuss the model predictions and their confidence intervals. After that we would like to introduce using the **qualitatve regressors** or **categorical variable** in the regression model.\n",
    "\n",
    "#### Topics:\n",
    "\n",
    "1. Model Formula / Specification\n",
    "2. Prediction\n",
    "3. Qualitative Regressors\n",
    "\n",
    "### 1. Model Formula / Specification\n",
    "\n",
    "If we run a regression in **statsmodels** using a syntax like\n",
    "\n",
    "``` python\n",
    "smf.ols('y ~ x1 + x2 + x3', data = sample)\n",
    "```\n",
    "\n",
    "the expression **y ~ x1 + x2 + x3** is referred to as a model **formula** or **specification**. It is a compact symbolic way to describe our regression equation. The dependent variable is separated from the regressors by a '~' and the regressors are separated by a '+' indicating that they enter the equation in a linear fashion. A constant is added by default. Such formula can be specified in more complex ways to indicate different kinds of regression equations. We will cover the most important ones in this section.\n",
    "\n",
    "#### Data Scaling: Arithmetic Operations within a Formula\n",
    "\n",
    "Woodldrige (2019) discusses how different scaling of the variables in the model affect the parameter estimates and other statistics in Section 6.1. As an example, a model relating the birth weight to cigarette smoking of the mother during pregnancy and the family income. The basic model equation is\n",
    "\n",
    "$$bwght = \\beta_0 + \\beta_1 cigs + \\beta_2 faminc + u$$\n",
    "\n",
    "which translates into formula syntax as **bwght ~ cigs + faminc**.\n",
    "\n",
    "If we want to measure the weight in pounds rather than ounces, there are two ways to implement different rescaling in *Python*. We can\n",
    "\n",
    "- Define a different variable like **bwghtlbs = bwght / 16** and use this variable in the formula: **bwghtlbs ~ cigs + faminc**\n",
    "- Specify this rescaling directly in the formula: **I(bwght/16) ~ cigs + faminc**\n",
    "\n",
    "The later approach can be more convenient. Note that the **I(...)** brackets describe any parts of the formula in which we specify arithmetic transformations.\n",
    "\n",
    "If we want to measure the number of cigarettes smoked per day in packs, we could again define a new variable **pack = cigs / 20** and use it as a regressor or simply specify the formula **bwght ~ I(cigs/20) + faminc**. Here, the importance to use the **I** function is easy to see. If we specified the formula **bwght ~ I(cigs/20 + faminc)** instead, we would have a nonsense model with only one regressor: the sum of the packs smoked and the income.\n",
    "\n",
    "Below example demonstrates these features. As discussed in Wooldridge (2019, Section 6.1), dividing the dependent variable by 16 changes all coefficients by the same factor $\\frac{1}{16}$ and dividing a regressor by 20 changes its coefficient by the factor 20. Other statistics like $R^2$ are unaffected.        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "educational-antenna",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dependencies\n",
    "import wooldridge as woo\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "magnetic-crime",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data set 'bwght'\n",
    "bwght = woo.dataWoo('bwght')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "nominated-equation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regress and report coefficients:\n",
    "reg = smf.ols(formula = 'bwght ~ cigs + faminc', data = bwght)\n",
    "results = reg.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "scenic-adventure",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight in pounds, manual way:\n",
    "bwght['bwght_lbs'] = bwght['bwght'] / 16\n",
    "reg_lbs = smf.ols(formula = 'bwght_lbs ~ cigs + faminc', data = bwght)\n",
    "results_lbs = reg_lbs.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "velvet-stylus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight in pounds, direct way:\n",
    "reg_lbs2 = smf.ols(formula = 'I(bwght/16) ~ cigs + faminc', data = bwght)\n",
    "results_lbs2 = reg_lbs2.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "chubby-series",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packs of cigarettes:\n",
    "reg_packs = smf.ols(formula = 'bwght ~ I(cigs / 20) + faminc', data = bwght)\n",
    "results_packs = reg_packs.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "atlantic-antibody",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compare Results: \n",
      "              No Scaling  Manual (Pounds)  Direct (Pounds)  Packs of Cigarettes\n",
      "I(cigs / 20)         NaN              NaN              NaN              -9.2682\n",
      "Intercept       116.9741           7.3109           7.3109             116.9741\n",
      "cigs             -0.4634          -0.0290          -0.0290                  NaN\n",
      "faminc            0.0928           0.0058           0.0058               0.0928\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compare results:\n",
    "table = pd.DataFrame({\"No Scaling\": round(results.params, 4),\n",
    "                     \"Manual (Pounds)\": round(results_lbs.params, 4),\n",
    "                     \"Direct (Pounds)\": round(results_lbs2.params, 4),\n",
    "                     \"Packs of Cigarettes\": round(results_packs.params, 4)})\n",
    "\n",
    "print(f'Compare Results: \\n{table}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "armed-dispute",
   "metadata": {},
   "source": [
    "#### Standardization: Beta Coefficients\n",
    "\n",
    "A specific arithmetic operation is the standardization. A variable is standardized by subtracting its mean and dividing by its standard deviation. For example, the standardized dependent variable $y$ and regressor $x_1$ are\n",
    "\n",
    "$$z_y = \\frac{y - \\bar{y}}{sd(y)}$$\n",
    "\n",
    "and\n",
    "\n",
    "$$z_{x_{1}} = \\frac{x_1 - \\bar{x_1}}{sd(x_1)}$$\n",
    "\n",
    "If the regression model only contains standardized variables, the coefficients have a special interpretation. They measure by how many *standard deviation* $y$ changes as the respective independent variable by *one standard deviation*. Inconsistent with the notation used here, they are sometimes referred to as beta coefficients.\n",
    "\n",
    "In *Python*, we can use the same type of arithmetic transformations to subtract the mean and divide by the standard deviation. It can be done more conveniently by defining and using a function **scale()** directly for all variables we want to standardize. \n",
    "\n",
    "#### Wooldridge, Example 6.1: Effects of Pollution on Housing Prices\n",
    "\n",
    "We are interested in how air pollution (nox) and other neighborhood characteristics affect the value of a house. A model using standardization for all variables is expressed in a formula as\n",
    "\n",
    "``` python\n",
    "price_sc ~ 0 + nox_sc + crime_sc + rooms_sc + dist_sc + stratio_sc\n",
    "```\n",
    "\n",
    "With **variable_sc** denoting the scaled version of **variable**. The ouptut shows the parameter estimates of this model. The housing price drops by 0.34 standard deviations as the air pollution increases by one standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "looking-revolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dependencies\n",
    "import wooldridge as woo\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "knowing-formula",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data set 'hprice2'\n",
    "hprice2 = woo.dataWoo('hprice2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "frequent-compilation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for the standardization\n",
    "def scale(x):\n",
    "    x_mean = np.mean(x)\n",
    "    x_var = np.var(x, ddof = 1)\n",
    "    x_scaled = (x - x_mean) / np.sqrt(x_var)\n",
    "    return x_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "industrial-deployment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the variables\n",
    "hprice2['price_sc'] = scale(hprice2['price'])\n",
    "hprice2['nox_sc'] = scale(hprice2['nox'])\n",
    "hprice2['crime_sc'] = scale(hprice2['crime'])\n",
    "hprice2['rooms_sc'] = scale(hprice2['rooms'])\n",
    "hprice2['dist_sc'] = scale(hprice2['dist'])\n",
    "hprice2['stratio_sc'] = scale(hprice2['stratio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "outer-young",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the regression model based on the standardized variables\n",
    "reg = smf.ols(formula = \n",
    "              'price_sc ~ 0 + nox_sc + crime_sc + rooms_sc + dist_sc + stratio_sc',\n",
    "             data = hprice2)\n",
    "results = reg.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "valid-viking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression Table: \n",
      "             Betas      SE   t-Stat  pValue\n",
      "nox_sc     -0.3404  0.0445  -7.6511     0.0\n",
      "crime_sc   -0.1433  0.0307  -4.6693     0.0\n",
      "rooms_sc    0.5139  0.0300  17.1295     0.0\n",
      "dist_sc    -0.2348  0.0430  -5.4641     0.0\n",
      "stratio_sc -0.2703  0.0299  -9.0274     0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print Regression Table\n",
    "table = pd.DataFrame({'Betas': round(results.params, 4),\n",
    "                     'SE': round(results.bse, 4),\n",
    "                     \"t-Stat\": round(results.tvalues, 4),\n",
    "                     \"pValue\": round(results.pvalues, 4)})\n",
    "\n",
    "print(f'Regression Table: \\n{table}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legendary-ordinance",
   "metadata": {},
   "source": [
    "#### Logarithms\n",
    "\n",
    "We have already seen in previous section that we can include **numpy** functions **log** directly in formulas to represent logarithmic and semi-logarithmic models. A simple example of a partially logarithmic model and its formula would be\n",
    "\n",
    "$$log(y) = \\beta_0 + \\beta_1 log(x_1) + \\beta_2 x_2 + u$$\n",
    "\n",
    "which can be expressed as **np.log(y) ~ np.log(x1) + x2**.\n",
    "\n",
    "Below script shows this again for the house price example.As the air pollution *nox* increases by *one percent*, the hous price drops by about 0.72 *percent*. As the number of rooms increases by *one*, the value of the house increases by roughly 30.6%. Wooldridge (2019, Section 6.2) discusses how the latter value is only an approximation and the actual estimated effect is (exp(0.306) - 1) = 0.358 which is 35.8%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wireless-equity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dependencies\n",
    "import wooldridge as woo\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corresponding-welcome",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data set 'hprice2'\n",
    "hprice2 = woo.dataWoo('hprice2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternative-investment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the log-log regression model\n",
    "reg = smf.ols(formula = 'np.log(price) ~ np.log(nox) + rooms', data = hprice2)\n",
    "results = reg.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quality-gravity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print regression table\n",
    "table = pd.DataFrame({'Betas': round(results.params, 4),\n",
    "                     'Standarde Errors': round(results.bse, 4),\n",
    "                     't Statistics': round(results.tvalues, 4),\n",
    "                     'p Value': round(results.pvalues, 4)})\n",
    "print(f'Regression Table: \\n{table}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coastal-romance",
   "metadata": {},
   "source": [
    "#### Quadratics and Polynomials\n",
    "\n",
    "Specifying quadratic terms or higher powers of regressors can be a useful way to make a model more flexible by allowing the partial effects or (semi-)elasticities to decrease or increase with the value of the regressor.\n",
    "\n",
    "Instead of creating additional variables containing the squared value of a regressor, in *Python* we can simply add **$I(x**2)$** to a formula. Higher order terms are specified accordingly. A simple cubic model and its corresponding formula are\n",
    "\n",
    "$$y = \\beta_0 + \\beta1 x + \\beta_2 x^2 + \\beta_3 x^3 + u$$\n",
    "\n",
    "which translates to **y ~ x + I(x**2) + I(x**3)** in formula syntax.\n",
    "\n",
    "For nonlinear model like this, it is often useful to get a graphcial illustration of the effects, which will be discussed in the later section.\n",
    "\n",
    "#### Wooldridge, Example 6.2: Effects of Pollution on Housing Prices\n",
    "\n",
    "This example of Wooldridge (2019) demonstrates the combination of logarithmic and quadratic specifications. The model for house price is\n",
    "\n",
    "$$log(price) = \\beta_0 + \\beta_1 log(nox) + \\beta_2 log(dist) + \\beta_3 rooms + \\beta_4 rooms^2 + \\beta_5 stratio + u$$\n",
    "\n",
    "In below example, we implement this model and present detailed results including *t* statistics and their *p* values. The quadratic term of *rooms* has a significantly positive coefficient $\\hat{\\beta_4}$ implying that the semi-elasticity increases with more rooms. The negative coefficient for rooms and the positive coefficient for *rooms* imply that for \"small\" number of rooms, the price decreases with the number of rooms and for \"large\" values, it increases. The number of rooms implying the smallest price can be found as\n",
    "\n",
    "$$\\text{rooms*} = -\\frac{\\beta_3}{2\\beta_4} \\approx 4.4$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lined-duration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dependencies\n",
    "import wooldridge as woo\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legislative-estimate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data set 'hprice2'\n",
    "hprice2 = woo.dataWoo('hprice2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "insured-criticism",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the regression model with quadratic term of rooms\n",
    "reg = smf.ols(formula = 'np.log(price) ~ np.log(nox) + np.log(dist) + rooms + I(rooms**2) + stratio', \n",
    "              data = hprice2)\n",
    "results = reg.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extreme-stadium",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print regresssion table\n",
    "table = pd.DataFrame({'Betas': round(results.params, 4),\n",
    "                     'Standarde Errors': round(results.bse, 4),\n",
    "                     't Statistics': round(results.tvalues, 4),\n",
    "                     'p Value': round(results.pvalues, 4)})\n",
    "print(f'Regression Table: \\n{table}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "therapeutic-dealer",
   "metadata": {},
   "source": [
    "#### Hypothesis Testing\n",
    "\n",
    "A natural question to ask is whether a regressor has additional statistically significant explanatory power in a regression model, given all the other regressors. In simple model specifications, this quation can be answered by a simple *t* test, so the results for all regresssors are availabel with a quick look at the standard regression table. When working with polynomials or other specifications, the influnece of one regressor is captured by several parameters. We can test its significance with an *F* test of the joint null hypothesis that all of these parameters are equal to zero. Let's revisit our previous example model\n",
    "\n",
    "$$log(price) = \\beta_0 + \\beta_1 log(nox) + \\beta_2 log(dist) + \\beta_3 rooms + \\beta_4 rooms^2 + \\beta_5 stratio + u$$\n",
    "\n",
    "The significance of *rooms* can be assessed with an *F* test of $H_0: \\beta_3 = \\beta_4 = 0$. As discussed, such as test can be performed with the command **f_test()** form the module **statsmodels**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optical-battle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dependencies\n",
    "import wooldridge as woo\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "filled-people",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data set 'hprice2'\n",
    "hprice2 = woo.dataWoo('hprice2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "induced-alarm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the regression model with quadratic term of rooms\n",
    "reg = smf.ols(formula = 'np.log(price) ~ np.log(nox) + np.log(dist) + rooms + I(rooms**2) + stratio', \n",
    "              data = hprice2)\n",
    "results = reg.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sublime-paintball",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement F Test for rooms\n",
    "hypotheses = ['rooms = 0', 'I(rooms ** 2) = 0']\n",
    "ftest = results.f_test(hypotheses)\n",
    "fstat = ftest.statistic[0][0]\n",
    "fpval = ftest.pvalue\n",
    "\n",
    "print(f'F Statistics: {fstat}\\n')\n",
    "print(f'F Test p-value: {fpval}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integrated-insured",
   "metadata": {},
   "source": [
    "#### Interaction Terms\n",
    "\n",
    "Models with interaction terms allow the effect of one variable $x_1$ to depend on the value of another variable $x_2$. A simple model including an interaction term would be\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1 x_1 + \\beta2 x_2 + \\beta_3 x_1 x_2 + u$$\n",
    "\n",
    "Of course, we can implement this in *Python* by defining a new variable containing the product of the two regressors. But again, a direct specification in model formula is more convenient. The expression **x1:x2** within a formula adds the interaction term $x_1 x_2$. Even more conveniently, **$x1*x2$** adds not only the interaction but also both original variables allowing for a very concise syntax. So the model can be specified in *Python* as either of the two formulas:\n",
    "\n",
    "``` python\n",
    "y ~ x1 + x2 + x1:x2\n",
    "```\n",
    "\n",
    "Or\n",
    "\n",
    "``` python\n",
    "y ~ x1*x2\n",
    "```\n",
    "\n",
    "If one variable $x_1$ is interacted with a set of other varaibles, they can be grouped by parentheses to allow for a compact syntax. For example, the shortest way to express the model equation\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1 x_1 + \\beta2 x_2 + \\beta_3 x_3 + \\beta_4 x_1 x_2 + \\beta_5 x_1 x_3 + u$$\n",
    "\n",
    "in *Python* syntax is $\n",
    "\n",
    "``` python\n",
    "y ~ x1*(x2 + x3)\n",
    "```\n",
    "\n",
    "**Wooldridge, Example 6.3: Effects of Attendance on Final Exam Performance**\n",
    "\n",
    "This example analyze a model including a standardized dependent variable, quadratic terms and an interaction. Standardized scores in the final exam are explained by class attendance, prior performance and an interaction term:\n",
    "\n",
    "$$stndfnl = \\beta_0 + \\beta_1 atnrte + \\beta2 priGPA + \\beta_3 ACT + \\beta_4 priGPA^2 + \\beta_5 ACT^2 + \\beta_6 (priGPA \\cdot atndrte) + u$$\n",
    "\n",
    "We estimate this model. The effect of attending classes is\n",
    "\n",
    "$$\\frac{\\delta stndfnl}{\\delta atndrte} = \\beta_1 + \\beta_6 priGPA$$\n",
    "\n",
    "For the average $\\bar{priGPA}$ = 2.59, we estimate this partial effect to be around 0.0078. It tests the null hypothesis that this effect is zero using a simple *F* test. With *p* value of 0.0034, this hypothesis can be rejected at all common significance level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exclusive-friday",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dependencies\n",
    "import wooldridge as woo\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "focused-ending",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data set 'attend'\n",
    "attend = woo.dataWoo('attend')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "looking-nepal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the number of observations\n",
    "n = attend.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "covered-integer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the regression model\n",
    "reg = smf.ols(formula = 'stndfnl ~ atndrte*priGPA + ACT + I(priGPA ** 2), I(ACT ** 2)',\n",
    "             data = attend)\n",
    "results = reg.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "undefined-night",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print regression table\n",
    "table = pd.DataFrame({'Betas': round(results.params, 4),\n",
    "                     'Standarde Errors': round(results.bse, 4),\n",
    "                     't Statistics': round(results.tvalues, 4),\n",
    "                     'p Value': round(results.pvalues, 4)})\n",
    "print(f'Regression Table: \\n{table}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "universal-agriculture",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate for partial effect at priGPA = 2.59\n",
    "b = results.params\n",
    "partial_effect = b['atndrte'] + 2.59 * b['atndrte:priGPA']\n",
    "print(f'Partial Effect: {partial_effect}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "embedded-cartridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F test for partial effect at priGPA = 2.59\n",
    "hypotheses = 'atndrte + 2.59 * atndrte:priGPA = 0'\n",
    "ftest = results.f_test(hypotheses)\n",
    "fstat = ftest.statistic[0][0]\n",
    "fpval = ftest.pvalue\n",
    "\n",
    "print(f'F Statistics: {fstat}\\n')\n",
    "print(f'F Test p-value: {fpval}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acceptable-pitch",
   "metadata": {},
   "source": [
    "### 2. Prediction\n",
    "\n",
    "In this section, we are concerned with predicting the value of the dependent variable *y* given certain values of the regressors $x_1, ..., x_k$. If these are the regressor values in our estimation sample, we called these predictions \"fitted values\" and discussed their calculation in the previous notebook. Now, we generalize this to arbitrary values and add standard errors, confidence intervals, and prediction intervals.\n",
    "\n",
    "#### Confidence and Prediction Intervals for Predictions\n",
    "\n",
    "Confidence intervals reflect the uncertainty about the *expected value* of the dependent variable given values of the regressors. If we are interested in predicting the college GPA of an *individual*, prediction intervals account for the additional uncertainty regarding the unobserved characteristics reflected by the error term *u*.\n",
    "\n",
    "Given a model\n",
    "\n",
    "$$y=\\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ... + \\beta_kx_k + u$$\n",
    "\n",
    "we are interested in the expected value of *y* given the regressors take specific values $c_1, c_2, ..., c_k$:\n",
    "\n",
    "$$\\theta_0 = E(y|x_1 = c_1, x_2 = c_2, ..., x_k = c_k) = \\beta_0 + \\beta_1 c_1 + \\beta_2 c_2 + ... + \\beta_k c_k$$\n",
    "\n",
    "The natural point estimates are \n",
    "\n",
    "$$\\hat{\\theta_0} = \\hat{\\beta_0} + \\hat{\\beta_1} c_1 + \\hat{\\beta_2} c_2 + ... + \\hat{\\beta_k} c_k$$\n",
    "\n",
    "and can readily be obtained once the parameter estimates $\\hat{\\beta_0}, \\hat{\\beta_1}, \\hat{\\beta_2}, ... \\hat{\\beta_k}$ are calculated.\n",
    "\n",
    "Standard errors and confidence intervals are less straightforward to compute. Wooldridge (2019, Section 6.4) suggests a smart way to obtain these for a modified regression. **statsmodels** provides an even simpler and more convenient approach.\n",
    "\n",
    "The method **predict()** automatically calculates $\\hat{theta}_0$. The method can be called on an object created by the **fit()** method. Its argument is a data frame containing the values of the regressors $c_1, c_2, ..., c_k$ of the regressors $x_1, x_2, ..., x_k$ with the same variable names as in the data frame used for estimation. If we don't have one yet, it can for example be specified with **pandas** as\n",
    "\n",
    "``` python\n",
    "pd.DataFrame({'x1':[c1], 'x2':[c2], ..., 'xk':[ck]}, index = ['newobservation1'])\n",
    "```\n",
    "\n",
    "where **x1** through **xk** are the variable names and **c1** through **ck** are the values which can also be specified as lists to get predictions at several values of the regressors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "moderate-metro",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dependencies\n",
    "import wooldridge as woo\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "exclusive-columbia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data set 'gpa2\n",
    "gpa2 = woo.dataWoo('gpa2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "answering-ceiling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the regression model\n",
    "reg = smf.ols(formula = 'colgpa ~ sat + hsperc + hsize + I(hsize**2)',\n",
    "             data = gpa2)\n",
    "results = reg.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "virgin-wealth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression Table: \n",
      "                Betas  Standarde Errors  t Statistics  p Value\n",
      "Intercept      1.4927            0.0753       19.8118   0.0000\n",
      "sat            0.0015            0.0001       22.8864   0.0000\n",
      "hsperc        -0.0139            0.0006      -24.6981   0.0000\n",
      "hsize         -0.0609            0.0165       -3.6895   0.0002\n",
      "I(hsize ** 2)  0.0055            0.0023        2.4056   0.0162\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the regression table\n",
    "table = pd.DataFrame({'Betas': round(results.params, 4),\n",
    "                     'Standarde Errors': round(results.bse, 4),\n",
    "                     't Statistics': round(results.tvalues, 4),\n",
    "                     'p Value': round(results.pvalues, 4)})\n",
    "print(f'Regression Table: \\n{table}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "favorite-gather",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Values for Prediction: \n",
      "             sat  hsperc  hsize\n",
      "newPerson1  1200      30      5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate data set containing the regressor values for predictions\n",
    "cvalues1 = pd.DataFrame({'sat':[1200], 'hsperc':[30],\n",
    "                        'hsize':[5]}, index = ['newPerson1'])\n",
    "print(f'Input Values for Prediction: \\n{cvalues1}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "lesser-ambassador",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted College GPA: \n",
      "newPerson1    2.700075\n",
      "dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Point estimate of prediction \n",
    "colgpa_pred1 = results.predict(cvalues1)\n",
    "print(f'Predicted College GPA: \\n{colgpa_pred1}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "popular-defense",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Three Sets of INput Values for Prediction: \n",
      "             sat  hsperc  hsize\n",
      "newPerson1  1200      30      5\n",
      "newPerson2   900      20      3\n",
      "newPerson3  1400       5      1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define three sets of regressor variables\n",
    "cvalues2 = pd.DataFrame({'sat':[1200, 900, 1400],\n",
    "                       'hsperc':[30, 20, 5],\n",
    "                       'hsize':[5, 3, 1]},\n",
    "                       index = ['newPerson1', 'newPerson2', 'newPerson3'])\n",
    "print(f'Three Sets of INput Values for Prediction: \\n{cvalues2}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "powerful-austria",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted College GPA: \n",
      "newPerson1    2.700075\n",
      "newPerson2    2.425282\n",
      "newPerson3    3.457448\n",
      "dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Point estimate of prediction\n",
    "colgpa_pred2 = results.predict(cvalues2)\n",
    "print(f'Predicted College GPA: \\n{colgpa_pred2}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cross-context",
   "metadata": {},
   "source": [
    "The method **get_prediction()** calculates not only $\\hat{\\theta}_0$ (i.e. the exact same predictions as method **predict()**), but also\n",
    "\n",
    "- standard errors of the predictions (column **mean_se**)\n",
    "- confidence intervals (columns **mean_ci_lower** and **mean_ci_upper**) and\n",
    "- prediction intervals (columns **obs_ci_lower** and **obs_ci_upper**). Wooldridge (2019) explains how to calculate the prediction interval manually.\n",
    "\n",
    "All you have to do is calling a second method **summary_frame()** to provide the significance level. \n",
    "\n",
    "#### Wooldridge, Example 6.5: Confidence Interval for Predicted College GPA\n",
    "\n",
    "We try to predict the college GPA, for example to support the admission decisions for our college. Our regression model equation is\n",
    "\n",
    "$$colgpa = \\beta_0 + \\beta_1 sat + \\beta_2 hsperc + \\beta_3 hsize + \\beta_4hsize^2 + u$$\n",
    "\n",
    "In this example, we show the implementation of the estimation and prediction. The estimation results are stored as the variable **results**. The values of the regressors for which we want to do the prediction are stored in the new data frame **cvalues2**. Then the commands **get_prediction()** and **summary_frame()** are called. For an SAT score of 1200, a high school percentile of 30 and a high school size of 5 (i.e. 500 students), the predicted college GPA is 2.7. Wooldridge (2019) obtains the same value using a general but more cumbersome regression approach. We define two other types of studnets with different values of *sat, hsperc,* and *hsize* in the data frame **cvalues2**.\n",
    "\n",
    "It aslo calculates the 95% and 99% confidence and prediction intervals. The object **colgpa_PICI_95** contains the 95% confidence interval, for example, which is reported in colmuns **mean_ci_lower** and **mean_ci_upper**. With 95% confidence we can say that the expected college GPA for students with the features fo the student named **newPerson1** is between 2.66 and 2.74. The object **colgpa_PICI_99** contains the 99% prediction interval, for example, which is reported in columns **obs_ci_lower** and **obs_ci_upper**. All results are the same as those manually calculated by Wooldridge (2019)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "green-involvement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dependencies\n",
    "import wooldridge as woo\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "satisfactory-basement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data set 'gpa2\n",
    "gpa2 = woo.dataWoo('gpa2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "minimal-discretion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the regression model\n",
    "reg = smf.ols(formula = 'colgpa ~ sat + hsperc + hsize + I(hsize**2)',\n",
    "             data = gpa2)\n",
    "results = reg.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "variable-render",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define three sets of regressor variables\n",
    "cvalues2 = pd.DataFrame({'sat':[1200, 900, 1400],\n",
    "                       'hsperc':[30, 20, 5],\n",
    "                       'hsize':[5, 3, 1]},\n",
    "                       index = ['newPerson1', 'newPerson2', 'newPerson3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "particular-irrigation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Table (95% confidence level): \n",
      "       mean   mean_se  mean_ci_lower  mean_ci_upper  obs_ci_lower  \\\n",
      "0  2.700075  0.019878       2.661104       2.739047      1.601749   \n",
      "1  2.425282  0.014258       2.397329       2.453235      1.327292   \n",
      "2  3.457448  0.027891       3.402766       3.512130      2.358452   \n",
      "\n",
      "   obs_ci_upper  \n",
      "0      3.798402  \n",
      "1      3.523273  \n",
      "2      4.556444  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Point estimates and 95% confidence and prediction intervals\n",
    "colgpa_PICI_95 = results.get_prediction(cvalues2).summary_frame(alpha = 0.05)\n",
    "print(f'Prediction Table (95% confidence level): \\n{colgpa_PICI_95}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "representative-computer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Table (99% confidence level): \n",
      "       mean   mean_se  mean_ci_lower  mean_ci_upper  obs_ci_lower  \\\n",
      "0  2.700075  0.019878       2.648850       2.751301      1.256386   \n",
      "1  2.425282  0.014258       2.388540       2.462025      0.982034   \n",
      "2  3.457448  0.027891       3.385572       3.529325      2.012879   \n",
      "\n",
      "   obs_ci_upper  \n",
      "0      4.143765  \n",
      "1      3.868530  \n",
      "2      4.902018  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Point estimates and 99% confidence and prediction intervals\n",
    "colgpa_PICI_99 = results.get_prediction(cvalues2).summary_frame(alpha = 0.01)\n",
    "print(f'Prediction Table (99% confidence level): \\n{colgpa_PICI_99}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revolutionary-dividend",
   "metadata": {},
   "source": [
    "#### Effect Plots for Nonlinear Specifications\n",
    "\n",
    "In models with quadratic or other nonlinear terms, the coefficients themselves are often difficult to interpret directly. We have to do additional calculations to obtain the partial effect at different values of the regressors or derive the extreme points. \n",
    "\n",
    "For a better visuzl understanding of the implications of our model, it is often useful to calculate predictions for *different values of one regressor* of interest while keeping *the other regressors fixed* at certain values like their overall sample means. By plotting the results against the regressor value, we get a very intuitive graph showing the estimated *ceteris paribus* effects of the regressor.\n",
    "\n",
    "We alreayd know how to calculate predictions and their confidence intervals. In this example, we repeat the regression from Example 6.2 and creates an effects plot for the number of rooms. The number of rooms is varied between 4 and 8 and other variables are set to their respective sample means for all predictions. The regressor values and the implied predictions are shown in a table then plotted with their confidence bands. We see the minimum at a number of rooms of around 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "corporate-horror",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dependencies\n",
    "import wooldridge as woo\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "exterior-sierra",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data set 'hprice2'\n",
    "hprice2 = woo.dataWoo('hprice2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "prepared-subsection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeating the regresssion from Example 6.2\n",
    "reg = smf.ols(formula = 'np.log(price) ~ np.log(nox) + np.log(dist) + rooms + I(rooms**2) + stratio', \n",
    "              data = hprice2)\n",
    "results = reg.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "enabling-involvement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Values for Prediction: \n",
      "   rooms       nox      dist    stratio\n",
      "0    4.0  5.549783  3.795751  18.459289\n",
      "1    5.0  5.549783  3.795751  18.459289\n",
      "2    6.0  5.549783  3.795751  18.459289\n",
      "3    7.0  5.549783  3.795751  18.459289\n",
      "4    8.0  5.549783  3.795751  18.459289\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predictions with rooms = 4 - 8, all others at the sample mean\n",
    "nox_mean = np.mean(hprice2['nox'])\n",
    "dist_mean = np.mean(hprice2['dist'])\n",
    "stratio_mean = np.mean(hprice2['stratio'])\n",
    "X = pd.DataFrame({'rooms': np.linspace(4, 8, num =5),\n",
    "                 'nox': nox_mean,\n",
    "                 'dist': dist_mean,\n",
    "                 'stratio': stratio_mean})\n",
    "print(f'Input Values for Prediction: \\n{X}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "sexual-webcam",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Values with 95% Confidence Interval: \n",
      "        mean  mean_ci_lower  mean_ci_upper\n",
      "0   9.661702       9.499811       9.823593\n",
      "1   9.676940       9.610215       9.743665\n",
      "2   9.816700       9.787055       9.846345\n",
      "3  10.080983      10.042409      10.119557\n",
      "4  10.469788      10.383361      10.556215\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate 95% confidence interval\n",
    "lpr_PICI = results.get_prediction(X).summary_frame(alpha = 0.05)\n",
    "lpr_CI = lpr_PICI[['mean', 'mean_ci_lower', 'mean_ci_upper']]\n",
    "print(f'Predicted Values with 95% Confidence Interval: \\n{lpr_CI}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "transsexual-mathematics",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEJCAYAAACOr7BbAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAABAp0lEQVR4nO3deXxU5b348c+TZJKZ7PueECBhRxYBgcqigCDu9Vrh/lppq0UrXWxtrbZ1ube11sr12lqtl7q2to2Ka11YpCKgKBIE2YQAWcgeyD7JZLbn90eSaUgmyQQyMyH5vl8vX5k555lzvnMc5jvPec7zPUprjRBCCNFVgL8DEEIIMThJghBCCOGWJAghhBBuSYIQQgjhliQIIYQQbkmCEEII4ZbXEoRS6lmlVJVS6kCnZbFKqc1Kqfz2vzE9vDZaKbVeKfWlUuqwUmqOt+IUQgjhnjd7EM8Dy7osuxvYorXOAba0P3fn98AGrfU4YApw2FtBCiGEcE95c6KcUioLeFtrPan9+RFgoda6XCmVAmzVWo/t8ppIYB8wSvczuPj4eJ2VlTUgsQshxHCQl5d3Smud4G5dkI9jSdJalwO0J4lEN21GAdXAc0qpKUAe8EOttbmvjWdlZbF79+4BDVgIIYYypVRRT+sG4yB1EDAd+JPWehpgpudTUSilViuldiuldldXV/sqRiGEGPJ8nSAq208t0f63yk2bEqBEa/1p+/P1tCUMt7TW67TWM7TWMxIS3PaShBBCnAVfJ4i3gFXtj1cBb3ZtoLWuAE4qpTrGJhYBh3wTnhBCiA5eG4NQSv0DWAjEK6VKgPuB3wIvK6VuBoqBG9rbpgJPa62Xt7/8+8DflFLBwAngW2cbh81mo6SkBIvFctbvZTgyGo2kp6djMBj8HYoQwk+8ehWTr82YMUN3HaQuKCggIiKCuLg4lFJ+iuz8orXm9OnTNDY2MnLkSH+HI4TwIqVUntZ6hrt1g3GQekBZLBZJDv2klCIuLk56XUIMc0M+QQCSHM6CHDMhxLBIEEIIMVTZ7XavbVsShJcVFhYyadKkM5Y98MADrF271qdx2Gw27r77bnJycpg0aRKzZs3ivffeA9omGJ46dcqn8Qghzp3FYiE/P5/a2lqvbN/XM6mFD2it0VoTEPDv/H/vvfdSXl7OgQMHCAkJobKykg8//NCPUQohzpXBYCAsLIywsDCvbF96EH62cOFC7rjjDubOncukSZPYtWsX0NbL+MY3vsGll15KTk4Of/7zn12veeSRR5g5cyYXXHAB999/P9DWUxk/fjy3334706dP5+TJk672zc3N/PnPf+bxxx8nJCQEgKSkJL72ta/58J0KIQaK3W7H6XQSGBhIZmYmwcHBXtnPsOtBnDhxotuyqKgo4uLicDqdFBYWdlsfExNDTEwMdrud4uLiM9aNGjXqnGMym818/PHHbNu2jW9/+9scONBWIf2LL77gk08+wWw2M23aNK644goOHDhAfn4+u3btQmvN1VdfzbZt28jMzOTIkSM899xzPPnkk2ds/9ixY2RmZhIZGXnOsQoh/Mtut1NQUEBwcDAjRozw6r6GXYLwtZ6uBuq8fOXKlQDMnz+fhoYG6urqALjmmmswmUyYTCYuueQSdu3axY4dO9i0aRPTpk0DoKmpifz8fDIzMxkxYgSzZ8/27hsSQviNw+GgqKgIq9VKSkqK1/c37BJEb7/4AwICel0fFBTU7x5DXFxctwGkmpqaMyagdU0iHc/dLddac88993Drrbeesa6wsLDH85DZ2dkUFxfT2NhIREREv+IXQgwOTqeT4uJiWlpayMzMJDw83Ov7lDEILwsPDyclJYUtW7YAbclhw4YNXHzxxa42L730EgA7duwgKiqKqKgoAN58800sFgunT59m69atzJw5k6VLl/Lss8/S1NQEQGlpKVVV7moe/ltoaCg333wzP/jBD7BarQCUl5fz4osvDvj7FUJ4R1lZGWazmfT0dJ+dLh52PQh/+Mtf/sKaNWu48847Abj//vsZPXq0a31MTAxz586loaGBZ5991rV81qxZXHHFFRQXF3PvvfeSmppKamoqhw8fZs6ctruwhoeH8+KLLxIYGNhrDL/+9a/55S9/yYQJEzAajYSFhfHf//3fXni3QghviI+PJzQ0lOjoaJ/tc8jXYjp8+DDjx4/3U0R9W7hwIWvXrmXGjDNLoTzwwAOEh4fzk5/8xE+RDf5jJ8RQp7WmqamJ8PBwr1U3GNa1mIQQ4nxVXV1NUVERjY2Nftm/nGLys61bt7pd/sADD/g0DiHE4HL69GmqqqqIjo7228Ul0oMQQohBpra2lvLyciIiIkhLS/Nb8UxJEEIIMYjYbDbKysoICwsjIyPDr5WV5RSTEEIMIgaDgREjRmAymc6op+YP0oMQQohBoKWlhYaGBqDt8vW+Ll33BUkQPuCLGY89Wbt2LePGjWPSpElMmTKFv/zlL0Db5bVdLwkWQviHxWKhsLCQiooKnE5nv15bVFTEP//5T6/EJaeYhhCHw3HGr46nnnqKzZs3s2vXLiIjI6mvr+eNN97wX4BCiG6sViuFhYUopRgxYkS/Tivl5+ezaNEiLBYLx48fH/CrnaQH4UNaa376058yadIkJk+e7Cqxcfvtt/PWW28BcN111/Htb38bgGeeeYZf/vKXALz44ovMmjWLqVOncuutt+JwOIC23sl9993HRRddxM6dO8/Y329+8xuefPJJ17T8qKgoVq1a5ZP3KoTom91up7CwEKfTSVZWlqscvycOHjzI/PnzaWlpYePGjV65FHbY9SA++OCDbssyMjLIzs7Gbrezffv2buuzsrIYOXIkra2tfPzxx2esu+SSSzze92uvvcbevXvZt28fp06dYubMmcyfP5/58+ezfft2rr76akpLSykvLwfaajOtWLGCw4cP89JLL/HRRx9hMBi4/fbb+dvf/sZNN92E2Wxm0qRJ3cpmNDY20tjYeEZJDyHE4FJbW4vNZmPkyJEYjUaPX7dnzx4uu+wygoOD+fDDD5kwYYJX4ht2CcKfduzYwcqVKwkMDCQpKYkFCxbw2WefMW/ePB577DEOHTrEhAkTXNdA79y5kz/84Q+88MIL5OXlMXPmTKBtMCsxMRGAwMBArr/++m770lr79fI4IUTf4uPjiYyM7FfPYefOnVx++eVERUWxZcsWsrOzvRbfsEsQvf3iDwoK6nV9SEhIv3oMXfVU9yotLY3a2lo2bNjA/Pnzqamp4eWXXyY8PJyIiAi01qxatYqHHnqo22uNRqPbqx0iIyMJCwvjxIkTA3JTIyHEwHA6nZSVlREfH4/RaOxXcvjggw+46qqrSElJ4f333/f6DYNkDMKH5s+fz0svvYTD4aC6uppt27Yxa9YsAObMmcNjjz3G/PnzmTdvHmvXrmXevHkALFq0iPXr17vKetfU1FBUVNTn/u655x7WrFnjunSuoaGBdevWeendCSH6orWmpKSEuro6Wlpa+vXad999l+XLlzNixAi2bdvm9eQAw7AH4U/XXXcdO3fuZMqUKSil+N3vfkdycjIA8+bNY9OmTWRnZzNixAhqampcCWLChAn8+te/5rLLLsPpdGIwGHjiiSf6/IB897vfpampiZkzZ2IwGDAYDK6S40II39JaU1ZWRkNDA8nJycTExHj82ldffZWVK1cyadIkNm3aRHx8vBcj/Tcp9y16JMdOiIFTUVHBqVOniI+Pd/0w9MSLL77IN7/5TWbNmsW777474PeDkHLfQgjhR06nk+bmZmJjY0lKSvL4devWreOmm25i/vz5bNq0yac3CwI5xSSEEF6ltSYgIICsrCyUUh5fXfi///u//PjHP2b58uWsX78ek8nk5Ui781oPQin1rFKqSil1oNOyWKXUZqVUfvvfHk/CKaUClVKfK6XePtdYhtJpNF+RYybEuauvr6egoACHw0FAQIDHyeHBBx/kxz/+Mddffz2vv/66X5IDePcU0/PAsi7L7ga2aK1zgC3tz3vyQ+DwuQZhNBo5ffq0fOH1g9aa06dP92vijhDiTI2NjZSUlAB4nBi01vz85z/nl7/8JV//+tfJzc0lODjYm2H2ymunmLTW25RSWV0WXwMsbH/8ArAV+FnX1yql0oErgAeBH59LHOnp6ZSUlFBdXX0umxl2jEYj6enp/g5DiPOS2WymuLiYkJAQj+srOZ1OfvSjH/GHP/yB1atX86c//cnv5b59PQaRpLUuB9BalyulEnto9xhwF9BncRGl1GpgNUBmZma39QaDgZEjR55tvEII0S8tLS0UFRVhMBjIysryqGy3w+Hg1ltv5ZlnnuGOO+7g0UcfHRSVEAbdVUxKqSuBKq11nifttdbrtNYztNYzEhISvBydEEL0LiAgAKPRSFZWFkFBff8Gt9ls3HTTTa7inIMlOYDvexCVSqmU9t5DClDlps1XgKuVUssBIxCplHpRa/11n0YqhBD90DEQHRISwsiRIz36km9tbWXFihW88cYbPPTQQ9x9d2/Dsr7n6x7EW0BHvelVwJtdG2it79Fap2uts4AVwL8kOQghBjO73c6JEyeoqKgAPBuUbm5u5pprruGNN97g97///VklB5vNxtatW13ldAaaNy9z/QewExirlCpRSt0M/BZYopTKB5a0P0cplaqUetdbsQghhLc4HA6KioqwWq2ue6/0pbGxkcsvv5xNmzbx9NNP84Mf/OCs9h0UFER0dHS/6zp5asiX2hBCCG9xOp0UFRVhNpvJzMz0KEHU1tZy+eWXs3v3bv7617+ycuXKfu/35MmTREZGEhUVdTZhn0FKbQghhBeUlJRgNptJT0/3KDlUVVVxySWX8Pnnn7N+/fp+JwetNQcPHmTnzp0cPnzO08T6JKU2hBDiLMXExBAWFuZRjaTS0lIWL15MYWEhb731FkuXLu3Xvux2O5999hknT55kxIgRzJjh9kf/gJIEIYQQ/aC1xmKxYDKZPL4PdGFhIYsWLaKqqooNGzawYMGCfu2ztbWV7du3U1NTw+TJkxk3bpxPLoWVBCGEEP1QVVVFdXU1o0aNIjQ0tM/2R48eZfHixTQ2NvL+++9z0UUX9XufQUFBBAcHM3fuXJ9WOJAEIYQQHjp16hTV1dVER0d7VEDvwIEDLF68GIfDwQcffMDUqVP7tb/y8nJiY2MJCQlh3rx5Pp9AJ4PUQgjhgdraWioqKoiMjCQtLa3PL+u8vDwWLFhAQEAA27Zt61dy0Frz5Zdfsn37dg4dOgR4XvBvIEkPQggh+mCxWCgtLSUsLIz09PQ+v6w/+ugjli9fTkxMDFu2bGH06NEe78vhcJCXl0dhYSEZGRlMnjz5XMM/a5IghBCiDyEhIaSlpREZGdlnhdUtW7Zw9dVXk5aWxpYtW8jIyPB4P62trXz00UecOnWKCRMmMHHiRL/WZZIEIYQQPWhubnYV34uJ6fH+Zi7vvPMO119/PTk5OWzevLlf956GtlNLra2tzJ492211al+TMQghhHDDYrFQVFRESUmJRzcce+WVV7j22muZNGkSW7du7VdyOHXqFE6nE6PRyNKlSwdFcgBJEEII0Y3VaqWwsBClFBkZGX2e5vnLX/7CihUrmDVrFlu2bCEuLs6j/WitOXr0KB988AFHjhwB8PtNgjobPJEIIcQgYLPZKCwsRGtNVlYWISEhvbZ/6qmnWLVqFZdccgmbNm3yuD6S0+kkLy+PvXv3kpKSQnZ29kCEP6BkDEIIITqprq7GbreTlZXV533ZH330Ue68806uuOIK1q9f7/F93K1WKx9//DFVVVWMGzeOyZMnD5qbBHUmCUIIITpJTk4mJiam14lwWmt+9atfcf/99/Mf//Ef/O1vfyM4ONjjfZjNZurq6pg1axZZWVkDELV3yCkmIcSw53Q6KS8vx263ExAQ0GdyuPvuu7n//vu56aab+Mc//uFxcmhsbATaivwtX758UCcHkAQhhBjmtNaUlJRw+vRpzGZzr22dTic/+MEP+N3vfsdtt93Gc88959F9pwGOHz/Ohg0bKC4uBuhXj8Nf5BSTEGLY0lpTVlZGQ0MDycnJvQ4wOxwOvvOd7/Dcc89x55138sgjj3g0buB0Otm3bx/5+fkkJyeTkpIykG/BqyRBCCGGrcrKSmpra0lISCA+Pr7HdjabjW984xu89NJL3HfffTzwwAMeJQebzcbOnTupqKggJyeHKVOmDKrLWPsiCUIIMSw5HA7q6+uJjY0lMTGxx3YWi4Ubb7yRt956i4cffpi77rrL431UV1dTVVXFhRde2K96TIOFJAghxLAUGBjI6NGjCQwM7LE30NzczLXXXsvmzZv54x//yJo1azzadmtrKyEhIaSmpnL55ZcTFhY2kKH7zPnT1xFCiAFQX1/vKp8RFBTUY3JoaGhg2bJlbNmyhWeffdbj5FBYWMg777zDqVOnAM7b5ADSgxBCDCONjY2cPHmS0NBQtNY9JoeamhqWLVvG559/zt///nduvPHGPrettWb//v18+eWXJCYmenw70sFMEoQQYlgwm80UFxdjNBoZMWJEj4PFVVVVLFmyhC+//JJXX32Vq6++us9t22w2du3aRWlpKaNGjWL69Onn1WB0TyRBCCGGvJaWFoqKijAYDGRlZREYGOi2XWlpKYsWLaK4uJi3336bJUuWeLT9wsJCysrKmDp1Kjk5OYOybMbZkAQhhBjyHA4HBoOBESNG9DixraCggEWLFnHq1Ck2btzIvHnzPNpuYGAg2dnZxMXFERsbO9Ch+9X53wcSQogeOJ1OAMLDw8nOzu5x9vKRI0eYN28edXV1vP/++x4lh+LiYt577z3MZjNKqSGXHEAShBBiiLLb7Rw/fpyamhqAHk/7fPHFF8yfPx+bzcbWrVuZNWtWr9vVWnPgwAE++eQTQkNDezxdNRTIKSYhxJDjcDgoLCzEarX2ej+Hzz77jKVLlxIaGsr777/PuHHjet2u3W7ns88+4+TJk2RlZXHhhRcO6QThtR6EUupZpVSVUupAp2WxSqnNSqn89r/dbvKqlMpQSn2glDqslDqolPqht2IUQgw9TqeToqIiLBYLmZmZPc5D2LFjB4sWLSIqKort27f3mRwADh06xMmTJ7nggguYOXPmkE4O4N1TTM8Dy7osuxvYorXOAba0P+/KDtyptR4PzAbWKKUmeDFOIcQQobXm5MmTNDc3k56e3uNchM2bN3PZZZeRkpLC9u3bGTlyZJ/bBRg/fjzz589n3LhxQ+ZKpd54LUForbcBNV0WXwO80P74BeBaN68r11rvaX/cCBwG0rwVpxBi6FBKERYWRkpKCtHR0W7b/POf/+TKK68kOzubbdu2kZ6e3us2S0tL+eCDD7Db7RgMBpKTk70Q+eDk60HqJK11ObQlAqDnClmAUioLmAZ86v3QhBDnK601VqsVgPj4eOLi4ty2e/nll/nqV7/KlClT2Lp1K0lJSb1u8/Dhw3z00Uc4nU7sdrtXYh/MBu1VTEqpcOBV4A6tdUMv7VYrpXYrpXZXV1f7LkAhxKBRVVXFsWPHaG1t7bHN888/z8qVK5k9ezbvv/9+r5elOhwOdu3axf79+8nMzGThwoUe3296KPF1gqhUSqUAtP+tctdIKWWgLTn8TWv9Wm8b1Fqv01rP0FrPSEhIGPCAhRCD26lTp6iuriYqKqrHeQ5PPvkk3/rWt7j00kvZsGEDkZGRvW5zz549FBUVMXHiRC666CKP7xo31Pg6QbwFrGp/vAp4s2sD1Tby8wxwWGv9qA9jE0KcZ2pra6moqCAyMpLU1FS3A8ePPPIIa9as4aqrruKf//ynR9VVx48fz5w5c5g4ceKwGIzuiTcvc/0HsBMYq5QqUUrdDPwWWKKUygeWtD9HKZWqlHq3/aVfAb4BXKqU2tv+33JvxSmEOD+ZzWZKS0sJCwsjPT292xe51poHHniAu+66ixtvvJFXX32119NE5eXlfPbZZ2itCQ8PJyMjw9tvYdDzWr9Ja72yh1WL3LQtA5a3P94BDN+ULYTwSGhoKImJicTFxXWrnKq15q677mLt2rV885vf5Omnn+5xzoLWmqNHj/LFF18QFRWFzWbr8VTVcDM8T6wJIc5bLS0tGAwGgoKC3N4q1Ol08r3vfY8//elP3H777Tz++OM9lt52OBx8/vnnnDhxgrS0tGE93uCOHAkhxHnDYrFQWFiIyWQiKyur23q73c4tt9zCCy+8wE9/+lMefvjhXscQPv30U0pKShg/fjyTJk0a1uMN7kiCEEKcF6xWK4WFhSilSE1Ndbv+61//Oq+88gr/9V//xb333tvnF352djapqaluk42QBCGEOA/YbDYKCgrQWjNy5MhuYwQWi4UbbriBt99+m7Vr13LnnXf2uK3Kykrq6uoYO3as21NU4t8kQQghBr3y8nIcDgdZWVndrkQym81cc801bNmyhSeffJLvfve7PW7n2LFjfP7550RGRpKdnT3ki+2dK0kQQohBLzU1ldbWVkJDQ89YXl9fzxVXXMHOnTt5/vnnWbVqldvXO51O9u3bR35+PikpKcyePVuSgwckQQghBiWn08np06eJi4sjKCio29VFp0+fZtmyZezdu5fc3FxuuOEGt9vRWvPxxx9TVlbGmDFjuOCCC3q8qkmcSRKEEGLQ0VpTUlJCQ0MDRqOxW9nuiooKlixZQn5+Pq+//jpXXnllj9tSSpGSkkJKSgqjR4/2duhDiiQIIcSgorWmtLSUhoYGkpOTuyWHkpISFi1aRElJCW+//TaLFy92u53q6mpsNhupqamSGM6SJAghxKDhcDgoLy+nrq6OhIQE4uPjz1h/4sQJFi1aRE1NDRs3buTiiy92u52CggLy8vKIiooiJSVF5jecJUkQQohBw+Fw0NDQQEJCQrdLUL/88ksWLVqExWJhy5YtzJgxo9vrnU4n+/fv58iRIyQlJTFnzhxJDudAEoQQwq+cTid1dXXExMQQHBzMmDFjug1I79u3jyVLlhAQEMDWrVuZPHlyt+04HA527txJWVkZo0ePZtq0aTIYfY4kQQgh/KajIqvVasVoNBIaGtotOezatYulS5cSHh7Oli1bGDNmjNttBQQEEBoayrRp08jJyfFF+EOeJAghhM85nU4qKys5ffo0BoOBrKysbnMcALZt28aVV15JQkICW7ZscVsS4/Tp0wQFBREVFcX06dN9EP3wIf0vIYTPFRUVcfr0aWJjY8nOziY8PLxbm02bNrFs2TLS0tLYtm2b2+RQVFTEBx98wOeff+6DqIcfj3sQSikTkKm1PuLFeIQQQ5TT6UQphVKKhIQEEhIS3CYGgDfffJOvfe1rjB8/nk2bNnUbsNZac/DgQQ4dOkRCQgJz5szxxVsYdjzqQSilrgL2Ahvan09VSr3lxbiEEENIS0sLx48fp7q6GoDw8HC3yUFrzTPPPMP111/P1KlT+de//tUtOdjtdnbu3MmhQ4cYOXIk8+fPJyQkxCfvY7jxtAfxADAL2Aqgtd6rlMryTkhCiKHC6XRSXV1NdXU1QUFBmEymHtsWFxdz22238d5773HJJZfwxhtvEBkZ2a2dUgqr1cqUKVMYM2aMXMbqRZ4mCLvWul7+RwghPGWxWDh58iStra1ER0eTkpLitkCe0+nkqaee4mc/+xlOp5Pf//73rFmzplvb2tpaQkNDCQkJYf78+XIJqw94miAOKKX+EwhUSuUAPwA+9l5YQojzndYap9NJZmam254AwNGjR7nlllvYvn07ixcvZt26dYwcObJbu5KSEj799FPS0tKYPXu2JAcf8fQofx+YCLQCfwfqgTu8FJMQ4jzV0tLiGmcwmUzk5OS4TQ52u53f/e53TJkyhf379/Pss8+yadOmbslBa82hQ4f4+OOPiY6OZsqUKT55H6KNRz0IrXUz8Iv2/4QQ4gxaa9dYQ2BgIDExMQQFBbn9pb9v3z6+/e1vs2fPHq677jqeeOIJUlJSurVzOBzs3r2boqIiMjMzmTlzptzDwcc8vYpps1IqutPzGKXURq9FJYQ4b1gsFo4fP05VVZXrTm1dZ0MDtLa2cu+99zJjxgxKSkp45ZVXePXVV90mB2i7zeipU6eYNGkSF110kSQHP/B0DCJea13X8URrXauUkpu5CjHMOZ1OCgoKAMjIyCAqKsptu507d3LzzTdz+PBhbrrpJh599FHi4uLctm1oaCA8PByj0chll12GwWDwWvyid56OQTiVUpkdT5RSIwDtnZCEEIOd1WpFa01AQAAZGRnk5OS4TQ5NTU3ccccdfOUrX8FsNvPee+/xwgsvuE0Odrud/fv3s3HjRg4cOAAgycHPPO1B/ALYoZT6sP35fGC1d0ISQgxWWmtOnz5NZWUlqampxMTE9DgbevPmzaxevZrCwkLWrFnDQw891O3mPx0qKyvJy8ujqamJrKwsxo4d6823ITzk6SD1BqXUdGA2oIAfaa1PeTUyIcSg0traSmlpKc3NzURERPSYGGpra/nJT37Cs88+y5gxY9i2bRvz5s3rcbuHDx9m//79hIeHs3Dhwm4zp4X/9JoglFLjtNZfticHgLL2v5lKqUyt9R7vhieEGAxqa2spKytDKUV6ejpRUVFuZzC//vrr3H777VRXV3P33Xdz//33YzQau7XTWuNwOAgKCiI1NRWHw8H48eNlIHqQ6asH8WPaTiX9j5t1Gri0pxcqpZ4FrgSqtNaT2pfFAi8BWUAh8DWtda2b1y4Dfg8EAk9rrX/b1xsRQnhPUFAQ4eHhpKamuh0XqKys5Pvf/z6vvPIKU6dO5Z133umx9HZDQwN5eXkYjUbmzJlDVFRUj4Pbwr96HaTWWq9WSgUAv9RaX9Llvx6TQ7vngWVdlt0NbNFa5wBb2p+fQSkVCDwBXA5MAFYqpSZ49naEEAOhY6yhY9JbREQEmZmZ3ZKD1pq//vWvTJgwgTfffJMHH3yQXbt2uU0ODoeDgwcPsmnTJurq6khMTERrudZlMOtzDEJr7VRKrQX6VU9Xa73NTUG/a4CF7Y9foK3438+6tJkFHNNanwBQSuW2v+5Qf/YvhDg7VquV0tJSzGYzERERaK1dZbo761xcb86cOTzzzDOMHz/e7Tbr6urYuXMnjY2NZGZmMnXqVLennsTg4ulVTJuUUtcDr+lzS/lJWutyAK11eQ9zKdKAk52elwAXncM+hRAe0FpTW1tLRUUFgOsqpa6JoXNxPa01f/jDH7j99tt7HT8wGo0EBQUxb968HifGicHH0wTxYyAMcCilWmi7kklrrd1X4Do37krG9piUlFKrab/kNjMzs6dmQog+WK1WysrKCAsLIy0tjeDg4G5tOhfXW7JkCevWrXN7pzetNcXFxZSUlDB37lyMRiOLFy+W0tznGY8mymmtI7TWAVprg9Y6sv352SSHSqVUCkD73yo3bUqAjE7P0/n31VPuYluntZ6htZ6RkJBwFiEJMXxprTGbzQCEhIQwevRosrKyuiUHu93Oww8/zAUXXMD+/ft57rnn2Lhxo9vk0NjYyLZt2/j0009paWmhtbUVQJLDeag/txz9KnAxbb/mt2ut3ziL/b0FrAJ+2/73TTdtPgNylFIjgVJgBfCfZ7EvIUQvbDYbpaWlNDU1MXLkSMLCwtze0Kc/xfWOHDnC4cOHUUoxbdo0Ro8eLaW5z2MeJQil1JNANvCP9kW3KaWWaK3X9PKaf9A2IB2vlCoB7qctMbyslLoZKAZuaG+bStvlrMu11nal1PeAjbRd5vqs1vrgWb07IUQ3Wmvq6uooLy9Ha01KSgqhoaHd2lksFn7961/z8MMPExcXx/r167n++ut73W5BQQEpKSlMnTrV7TbF+UV5MuaslDoITOoYoG6/9HW/1nqil+PrlxkzZujdu3f7OwwhBrWTJ09SX19PaGgoaWlpbu/n/PHHH3PzzTfz5ZdfsmrVKh599FFiY2O7tbNarRw5coTx48cTFBREa2ur3B/6PKOUytNaz3C3ztNTTEeATKCo/XkG8MUAxCaE8IGOH4JKKcLDwzGZTMTFxXUbF2hqauIXv/gFjz/+OBkZGWzYsIGlS5e63V5JSQmff/45ra2txMXFkZqaKslhiPE0QcQBh5VSu9qfzwR2KqXeAtBaX+2N4IQQ585ms1FWVkZERASxsbHExMS4bde5uN73vvc9fvOb37gtrmc2m9mzZw/l5eXExMRw8cUXu+1diPOfpwniPq9GIYQYcFpr6uvrKS8vx+l0elxcb/v27Vx88cU9bnf37t2cPn2aKVOmkJOTI4PQQ5in1Vw/7LuVEGKwsNvtlJWV0dDQgMlkIj093e3pH0+L69XU1GAymTCZTEyfPp2AgADCwsJ88VaEH/VVzbUR95PUvDlRTghxjlpaWmhsbCQpKYn4+PhuYw2eFtez2Wzs37+fY8eOMXr0aC688MIe7+kghp5eE4TWWj4JQpwn7HY7ZrOZqKgoIiIiGDNmTI/F9e644w7MZjMPPvggP/3pT91WaC0tLWXPnj20tLQwevRoJk+e7Ku3IgYJjyfKCSEGr4aGBkpLS3E6nYSFhREUFNTtS7+4uJhbb72VDRs2MHfuXJ555hnGjRvndntHjx5l7969REVFMXfu3B7vHy2GNkkQQpzH7HY75eXl1NfXYzQaSU9PJyjozH/WTqeTP/3pT9x9992u4npr1qzpNrjsdDqxWq0YjUYyMzPRWssg9DAnCUKI85TT6eT48ePYbDYSExNJSEjoNtZw5MgRbrnlFnbs2NFrcb3a2lry8vJQSnHppZdiNBrlvtBCEoQQ5xun00lAQAABAQEkJCS4ri7qzG63s3btWh544AFMJhPPPfccq1at6pZA7HY7Bw8e5OjRowQHBzNt2jRfvhUxyEmCEOI80tjYSGlpKWlpaa6Jb13t3buXm2++mT179vDVr36VJ554guTk5G7t6uvr2b59O83NzYwcOZILLrhAZkKLM0iCEOI84HA4qKiooLa2lpCQkG7jDOB5cb2OO8SFhYURFRXFRRddhJTKF+5IghBikGtqaqK0tBSbzUZ8fDyJiYndBo49Ka6ntebEiROcOHGCSy65xHWHNyF6IglCiEHOarWilGLUqFHdSmh7Wlyvvr7eVSIjMTERm83mthciRGfyCRFiEGpqasLhcBAVFUVMTAzR0dHdeg2eFNdzOp0cPHiQI0eOEBQUxKxZsxgxYoTc3U14RBKEEIOIw+GgsrKSmpoajEYjkZGRKKXO+EKvra3lzjvv5LnnnmPs2LG9FtdTSlFdXU1mZiZTpkyRQWjRL5IghBgkzGYzJSUl2Gw24uLiSEpK6vZLv3NxvXvuuYf77ruvW3E9i8XCgQMHmDhxIiaTiQULFhAYGOjLtyKGCEkQQgwCra2tFBQUYDAYXPeH7qyiooLvf//7rF+/vsfielprCgsL2bdvH3a7naSkJDIyMiQ5iLMmCUIIP7LZbBgMBkJCQkhPTyciIuKML/TOxfWam5v5zW9+w09+8pNudZYaGhrIy8ujurqa+Ph4LrzwQqKionz9dsQQIwlCCD9wOp2usYZRo0ZhMpmIjo4+o01RURG33norGzdu7LO43uHDh6mrq+PCCy9k1KhRMggtBoQkCCF8rLm5mZKSEqxWK7GxsQQHB5+xvmtxvccff5zbb7+921VM1dXVBAcHExUVxZQpU7jgggu6ldwQ4lxIghDChyorK6mursZgMJCVldXtNqCdi+tddtll/N///V+34nqtra188cUXFBQUkJGRwZw5c9zeBU6IcyUJQggfi4mJITk5+Yyxhs7F9UJDQ3n++ee56aabzjhVpLWmuLiYvXv3YrVaGTt2LBMnTvTHWxDDhCQIIbyk4w5vTU1NREZGEhERQWJiYrfxgc7F9a6//nr++Mc/ui2uV1BQwO7du4mNjWXBggXdxiyEGGiSIIQYQFprKioqaGpqorW1FYCAgACCgoKIiIg4IzlYLBZ+9atf8fDDDxMfH++2uJ7T6cRsNhMREUFmZiYAWVlZchMf4ROSIIQ4Sw6HA7PZjNlsRmtNamoqSilaWlowGAxER0cTFhaGyWTq1mvwpLjeqVOnyMvLw263s2zZMoKCghg1apQv36IY5iRBCNFPNTU11NTUYLFYgLZyFuHh4a4y2iNHjuzxMtOmpiZ+/vOf88c//rHH4npWq5X9+/dz/PhxTCYT06dPl8luwi8kQdDWjW9qasJkMhEUFCTXkAugrYfQ3Nzs6iVkZWURGBiIw+EgMDCQxMREVw+h8ymfnj4/mzZtYvXq1RQXF7NmzRq3xfWampr417/+RWtrKzk5OUyaNKnbpDghfEUSBNDS0kJxcTEAQUFBGI1GTCYTMTEx3a5RF0Of2WymoqKClpYWoO0L32QyYbfbCQwMJCEhoV832KmpqeHOO+/k+eefZ+zYsWzbtq1bcb2OpBMWFkZqaiqjRo1ye7c4IXzJLwlCKfVD4DuAAv6stX6sy/oo4EUgk7YY12qtn/NWPCaTiZEjR2KxWGhpaaGlpYWmpiYiIiIIDg6moaHBVV2z4/6/BoNBehrnuY4B4I7/4uPjiYqKIjAwEKUUCQkJhIWFERoa2u9B4ZaWFt59911yc3N5++23sdlsbovrOZ1O8vPzOXr0KIsXL8ZkMjFjxoyBfqtCnBWfJwil1CTaksMswApsUEq9o7XO79RsDXBIa32VUioBOKKU+pvW2uqNmAICAggLCzujQJrT6XQlAK01drudU6dOnfGaMWPGEBQU5LpaJTg4WJLGINYxRuBwOCgqKqKlpQWtNcAZA8lGo/GsBoOtViubN28mNzeXN954g6amJhITE7nllltYvXo1kydPPqN9TU0Nu3fvpq6ujpSUFFcsQgwW/uhBjAc+0Vo3AyilPgSuA37XqY0GIlTbv9hwoAaw+zLIzr8Yo6KiiIqKwul00traSktLC62tra6Bw+rqaurq6ggICDijlyHXqfuX0+k8YwwhODiY9PR0AgICCAwMJC4uztVDONtBYIfDwdatW8nNzeXVV1+ltraWmJgYVq5cyY033siCBQu63blNa83evXs5duwYISEhzJkzh/T0dPlxIQYdfySIA8CDSqk4oAVYDuzu0uaPwFtAGRAB3Ki1dvo0SjcCAgJcX/6ddZyKaGlpwWKxUFNTQ3BwsCtBlJeXo7XGZDJhNBoxGo3yZeAFHT0EgNLSUurq6s7oIXTcLEcpxYgRI856P06nk507d5Kbm8srr7xCZWUl4eHhXHfddaxYsYLFixf3OnallMJqtTJq1CgmT54s41xi0PJ5gtBaH1ZKPQxsBpqAfXTvHSwF9gKXAqOBzUqp7Vrrhq7bU0qtBlYDrolEvhYSEkJISAgxMTHAv09JdbDZbDQ1NVFTUwO0fUHExMSQmpoKtE2YCg4OlslP/eR0OmlpaXH1EFpbWxk7dixKKYKDg4mNjXWdOjzXy0S11uzZs4fc3FxeeuklTp48idFo5Morr2TlypVcfvnlvRbKa25uZu/evUycOJGoqChmzZolPxLEoOeXQWqt9TPAMwBKqd8AJV2afAv4rW77+XdMKVUAjAN2udnWOmAdwIwZMwbFSVyl1BmXJmZmZqK1xmq1unoZHb8anU4nx44dA3D1LkwmE+Hh4XJ7yC46egNKKWpqalw9M2g7dh2nATuuNBoIBw8eJDc3l9zcXI4dO4bBYGDp0qU89NBDXH311d0uU+3KarVy4sQJDh06hNaatLQ0oqKiJDmI84K/rmJK1FpXKaUyga8Cc7o0KQYWAduVUknAWOCEj8McUEopV0+jq4yMDFfiaGxspK6ujuTkZEJCQrDZbFRWVroSh9FoHDaTprTWZ/QQmpubyczMJDw8HKPRSExMDOHh4YSGhnY7z38ujh07xksvvURubi4HDhwgICCARYsWcc8993Dddde5eop9OXDgAEePHsVut5OcnMz06dO7VW8VYjDz1zyIV9vHIGzAGq11rVLqNgCt9VPAr4DnlVL7absU9mda61M9b+78FRAQ4BoEh7YvRZvN5jrd1HF6qq6uzvWa4OBg0tLSCAsLw+FwAAyJpKG1dvUAWltbOX78OE5n29BTSEgI0dHRrkQQGhpKaGjogO375MmTvPzyy+Tm5rJ7d9uQ2Lx583jiiSe4/vrrSUpK8ij+6upqEhISUEqhlCI9PZ2cnByPk4oQg4kaSpfWzZgxQ3f84x5qbDbbGfM0OnoYNTU1lJWVYTAYXAPoRqORsLCwQT+mobXGYrGcMRchJibGdclneXm5awxhIHsIHSorK1m/fj25ubns2LEDgJkzZ7JixQpuuOEGMjIyPNqO3W6nqKiI/Px8GhoauPjii13jS0IMdkqpPK2128k3MpP6PGEwGDAYDN3OeYeGhpKUlORKHA0NbeP448ePB6C+vp7W1lZX4vBn2YaOwfuOGI4dO3bGHJKoqCjXKRillFe+ZGtqanj99dfJzc3lX//6F06nk8mTJ/Pggw9y4403Mnr0aI+3ZbPZOHToEAUFBVitVmJiYpg1a5ZHvQ0hzgeSIM5zHQPbHRwOBxaLxXXKyWw2u66egrZSImFhYa5fxw6Hg4CAAK8MmmqtaW1tPaOHEBAQwNixYwGIi4tzTVL0ZuJqbGzkrbfeIjc3l40bN2Kz2cjOzuYXv/gFN954Y79uutPR6zGZTAQGBlJaWkpiYiI5OTnEx8fL4LMYUiRBDDEd9Xw6pKamkpSU5Do9ZbFYzpixW1hYiNVqdfUwOk5Tnc21+R0JISQkBKUU5eXlruTU0fsJCwtzzVfwZq2hrqUuLBYLGRkZ3HHHHaxYsYJp06b168vc4XBQUlLC0aNHaWlp4YorriAwMJDLLrvMK6e/hBgM5JM9DHQkjc6Jo0NMTAzNzc1YLBZXKZHIyEjXnJKqqipCQkIwGo3dSol0XLrbuYdgt9vJzs7GaDQSHR2NyWQiLCzMJ5PB3JW6SEpK4jvf+Q4rVqxg9uzZ/R6XsVgsHD9+nOPHj2OxWIiIiGDChAmu9ZIcxFAmn+5hLjY21vVLvqOUSAeHw0F1dbWrx9FRSiQuLo6oqCiampooKioC/n3qqvOA8kBfaeSO3W5n69atvPTSS91KXaxYsYIFCxac1RVeTqeTgIAAGhoaOHjwIMnJyeTk5JCcnCynkcSwIQlCuHSUEukQGBjI+PHjXfWnOk5TdSSM0NBQUlNTXT0EX31xOp1OPv74Y1epi6qqKiIiIrj22ms9KnXR23bLy8s5evQo0dHRTJs2jYSEBC6//PI+J8QJMRRJghC96qn+FLQlEF/ds0BrTV5enqvURUlJCUajkauuuooVK1b0WeqiN1arlYKCAo4dO4bZbCY0NNQ1iK+UkuQghi1JEGJQO3DggKvUxfHjxzEYDCxbtoyHH36Yq666akC+vL/44gtOnDhBfHw8U6ZMITU1ddDPIRHCFyRBiEHn2LFjrqRw8OBBV6mLn//85/0qdeGO1prKykry8/OZOHEisbGxjBs3jtGjR8tsZyG6kAQhBoXi4mJXqYu8vDyg/6UuetN1tnNISAjNzc3ExsZKfSQheiAJQvhNZWUlr7zyCrm5uXz00UdAW6mL//mf/+lXqYu+aK3ZuHEjZrOZ6OhoZs2aRUZGxpCoXyWEN0mCED5VU1PDa6+9Rm5uLh988ME5lbroidaa06dPc/LkSaZOnYpSikmTJhEaGiqznYXoB0kQwusaGxt58803XaUuOibTnU2pi950nu1cW1uLwWAgOzubiIiIc7qDnBDDlSQI4RUtLS2888475Obm8s4777hKXfzoRz86q1IXfamvr+fDDz90zXaePn06WVlZMtNZiHMg/3rEgLFarWzatInc3FzefPPNASl10Zu6ujqam5tJTU0lIiKCxMRERowYIbOdhRggkiDEOekodZGbm8trr71GbW0tsbGx51zqoiedZztXV1cTERFBSkoKAQEBzJ49e8D2I4SQBCHOgrdKXfSltLSUvXv3umY7X3DBBYwaNUp6C0J4iSSIIaSj3HZLSwvNzc2uv50fe7Kur2XNzc2umxBdeeWV51zqojeNjY0EBQW57r9gMplktrMQPiIJwgdsNpvXv7Q7/p7NLWQDAgIIDQ3FZDK5KrB2PI6IiCApKemMZSaTialTpw5YqYuuOs92Li8vZ+zYsUyZMoWkpCSSk5MHfH9CCPckQdA2i9ebX9p2u/2s4uooktf1S9tkMhETE9Ntmbt2PS3rvM5gMAya0zQnTpzg6NGjrtnOEyZMcM2NGCwxCjFcSIIAxo4di8Vi8ahtcHBwj1+4CQkJ5/Sl3fmx0WgcNl+IFovFddvUqqoqAgICZLazEIOAJAhg3bp1BAUFefRFLl9YA6NjtnN+fj4lJSUsWbKE6OhoZsyYQWBg4LBJjkIMZpIggG984xv+DmHYcDqdnDx5kvz8fGpqajAYDIwZM4aQkBBAbuEpxGAi/xqFT2itUUpht9vJy8vDZDIxffp0RowYgcFg8Hd4Qgg3JEEIr6qrq3MNOi9atIjg4GAWL15MRESEnEYSYpCTBCEGXNfZzoGBgWRlZeFwOAgKCiIyMtLfIQohPCAJQgy4kpISPvnkkzNmO3tjZrUQwrskQYhz1tjYSH5+PpGRkWRnZ5OWlsbcuXNltrMQ5zm/JAil1A+B7wAK+LPW+jE3bRYCjwEG4JTWeoHvIhR96TrbOSAggDFjxgAQGBhIenq6nyMUQpwrnycIpdQk2pLDLMAKbFBKvaO1zu/UJhp4ElimtS5WSiX6Ok7RXUcZD6UUu3fvpqCg4IzZzt6oxSSE8B9/9CDGA59orZsBlFIfAtcBv+vU5j+B17TWxQBa6yqfRykAaG1tpaqqisrKSioqKliwYAERERFkZWWRkJAgs52FGML8kSAOAA8qpeKAFmA5sLtLmzGAQSm1FYgAfq+1/otPoxzm6uvr+eyzz6ipqQHAYDCQmJiI0+kEICEhgYSEBH+GKITwMp8nCK31YaXUw8BmoAnYB3StZhcEXAgsAkzATqXUJ1rro123p5RaDawGyMzM9GboQ5LWmsbGRioqKqisrCQlJYXs7GxXLaiJEyeSlJREbGysDDgLMcz4ZZBaa/0M8AyAUuo3QEmXJiW0DUybAbNSahswBeiWILTW64B1ADNmzOh/rethSmtNXl4e5eXltLS0ABAeHk5KSgoAISEhLFq0yJ8hCiH8zF9XMSVqrauUUpnAV4E5XZq8CfxRKRUEBAMXAf/r4zCHDIfDwalTp6isrMRqtTJjxgyUUlgsFuLi4khKSiIpKYnw8HB/hyqEGET8NQ/i1fYxCBuwRmtdq5S6DUBr/VT7aagNwBeAE3haa33AT7Get0pKSjhx4gTV1dU4HA6UUiQmJrrqIl188cX+DlEIMYj56xTTPDfLnury/BHgEZ8FdZ5raWmhqqqKiooKpk6dSkhICE1NTZjNZkaNGkVSUhIJCQlSGE8I4TGZSX0eM5vNHDt2jIqKCurr64G2GxplZ2cTEhLC2LFjGTdunJ+jFEKcryRBnCe01tTV1VFZWUlMTAxJSUk4HA7y8/OJj49n8uTJJCcnEx0d7aqSKtVShRDnQhLEIKa1pqioyHUJamtrKwDjxo0jKSmJiIgIrr32WrnJjhDCK+SbZRCx2+1UV1djsVgYOXIkSikOHz6M1WolKSmJ5ORkkpKSXCUtlFKSHIQQXiPfLn5WX19PWVkZFRUVnD59GqfTiclkIisrC6UUCxcudE1aE0IIX5IE4WNms5nKykpGjBhBYGAghYWFHDlyhOjoaHJyckhKSiI+Pt6VEKQAnhDCXyRBeJndbnddflpZWUljYyMAERERJCQkMGbMGMaOHYvRaPRzpEIIcSZJEAPM6XRSW1tLcHAwERER1NbWsmPHDgIDA0lISGDUqFEkJye7brspPQQhxGAlCWIANDU1UVlZSWVlJVVVVVitVsaMGcPUqVOJi4tjwYIFxMfHS1lsIcR5RRLEWbBarTQ3NxMdHY3Wmvfffx+r1UpoaChpaWmu2kYAAQEBrsdCCHE+kQThAafTSU1NjWscoaamhrCwMJYvX45SiosuuoiwsDAiIiLkaiMhxJAhCcINrTVms5mwsDCUUuTl5VFQUIBSipiYGMaNG0dycrKr6F1HiWwhhBhKJEG063xrzcrKSsxmM8uWLSMyMtI1sJyYmEhISIi/QxVCCJ+QBAFUVlaybds2tNYYDAbX5acdySAuLo64uDg/RymEEL4lCQKIiYlh/PjxJCcny601hRCinSQI2kpkT5o0yd9hCCHEoCI/lYUQQrglCUIIIYRbkiCEEEK4JQlCCCGEW5IghBBCuCUJQgghhFuSIIQQQrglCUIIIYRbSmvt7xgGjFKqGig6y5fHA6cGMJyBInH1j8TVPxJX/wzFuEZorRPcrRhSCeJcKKV2a61n+DuOriSu/pG4+kfi6p/hFpecYhJCCOGWJAghhBBuSYL4t3X+DqAHElf/SFz9I3H1z7CKS8YghBBCuCU9CCGEEG4NuwShlApUSn2ulHrbzTqllPqDUuqYUuoLpdT0QRLXQqVUvVJqb/t/9/kwrkKl1P72/e52s94vx8yDuPxyzJRS0Uqp9UqpL5VSh5VSc7qs99fx6isunx8vpdTYTvvbq5RqUErd0aWNz4+Xh3H56/P1I6XUQaXUAaXUP5RSxi7rB/Z4aa2H1X/Aj4G/A2+7WbcceA9QwGzg00ES10J3y30UVyEQ38t6vxwzD+LyyzEDXgBuaX8cDEQPkuPVV1x++4y17z8QqKDtmny/Hy8P4vL58QLSgALA1P78ZeCb3jxew6oHoZRKB64Anu6hyTXAX3SbT4BopVTKIIhrMPPLMRuMlFKRwHzgGQCttVVrXdelmc+Pl4dx+dsi4LjWuutEV39/vnqKy1+CAJNSKggIBcq6rB/Q4zWsEgTwGHAX4OxhfRpwstPzkvZl3vYYvccFMEcptU8p9Z5SaqIPYuqggU1KqTyl1Go36/11zPqKC3x/zEYB1cBz7acLn1ZKhXVp44/j5Ulc4L/PGMAK4B9ulvvr89Whp7jAx8dLa10KrAWKgXKgXmu9qUuzAT1ewyZBKKWuBKq01nm9NXOzzKuXeXkY1x7aurhTgMeBN7wZUxdf0VpPBy4H1iil5ndZ7/Nj1q6vuPxxzIKA6cCftNbTADNwd5c2/jhensTlt8+YUioYuBp4xd1qN8t8cullH3H5/HgppWJo6yGMBFKBMKXU17s2c/PSsz5ewyZBAF8BrlZKFQK5wKVKqRe7tCkBMjo9T6d7F87ncWmtG7TWTe2P3wUMSql4L8fVse+y9r9VwOvArC5N/HHM+ozLT8esBCjRWn/a/nw9bV/MXdv4+nj1GZc/P2O0Jfk9WutKN+v88vlq12Ncfjpei4ECrXW11toGvAbM7dJmQI/XsEkQWut7tNbpWuss2rqN/9Jad82+bwE3tV8JMJu2Lly5v+NSSiUrpVT741m0/X877c242vcVppSK6HgMXAYc6NLM58fMk7j8ccy01hXASaXU2PZFi4BDXZr54zPWZ1z++oy1W0nPp3F8frw8ictPx6sYmK2UCm3f9yLgcJc2A3q8gs4+1qFBKXUbgNb6KeBd2q4COAY0A98aJHH9B/BdpZQdaAFW6PZLFrwsCXi9/d9BEPB3rfWGQXDMPInLX8fs+8Df2k9PnAC+NQiOlydx+eV4KaVCgSXArZ2W+f14eRCXz4+X1vpTpdR62k5v2YHPgXXePF4yk1oIIYRbw+YUkxBCiP6RBCGEEMItSRBCCCHckgQhhBDCLUkQQggh3JIEIYQQwi1JEEKchfaJSPLvRwxp8gEXwkNKqSzVdi+FJ2mbrPSMaqvLv18pdWN7G6WUesTN8oVKqQ+VUi8rpY4qpX6rlPp/Sqld7e1Gt7e7of21+5RS2/z3boWQmdRC9NdY2manbgFuA6YA8cBn7V/oc4GpbpbTvmw8UEPbbOantdazlFI/pG2m8x3AfcBSrXWpUiraR+9JCLekByFE/xS119m/GPiH1trRXsztQ2BmL8sBPtNal2utW4HjQEep5v1AVvvjj4DnlVLfoe1mNUL4jSQIIfrH3P7XXVnl3pYDtHZ67Oz03El7b15rfRvwS9oqcu5VSsWdfahCnBtJEEKcnW3AjartXuIJtN2xbVcvyz2ilBqttf5Ua30fcIozSzcL4VMyBiHE2XkdmAPso+2GLHdprSuUUj0tH+fhdh9RSuXQ1hPZ0r4dIfxCqrkKIYRwS04xCSGEcEsShBBCCLckQQghhHBLEoQQQgi3JEEIIYRwSxKEEEIItyRBCCGEcEsShBBCCLf+P8+CCB9aQ1SYAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the predicted values against 'rooms'\n",
    "plt.plot(X['rooms'], lpr_CI['mean'], color = 'black',\n",
    "        linestyle = '-', label = '')\n",
    "plt.plot(X['rooms'], lpr_CI['mean_ci_upper'], color = 'lightgrey',\n",
    "        linestyle = \"--\", label = 'Upper CI')\n",
    "plt.plot(X['rooms'], lpr_CI['mean_ci_lower'], color = 'darkgrey',\n",
    "        linestyle = '--', label = 'lower CI')\n",
    "plt.ylabel('lprice')\n",
    "plt.xlabel('rooms')\n",
    "plt.legend()\n",
    "plt.savefig('images/Effect-Manual.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viral-geometry",
   "metadata": {},
   "source": [
    "### 3. Qualitative Regressors\n",
    "\n",
    "Many variables of interest are qualitative rather than quantitative. Examples include gender, race, labor market status, marital status, and brand choice. In this section, we discuss the use of qualitative variables as regressors. Wooldridge (2019, Section 7.5) also covers linear probability models wtih a binary dependent variable in a linear regression. Since this does not change the implementation, we will skip this topic here.\n",
    "\n",
    "Qualitative information can be represented as binary or dummy varaibles which can only take the value zero or one. In our examples, we see that dummy varaible can be used as regressors just as any other variable. An even more natural way to store *yes / no* type of information in *Python* is to use Boolean variables which can also be directly used as regressors.\n",
    "\n",
    "While qualitative varaibles with more than two outcomes can be represented by a set of dummy variables, the more natrual and convenient way to do this are categorical varaibles as covered in the previous notebook. There are some special cases in which we wish to break a numeric variable into categories is discussed in the later example. Finally, in the last section, we revisit interaction effect and show how these can be used with categorical varaibles to conveniently allow and test for difference in the regression equation.\n",
    "\n",
    "#### Linear Regression with Dummy as Regressors\n",
    "\n",
    "If qualitative data are stored as dummy variable (i.e. variables taking the value zero or one), these can easily be used as regressors in linear regression. If a single dummy varaible is used in a model, its coefficent represents the difference in the intercept between groups, see Wooldridge (2019, Section 7.2).\n",
    "\n",
    "A qualitative variable can also take $g > 2$ values. A variable **MobileOS** could for example take one of the $g = 4$ values \"Android\", \"iOS\", \"Windows\", \"other\". This information can be represented by $g - 1$ dummy variables, each take the value zero or one, where one category is left out to serve as reference category. They take the value one if the respective operating system is used and zero otherwise. Wooldridge (2019, Section 7.3) gives more information on these variables and their interpretation.\n",
    "\n",
    "Here, we are concerned with implmenting linear regressions with dummy variables as regressors. Everything works as before once we have generated the dummy variables. In the example, data sets provided with Wooldridge (2019), this has usually alreayd been done for us, so we don't have to learn anything new in terms of implmentation. We show two examples.\n",
    "\n",
    "#### Wooldridge, Example 7.1: Hourly Wage Equation\n",
    "\n",
    "We are interested in the wage differences by gender and regress the hourly wage on a dummy variable which is equal to one for females and zero for males. We also include regressors for education, experience, and tenure. The implementation with **statsmodels** is standard and the dummy variable **female** is used just as any other regressor. Its estimated coefficent of -1.81 indicates that on average, a woman maeks $1.81 per hour less than a man with the same education, experience, and tenure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monthly-magnitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dependencies\n",
    "import wooldridge as woo\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wooden-assessment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data set \"wage1\"\n",
    "wage1 = woo.dataWoo('wage1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bigger-prison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the regression model\n",
    "reg = smf.ols(formula = 'wage ~ female + educ + exper + tenure',\n",
    "             data = wage1)\n",
    "results = reg.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arbitrary-debut",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print regression table\n",
    "table = pd.DataFrame({'Betas': round(results.params, 4),\n",
    "                     'Standarde Errors': round(results.bse, 4),\n",
    "                     't Statistics': round(results.tvalues, 4),\n",
    "                     'p Value': round(results.pvalues, 4)})\n",
    "print(f'Regression Table: \\n{table}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painted-beast",
   "metadata": {},
   "source": [
    "#### Wooldridge, Example 7.6: Log Hourly Wage Equation\n",
    "\n",
    "We used log wage as dependent variable and distinguish gender and marital status using a qualitative variable with the four outcomes \"single female\", \"single male\", \"married female\", and \"married male\". We actually implement this regression using an interaction term between **married** and **female**. Relative to the reference group of single makes with the same education, experience, and tenure, married males make about 21.3% more (the coefficent of **married**), and single females make about 11.0% less (the coefficient of **female**). The coefficient of the interaction term implies that married females make around 30.1% - 21.3% = 8.7% less than single females, 30.1% - 21.3% = 41.1% less than married males, and 30% + 11.0% - 21.3% = 19.8% less than single males. Note once again that the approximate interpretation as percent may be inaccurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "therapeutic-panel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dependencies\n",
    "import wooldridge as woo\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brave-queue",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data set \"wage1\"\n",
    "wage1 = woo.dataWoo('wage1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animated-above",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the regression model\n",
    "reg = smf.ols(formula = 'np.log(wage) ~ married*female + educ + exper +'\n",
    "              'I(exper**2) + tenure + I(tenure**2)',\n",
    "             data = wage1)\n",
    "results = reg.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entire-feelings",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print regression table\n",
    "table = pd.DataFrame({'Betas': round(results.params, 4),\n",
    "                     'Standarde Errors': round(results.bse, 4),\n",
    "                     't Statistics': round(results.tvalues, 4),\n",
    "                     'p Value': round(results.pvalues, 4)})\n",
    "print(f'Regression Table: \\n{table}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exact-writing",
   "metadata": {},
   "source": [
    "#### Boolean Variables\n",
    "\n",
    "A natural way for storing qualitative *yes / no* informationin *Python* is to use **Boolean** variables. They can take the values **True** or **False** and can be transformed into a 0/1 dummy variable with the function **int()** where **True = 1** and **Flase = 0**. 0/1-coded dummies can *vice versa* be transformed into logical variables with the function **bool()**.\n",
    "\n",
    "Instead of transforming Boolean variable into dummies, they can be directly used as regressors. The coefficient is then named **varname[T.True]** indicating that **True** was treated as **1**. In the following example we repeat the analysis of Example 7.1 with the regressor **female** being coded as **bool** instead of a 0/1 dummy variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sophisticated-birthday",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dependencies\n",
    "import wooldridge as woo\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organized-recording",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data set \"wage1\"\n",
    "wage1 = woo.dataWoo('wage1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "danish-pressure",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Boolean variable \"isfemale\"\n",
    "wage1['isfemale'] = (wage1['female'] == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continent-america",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the regression model with boolean variable\n",
    "reg = smf.ols(formula = 'wage ~ isfemale + educ + exper + tenure',\n",
    "             data = wage1)\n",
    "results = reg.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selected-family",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print regression table\n",
    "table = pd.DataFrame({'Betas': round(results.params, 4),\n",
    "                     'Standarde Errors': round(results.bse, 4),\n",
    "                     't Statistics': round(results.tvalues, 4),\n",
    "                     'p Value': round(results.pvalues, 4)})\n",
    "print(f'Regression Table: \\n{table}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "middle-tender",
   "metadata": {},
   "source": [
    "In real-world data set, qualitative information is often not readily coded as logical or dummy variables, so we might want to create our own regressors. Suppose a qualitative variable saved as the **numpy** array **OS** take one of the three string values \"Android\", \"iOS\", \"Windows\", or \"other\". We can manually define the three relevant logical variables with \"Android\" as the reference category with\n",
    "\n",
    "``` Python\n",
    "iOS = os == 'iOS'\n",
    "wind = os == 'Windows'\n",
    "oth = os == 'other'\n",
    "```\n",
    "\n",
    "A more convenient and elegant way to deal with qualitative variables are categorical variables discussed in the next section.\n",
    "\n",
    "#### Categorical Variables\n",
    "\n",
    "We have introduced categorical variables of type **Categorical**. They take one of the given set of outcomes which can be labeled arbitrarily. This makes the natural type to store qualitative information.\n",
    "\n",
    "In a linear regression performed by **Statsmodels** we can easily transform any variable into a categorical variable using the function **C()** in the definition of the fomula. The function **ols()** is clever enough to implicitly add $g - 1$ dummy variables if the variable has *g* outcomes. As a reference category, the first category is left out by default.\n",
    "\n",
    "In the following example, we show how categorical varaibles are used. It uses the data set *CPS1982*. This data set is similar to the one used in Example 7.1 and 7.6 in that it contains wage and other data for 534 individuals. The frequency table for the two variables **gender** and **occupation** are shown in the output. The variable **gender** has two categories **male** and **female**. The variable **occupation** has size categories.\n",
    "\n",
    "In the output, the coefficidents are labeled with a combination of the variable and category name. As an example, the estimated coefficient of 0.224 for **C(gender)[T.male]** in **results** implies that men make about 22.4% more than women who are the same in terms of the other regressors. Employees in technical positions earn arround 1% (see coefficient of **C(occupation)[T.technical]**)less than otherwise equal management positions (who are the reference category).\n",
    "\n",
    "We can choose different reference categories using a second argument of the **C** command, where we provide a new reference group **somegroup** with the command **treatment('somegroup')**. In the specification **results_newref**, we choose **male** and **technical**. When we rerun the same regression command, we see the expected results: Variables like **education** and **experience** get the same coefficients. The dummy variable for females gets the negative of what the males got previously. Obviously, it is equivalent to say \"female log wages are lower by 0.224\" and \"male log wages are higher by 0.224\".\n",
    "\n",
    "The coefficients for the occupation are now relative to **technical**. From the first regression we already knew that technical positions make 1% less than managers, so it is not surprising that in the second regression we find that managers make 1% more than technical positions. The other occupation coefficents are higher by 0.010085 implying the same relative comparisons as in the first specification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "unique-teach",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dependencies\n",
    "import wooldridge as woo\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aggressive-replica",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data set \"CPS1985\" from local drive\n",
    "CPS1985 = pd.read_csv('data/CPS1985.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "suffering-forge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename variable to make outputs more compact\n",
    "CPS1985['cc'] = CPS1985['occupation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "necessary-polls",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency Table by Gender: \n",
      "col_0   count\n",
      "gender       \n",
      "female    245\n",
      "male      289\n",
      "\n",
      "Frequency Table by Occupation: \n",
      "col_0       count\n",
      "cc               \n",
      "management     55\n",
      "office         97\n",
      "sales          38\n",
      "services       83\n",
      "technical     105\n",
      "worker        156\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Table of categories and frequencies for two categorical variables\n",
    "freq_gender = pd.crosstab(CPS1985['gender'], columns = 'count')\n",
    "print(f'Frequency Table by Gender: \\n{freq_gender}\\n')\n",
    "\n",
    "freq_occupation = pd.crosstab(CPS1985['cc'], columns = 'count')\n",
    "print(f'Frequency Table by Occupation: \\n{freq_occupation}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "documented-foster",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directly using categorical variables in regression formula\n",
    "reg = smf.ols(formula = 'np.log(wage) ~ education + experience + C(gender) + C(cc)',\n",
    "             data = CPS1985)\n",
    "results = reg.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "unusual-zoning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression Table: \n",
      "                     Betas  Standarde Errors  t Statistics  p Value\n",
      "Intercept           0.9050            0.1717        5.2718   0.0000\n",
      "C(gender)[T.male]   0.2238            0.0423        5.2979   0.0000\n",
      "C(cc)[T.office]    -0.2073            0.0776       -2.6699   0.0078\n",
      "C(cc)[T.sales]     -0.3601            0.0936       -3.8455   0.0001\n",
      "C(cc)[T.services]  -0.3626            0.0818       -4.4305   0.0000\n",
      "C(cc)[T.technical] -0.0101            0.0740       -0.1363   0.8916\n",
      "C(cc)[T.worker]    -0.1525            0.0763       -1.9981   0.0462\n",
      "education           0.0759            0.0101        7.5449   0.0000\n",
      "experience          0.0119            0.0017        7.0895   0.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print regression table\n",
    "table = pd.DataFrame({'Betas': round(results.params, 4),\n",
    "                     'Standarde Errors': round(results.bse, 4),\n",
    "                     't Statistics': round(results.tvalues, 4),\n",
    "                     'p Value': round(results.pvalues, 4)})\n",
    "print(f'Regression Table: \\n{table}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "graphic-sigma",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rerun the regression with different reference category\n",
    "reg_newref = smf.ols(formula = 'np.log(wage) ~ education + experience +'\n",
    "                     'C(gender, Treatment(\"male\")) + C(cc, Treatment(\"technical\"))',\n",
    "             data = CPS1985)\n",
    "results_newref = reg_newref.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "posted-enforcement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression Table: \n",
      "                                              Betas  Standarde Errors  \\\n",
      "Intercept                                    1.1187            0.1765   \n",
      "C(gender, Treatment(\"male\"))[T.female]      -0.2238            0.0423   \n",
      "C(cc, Treatment(\"technical\"))[T.management]  0.0101            0.0740   \n",
      "C(cc, Treatment(\"technical\"))[T.office]     -0.1972            0.0678   \n",
      "C(cc, Treatment(\"technical\"))[T.sales]      -0.3500            0.0863   \n",
      "C(cc, Treatment(\"technical\"))[T.services]   -0.3525            0.0750   \n",
      "C(cc, Treatment(\"technical\"))[T.worker]     -0.1425            0.0705   \n",
      "education                                    0.0759            0.0101   \n",
      "experience                                   0.0119            0.0017   \n",
      "\n",
      "                                             t Statistics  p Value  \n",
      "Intercept                                          6.3393   0.0000  \n",
      "C(gender, Treatment(\"male\"))[T.female]            -5.2979   0.0000  \n",
      "C(cc, Treatment(\"technical\"))[T.management]        0.1363   0.8916  \n",
      "C(cc, Treatment(\"technical\"))[T.office]           -2.9082   0.0038  \n",
      "C(cc, Treatment(\"technical\"))[T.sales]            -4.0541   0.0001  \n",
      "C(cc, Treatment(\"technical\"))[T.services]         -4.7030   0.0000  \n",
      "C(cc, Treatment(\"technical\"))[T.worker]           -2.0218   0.0437  \n",
      "education                                          7.5449   0.0000  \n",
      "experience                                         7.0895   0.0000  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print regression table\n",
    "table_newref = pd.DataFrame({'Betas': round(results_newref.params, 4),\n",
    "                     'Standarde Errors': round(results_newref.bse, 4),\n",
    "                     't Statistics': round(results_newref.tvalues, 4),\n",
    "                     'p Value': round(results_newref.pvalues, 4)})\n",
    "print(f'Regression Table: \\n{table_newref}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worst-diagnosis",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chubby-battlefield",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painted-finland",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biological-perception",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collectible-turning",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cubic-ghana",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ancient-combine",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
