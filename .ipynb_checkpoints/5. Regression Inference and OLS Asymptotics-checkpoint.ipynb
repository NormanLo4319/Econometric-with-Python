{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "statutory-green",
   "metadata": {},
   "source": [
    "## Regression Inference and OLS Asymptotics\n",
    "\n",
    "In this notebook, we are going to study and demonstrate the use of *Python* to perform **statistical inference** to test our regression models. We are also going to explore the **Asymptotic Theory** and understand how it allows us to relax some assumptions needed to derive the sampling distribution of estimators if the sample is size is large enough.\n",
    "\n",
    "**Topics:**\n",
    "\n",
    "1. The *t* Test\n",
    "2. Confidence Intervals\n",
    "3. Linear Restrictions: *F* Tests\n",
    "4. Simulation Exercises\n",
    "5. LM Test\n",
    "\n",
    "Section 4.1 of Wooldridge (2019) adds assumption MLR.6 (normal distribution of the error term) to the previous assumptions MLR.1 through MLR.5. Together, these assumptions consitute the classical linear model (CLM).\n",
    "\n",
    "The main additional result we get from this assumption is stated in Theorem 4.1: The OLS parameter estimators are normally distributed (conditional on the regressors $x_1, x_2, ..., x_k$). The benefit of this result is that it allows us to do statistical inference similar to the approaches discussed the simple estimator of the mean of a normally distributed random variable.\n",
    "\n",
    "### 1. The *t* Test\n",
    "\n",
    "After the sign and magnitude of the estimated parameters, empirical resarch typically pays most attention to the results of *t* tests discussed in this section.\n",
    "\n",
    "#### General Setup\n",
    "\n",
    "An important type of hypotheses we are often interested in is of the form\n",
    "\n",
    "$$H_0: \\beta_j = a_j$$\n",
    "\n",
    "where $a_j$ is some given number, very often, $a_j$ = 0. For the most common cast of two-tailed tests, the alternative hypothesis is \n",
    "\n",
    "$$H_1: \\beta_j \\neq a_j$$\n",
    "\n",
    "and for one-tailed tests it is either one of\n",
    "\n",
    "$$H_1: \\beta_j > a_j$$\n",
    "\n",
    "or \n",
    "\n",
    "$$H_1: \\beta_j < a_j$$\n",
    "\n",
    "These hypotheses can be conveniently tested using a *t* test which is based on the test statistic\n",
    "\n",
    "$$t = \\frac{\\hat{\\beta}_j - a_j}{se(\\hat{\\beta}_j)}$$\n",
    "\n",
    "If $H_0$ is in fact true and the CLM assumptions holds, then this statistic has a t distribution with *n - k - 1* degree of freedom.\n",
    "\n",
    "#### Standard Case\n",
    "\n",
    "Very often, we want to test whether there is any relation at all between the dependent variable *y* and a regressor $x_j$ and do not want to impose a sign on the partial effect *a priori*. This is a mission for the standard two-sided *t* test with the hypothetical value $a_j$ = 0, so\n",
    "\n",
    "$$H_0: \\beta_j = 0$$\n",
    "\n",
    "$$H_1: \\beta_j \\neq 0$$\n",
    "\n",
    "$$t_{\\hat{\\beta}_j} = \\frac{\\hat{\\beta}_j}{se(\\hat{\\beta}_j)}$$\n",
    "\n",
    "The subscript on the *t* statistic indicates that this is **the** *t* value for $\\hat{\\beta}_j$ for this frequent version of the test. Under $H_0$, it has the *t* distribution with *n - k - 1* degree of freedom implying that the probability that $|t_{\\hat{\\beta}_j}| > c$ is equal to $\\alpha$ if *c* is the $1 - \\frac{\\alpha}{2}$ quantile of this distribution. If $\\alpha$ is our significance level (e.g. $\\alpha = 5\\%$), then we reject $H_0$ if $|t_{\\hat{\\beta}_j}| > c$ in our sample. For the typical significance level $\\alpha = 5\\%$, the critical value *c* will be around 2 for reasonable large degrees of freedom and approach the counterpart of 1.96 from the standard normal distribution in very large samples.\n",
    "\n",
    "The *p* value indicates the smallest value of the significance level $\\alpha$ for which we would still reject $H_0$ using our sample. So it is the probability for a random variable *T* with the respective *t* distribution that |*T*| > $|t_{\\hat{\\beta}_j}|$ where $t_{\\hat{\\beta}_j}$ is the value of the *t* statistic in our particular sample. In our two-tailed test, it can be calculated as\n",
    "\n",
    "$$p_{\\hat{\\beta}_j} = 2 \\cdot F_{t_{n-k-1}} \\cdot (-|t_{\\hat{\\beta}_j}|)$$\n",
    "\n",
    "where $F_{t_{n-k-1}} (\\cdot)$ is the CDF of the *t* distribution with *n - k - 1* degree of freedom. If our software provides us with the relevant *p* values, they are easy to use: We reject $H_0$ if $p_{\\hat{\\beta}_j} \\leq \\alpha$.\n",
    "\n",
    "Since this standard case of a *t* test is so common, **statsmodels** provides us with the relevant *t* and *p* values directly in the **summary** of the estimation results we already saw in the previous notebooks. The regression table includes for all regressors and the intercept:\n",
    "\n",
    "- parameter estimates and standard errors\n",
    "- test statistics $t_{\\hat{\\beta}_j}$ in column **t**\n",
    "- respective *p* values $p_{\\hat{\\beta}_j}$ in the colmun **P>|t|**\n",
    "- respective 95% confidence interval in columns **[0.025 and 0.975]**\n",
    "\n",
    "#### Wooldridge, Example 4.3: Determinants of College GPA\n",
    "\n",
    "We have repeatedly used the data set *GPA1* in the previous notebooks. This example uses three regressors and estimates a regression model of the form.\n",
    "\n",
    "$$colGPA = \\beta_0 + \\beta_1 \\cdot hsGPA + \\beta_2 \\cdot ACT + \\beta_3 \\cdot skipped + u$$\n",
    "\n",
    "For the critical values of the *t* test, using the normal approximation instead of the exact *t* distribution with *n - k - 1* = 137 d.f. doesn't make much of a difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ahead-mongolia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Modules\n",
    "import scipy.stats as stats\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "smart-condition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Critical Values by t Distribution: [1.97743121 2.61219198]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CV for alpha = 5% and 1% using the t distribution with 137 d.f.\n",
    "alpha = np.array([0.05, 0.01])\n",
    "cv_t = stats.t.ppf(1 - alpha / 2, 137)\n",
    "print(f'Critical Values by t Distribution: {cv_t}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "indoor-headquarters",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Critical Values by Normal Distribution: [1.95996398 2.5758293 ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CV for alpha = 5% and 1% using the normal approximation\n",
    "cv_n = stats.norm.ppf(1 - alpha / 2)\n",
    "print(f'Critical Values by Normal Distribution: {cv_n}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respected-sympathy",
   "metadata": {},
   "source": [
    "We presents the standard **summary** which directly contains all the information to test the hypotheses for all parameters. The *t* statistics for all coefficients excepts $\\beta_2$ are larger in absolute value than the *critical value* c = 2.61 (or c = 2.58 using the normal approximation) for $\\alpha$ = 1%. So we would reject $H_0$ for all usual significance levels. By construction, we draw the same conclusions from the *p* values.\n",
    "\n",
    "In order to confirm that **statsmodels** is exactly using the formulas of Wooldridge (2019). We next reconstruct the *t* and *p* values manually. We extract the coefficients (**params**) and standard errors (**bse**) from the regression results, and simply apply the $t_{\\hat{\\beta}_j}$ and $p_{\\hat{\\beta}_j}$ equations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exotic-bruce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Modules\n",
    "import scipy.stats as stats\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compressed-longitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data set gpa1\n",
    "gpa1 = woo.dataWoo('gpa1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flying-irish",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store and display results:\n",
    "reg = smf.ols(formula = 'colGPA ~ hsGPA + ACT + skipped', data = gpa1)\n",
    "results = reg.fit()\n",
    "print(f'Regression Summary: \\n{results.summary()}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neither-visitor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually confirm the formulas:\n",
    "\n",
    "# Extract coefficients and SE\n",
    "b = results.params\n",
    "se = results.bse\n",
    "\n",
    "# Reproduce t statistic\n",
    "tstat = b / se\n",
    "print(f't Statistics: \\n{tstatat}\\n')\n",
    "\n",
    "# Reproduce p value\n",
    "pval = 2 * stats.t.cdf(-abs(tstat), 137)\n",
    "print(f'P Value: \\n{pval}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binding-burden",
   "metadata": {},
   "source": [
    "#### Other Hypotheses\n",
    "\n",
    "For a one-tailed test, the critical value *c* of the *t* test and the *p* values have to be adjsuted appropriately Wooldridge (2019) provides a general discussion in Section 4.2. For testing the null hypothesis $H_0: \\beta_j = a_j$, the tests for the three common alternative hypotheses are summarized in the following table.\n",
    "\n",
    "**One- and Two-tailed *t* Tests for $H_0: \\beta_j = a_j$**\n",
    "\n",
    "|  $H_1$:  |  $\\beta_j \\neq a_j$  |  $\\beta_j > a_j$  |  $\\beta_j < a_j$  |\n",
    "|  :---:  |  :---:  |  :---:  |  :---:  |\n",
    "|  *c* = quantile  |  $1 - \\frac{\\alpha}{2}$  |  $1 - \\alpha$  |  $1 - \\alpha$  |\n",
    "|  Reject $H_0$ if  |  $|t_{\\hat{\\beta}_j}| > c$  |  $\\hat{\\beta}_j > c$  |  $\\hat{\\beta}_j > c$  |\n",
    "|  *p* value  |  $2 \\cdot F_{t_{n - k - 1}} \\cdot (-|t_{\\hat{\\beta}_j}|)$  |  $F_{t_{n - k - 1}} \\cdot (-t_{\\hat{\\beta}_j})$  |  $F_{t_{n - k - 1}} \\cdot (-t_{\\hat{\\beta}_j})$  |\n",
    "\n",
    "Given the stardard regerssion output including the *p* value for two-sided tests $p_{\\hat{\\beta}_j}$, we can easily do one-sided *t* tests for the null hypothesis $H_0: \\beta_j = 0$ in two steps:\n",
    "\n",
    "* Is $\\hat{\\beta}_j$ positive (if $H_1: \\beta_j > 0$) or negative (if $H_1: \\beta_j < 0$)?\n",
    "- No -> Do not reject $H_0$ since this cannot be evidence against $H_0$.\n",
    "- Yes -> The relevent *p* value is half of the reported $p_{\\hat{\\beta}_j}$.\n",
    "- Reject $H_0$ if $p = \\frac{1}{2} p_{\\hat{\\beta}_j} < \\alpha$.\n",
    "\n",
    "#### Wooldridge, Example 4.1: Hourly Wage Equation\n",
    "\n",
    "We have already estimated the wage equation\n",
    "\n",
    "$$log(wage) = \\beta_0 + \\beta_1 \\cdot educ + \\beta_2 \\cdot exper + \\beta_3 \\cdot tenure + u$$\n",
    "\n",
    "Now we are ready to test $H_0: \\beta_2 > 0$. For the critical values of the *t* test, using the normal approximation instead of the exact *t* distribution with *n - k - 1* = 522 d.f. doesn't make any relevant difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranking-associate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Modules\n",
    "import scipy.stats as stats\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "previous-reasoning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV for alpha = 5% and 1% using the t distribution with 522 d.f.\n",
    "alpha = np.array([0.05, 0.01])\n",
    "cv_t = stats.t.ppf(1 - alpha, 522)\n",
    "print(f'Critical Values by t Distribution: {cv_t}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dependent-minimum",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV for alpha = 5% and 1% using the normal approximation\n",
    "cv_n = stats.norm.ppf(1 - alpha)\n",
    "print(f'Critical Values by Normal Distribution: {cv_n}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "auburn-covering",
   "metadata": {},
   "source": [
    "In this example, we show the standard regression output. The reported *t* statistic for the parameter of *exper* is $t_{\\hat{\\beta}_2}$ = 2.391 which is larger than the critical value *c* = 2.33 for the significance level $\\alpha$ = 1%, so we reject $H_0$. By construction, we get the same answer from looking at the *p* value. Like always, the reported $p_{\\hat{\\beta}_j}$ value is for a two-sided test, so we have to divide it by 2. The resulting value $p = \\frac{0.017}{2} = 0.0085 < 0.01$, so we reject $H-0$ using an $\\alpha$ = 1% significance level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finite-governor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Modules\n",
    "import wooldridge as woo\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raised-kidney",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data set 'wage1'\n",
    "wage1 = woo.dataWoo('wage1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "single-england",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the regression model and print the model summary\n",
    "reg = smf.ols(formula = 'np.log(wage) ~ educ + exper + tenure', data = wage1)\n",
    "results = reg.fit()\n",
    "print(f'Regression Summary Output: \\n{results}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convenient-wagner",
   "metadata": {},
   "source": [
    "### 2. Confidence Intervals\n",
    "\n",
    "We have already looked at confidence intervals (CI) for the mean of a normally distributed random variabel in the previous notebook. CI for the regression parameters are equally easy to construct and closely related to *t* test. Wooldridge (2019, Section 4.3) provides a succinct discussion. The 95% confidence interval for parameter $\\beta_j$ is simply.\n",
    "\n",
    "$$\\hat{\\beta}_j \\pm c \\cdot se(\\hat{\\beta}_j)$$\n",
    "\n",
    "where *c* is the same critical value for the two-sided *t* test using a significance level $\\alpha$ = 5%. Wooldridge (2019) shows examples of how to manually construct these CI.\n",
    "\n",
    "**statsmodels** provides the 95% confdience intervals for all parameters in the regression table. If you use the method **conf_int()** on the object with the regression results, you can compute other significance levels. Below example demonstrates the procedure.\n",
    "\n",
    "#### Wooldridge, Example 4.8: Model of R&D Expenditures\n",
    "\n",
    "We study the relationship between the R&D expenditures of a firm, its size, and the profit margin for a sample of 32 firms in the chemical industry. The regression equation is\n",
    "\n",
    "$$log(rd) = \\beta_0 +\\beta_1 \\cdot log(sales) + \\beta_2 \\cdot profmarg + u$$\n",
    "\n",
    "Here, we present the regression results as well as the 95% and 99% CI. See Wooldridge (2019) for the manual calculation of the CI and comments on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governing-stamp",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Modules\n",
    "import wooldridge as woo\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modern-excerpt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data set 'rdchem'\n",
    "rdchem = woo.dataWoo('rdchem')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "individual-saint",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLS regression\n",
    "reg = smf.ols(formula = 'np.log(rd) ~ np.log(sales) + profmarg', data = rdchem)\n",
    "results = reg.fit()\n",
    "print(f'Regression Summary Output: \\n{results}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "objective-performance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 95% and 99% Confidence Interval:\n",
    "ci95 = results.conf_int(0.05)\n",
    "ci99 = results.conf_int(0.01)\n",
    "\n",
    "print(f'95% Confidence Interval: {ci95}\\n')\n",
    "print(f'99% Confidence Interval: {ci99}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saving-height",
   "metadata": {},
   "source": [
    "### 3. Linear Restrictions: *F* Tests\n",
    "\n",
    "Wooldridge (2019, Sections 4.4 and 4.5) discusses more general tests than those for the null hypotheses for individual estimated parameter. They can involve one or more hypotheses involving one or more population parameters in a linear fashion.\n",
    "\n",
    "We follow the illustrative example of Wooldridge (2019, Section 4.5) and analyze major league baseball players' salaries using the data set *MLB1* and the regression model\n",
    "\n",
    "$$log(salary) = \\beta_0 + \\beta_1 \\cdot years + \\beta_2 \\cdot gamesyr + \\beta_3 \\cdot bavg + \\beta_4 \\cdot hrunsyr + \\beta_5 \\cdot rbisyr + u$$\n",
    "\n",
    "We want to test whether the performance measures batting average (*bavg*), home runs per year (*hrunsyr*), and runs batted in per year (*rbisyr*) have an impact on the salary once we control for the number of years as an active player (*years*) and the number of games played per year (*gamesyr*). So we state our null hypothesis as $H_0: \\beta_3 = \\beta_4 = \\beta_5 = 0$ versus $H_1: H_0$ is false, i.e. at least one of the performance measures matters.\n",
    "\n",
    "The test statistic of the *F* test is based on the relative difference between the sum of squared residuals in the general (unrestricted) model and a restricted model in which the hypotheses are imposed $SSR_{ur}$ and $SSR_{r}$, respectively. In our example, the restricted model is one in which *bavg*, *hrunsyr*, and *rbisyr* are excluded as regressors. If both models involve the same dependent variable, it can also be written in terms of the coefficient of determination in the unrestricted and the restricted model $R_{ur}^2$ and $R_{r}^2$, respectively:\n",
    "\n",
    "$$F = \\frac{SSR_{r} - SSR_{ur}}{SSR_{ur}} \\cdot \\frac{n - k - 1}{q} = \\frac{R_{ur}^2 - R_{r}^2}{R_{ur}^2} \\cdot \\frac{n - k - 1}{q}$$\n",
    "\n",
    "where *q* is the number of restrictions (in our example, *q* = 3). Intuitively, if the null hypothesis is correct, then imposing it as a restriction will not lead to a significant drop in the model fit and the *F* test statistic should be relatively small. It can be shown that under the CLM assumptions and the null hypothesis, the statistic has an *F* distribution with the numerator degrees of freedom equal to *q* if *F* > *c*, where critical value *c* is the 1 - $\\alpha$ quantile of the relevant $F_{q, n-k-1}$ distribution. In our example, *n* = 353, *k* = 5, *q* = 3. So with $\\alpha$ = 1%, the critical value is 3.84 and can be calculated using the **f.ppf()** function in **scipy.stats** as\n",
    "\n",
    "``` Python\n",
    "f.ppf(1 - 0.01, 3, 347)\n",
    "```\n",
    "\n",
    "Here, we show the calculation for this example. The result is *F* = 9.55 > 3.84, so we clearly reject $H_0$. We also calculate the *p* value for this test. It is $p = 4.47 \\cdot 10^{-06} = 0.00000447$, so we reject $H_0$ for any reasonable significance level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distant-leadership",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Modules\n",
    "import wooldridge as woo\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "irish-combining",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data set \"mlb1\"\n",
    "mlb1 = woo.dataWoo('mlb1')\n",
    "n = mlb1.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integral-lottery",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unrestricted OLS Regression\n",
    "reg_ur = smf.ols(\n",
    "    formula = 'np.log(salary) ~ years + gamesyr + bavg + hrunsyr + rbisyr',\n",
    "    data = mlb1)\n",
    "fit_ur = reg_ur.fit()\n",
    "r2_ur = fit_ur.rsquared\n",
    "print(f'R Square for Unrestricted Model: {r2_ur}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "declared-compound",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restricted OLS Regression\n",
    "reg_r = smf.ols(\n",
    "    formula = 'np.log(salary) ~ years + gamesyr',\n",
    "    data = mlb1)\n",
    "fit_r = reg_r.fit()\n",
    "r2_r = fit_r.rsquared\n",
    "print(f'R Square for Restricted Model: {r2_r}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "static-launch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F statistic\n",
    "fstat = (r2_ur - r2_r) / (1 - r2_ur) * (n - 6) / 3\n",
    "print(f'F Statistic: {fstat}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logical-article",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Critical Value for alpha = 1%\n",
    "cv = stats.f.ppf(1 - 0.01, 3, 347)\n",
    "print(f'Critical Value at 1% Significant Level: {cv}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brave-inflation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p value = 1 - cdf of the appropriate F distribution\n",
    "fpval = 1 - stats.f.cdf(fstat, 3, 347)\n",
    "print(f'p-value for the F Test: {fpval}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handled-manufacturer",
   "metadata": {},
   "source": [
    "It should not be surprising that there is more convenient way to do this. The module **statsmodels** provides a command **f_test()** which is well suited for these kinds of tests. Given the object with regression results, for example **results**, an *F* test is conducted with\n",
    "\n",
    "``` Python\n",
    "hypotheses = ['var_name1 = 0', 'var_name2 = 0']\n",
    "ftest = results.f_test(hypotheses)\n",
    "```\n",
    "\n",
    "where **hypotheses** collects null hypothesis to be tested. It is a list of length *q* where each restriction is described as a text in which the variable name takes the place of its parameter. In our example, $H_0$ is that the three parameters of *bavg*, *hrunsyr*, and *rbisyr* are all equal to zero, which translates as **hypotheses = ['bavg = 0', 'hrunsyr = 0', 'rbisyr = 0']**. We implement this for the same test as the manual calculations done in the previous example and results in exactly the same *F* statistic and *p* value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expected-dispatch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Modules\n",
    "import wooldridge as woo\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sonic-thunder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data set \"mlb1\"\n",
    "mlb1 = woo.dataWoo('mlb1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tender-invasion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLS Regression\n",
    "reg = smf.ols(\n",
    "    formula = 'np.log(salary) ~ years + gamesyr + bavg + hrunsyr + rbisyr',\n",
    "    data = mlb1)\n",
    "results = reg.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verbal-management",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automate F test\n",
    "hypotheses = ['bavg = 0', 'hrunsyr = 0', 'rbisyr = 0']\n",
    "ftest = results.f_test(hypotheses)\n",
    "fstat = ftest.statistic[0][0]\n",
    "fpval = ftestt.pvalue\n",
    "\n",
    "print(f'F Statistic: {fstat}\\n')\n",
    "print(f'p value for the F Test: {fpval}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educational-windows",
   "metadata": {},
   "source": [
    "This function can also be used to test more complicated null hypotheses. For example, suppose a sport reporter claims that the batting average plays no role and that the number of home runs has twice the impact as the number of runs batted in. This translates (using variable names instead of numbers as subscripts) as $H_0: \\beta_{bavg} = 0, \\beta_{hrunsyr} = 2 \\cdot \\beta_{rbisyr}$. For *Python* we translate it as **hypotheses = ['bavg = 0', 'hrunsyr = 2 * rbisyr']**. The output shows the results of this test. The *p* value is 0.6, so we cannot reject $H_0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endless-wells",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automate F test\n",
    "hypotheses = ['bavg = 0', 'hrunsyr = 2 * rbisyr']\n",
    "ftest = results.f_test(hypotheses)\n",
    "fstat = ftest.statistic[0][0]\n",
    "fpval = ftestt.pvalue\n",
    "\n",
    "print(f'F Statistic: {fstat}\\n')\n",
    "print(f'p value for the F Test: {fpval}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "posted-magnet",
   "metadata": {},
   "source": [
    "Both the most important and the most straightforward *F* test is the one for **overall significance**. The null hypothesis is that all parameters except for the constant are equal to zero. If this null hypothesis holds, the regressors do not have any joint explanatory power for *y*. The result of such a test are automatically included in the upper part of the **summary** output as **F-statistic** (F statistic) and **Prob(F-statistic)** (*p* value).\n",
    "\n",
    "******\n",
    "\n",
    "Asymptotic theory allows us to relax some assumptions needed to derive the sampling distribution of estimators if the sample size is large enough. For running a regression in a software package, it does not matter whether we rely on stronger assumptions or on asymptotic arguments. So we don't have to learn anything new regarding the implementation. \n",
    "\n",
    "Instead, we aim to imporve our intuition regarding the working of asymptotics by looking at some simulation exercises briefly discusses the implementation of the regression-based Lagrange multiplier (LM) test presented by Wooldridge (2019, Section 5.2).\n",
    "\n",
    "### 4. Simulation Exercises\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "union-isolation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greek-history",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "further-delta",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instrumental-surname",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "duplicate-bankruptcy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instrumental-album",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "covered-serial",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "gorgeous-certification",
   "metadata": {},
   "source": [
    "### 5. LM Test"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
