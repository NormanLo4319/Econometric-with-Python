{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "statutory-green",
   "metadata": {},
   "source": [
    "## Regression Inference and OLS Asymptotics\n",
    "\n",
    "In this notebook, we are going to study and demonstrate the use of *Python* to perform **statistical inference** to test our regression models. We are also going to explore the **Asymptotic Theory** and understand how it allows us to relax some assumptions needed to derive the sampling distribution of estimators if the sample is size is large enough.\n",
    "\n",
    "**Topics:**\n",
    "\n",
    "1. The *t* Test\n",
    "2. Confidence Intervals\n",
    "3. Linear Restrictions: *F* Tests\n",
    "4. Simulation Exercises\n",
    "5. LM Test\n",
    "\n",
    "Section 4.1 of Wooldridge (2019) adds assumption MLR.6 (normal distribution of the error term) to the previous assumptions MLR.1 through MLR.5. Together, these assumptions consitute the classical linear model (CLM).\n",
    "\n",
    "The main additional result we get from this assumption is stated in Theorem 4.1: The OLS parameter estimators are normally distributed (conditional on the regressors $x_1, x_2, ..., x_k$). The benefit of this result is that it allows us to do statistical inference similar to the approaches discussed the simple estimator of the mean of a normally distributed random variable.\n",
    "\n",
    "### 1. The *t* Test\n",
    "\n",
    "After the sign and magnitude of the estimated parameters, empirical resarch typically pays most attention to the results of *t* tests discussed in this section.\n",
    "\n",
    "#### General Setup\n",
    "\n",
    "An important type of hypotheses we are often interested in is of the form\n",
    "\n",
    "$$H_0: \\beta_j = a_j$$\n",
    "\n",
    "where $a_j$ is some given number, very often, $a_j$ = 0. For the most common cast of two-tailed tests, the alternative hypothesis is \n",
    "\n",
    "$$H_1: \\beta_j \\neq a_j$$\n",
    "\n",
    "and for one-tailed tests it is either one of\n",
    "\n",
    "$$H_1: \\beta_j > a_j$$\n",
    "\n",
    "or \n",
    "\n",
    "$$H_1: \\beta_j < a_j$$\n",
    "\n",
    "These hypotheses can be conveniently tested using a *t* test which is based on the test statistic\n",
    "\n",
    "$$t = \\frac{\\hat{\\beta}_j - a_j}{se(\\hat{\\beta}_j)}$$\n",
    "\n",
    "If $H_0$ is in fact true and the CLM assumptions holds, then this statistic has a t distribution with *n - k - 1* degree of freedom.\n",
    "\n",
    "#### Standard Case\n",
    "\n",
    "Very often, we want to test whether there is any relation at all between the dependent variable *y* and a regressor $x_j$ and do not want to impose a sign on the partial effect *a priori*. This is a mission for the standard two-sided *t* test with the hypothetical value $a_j$ = 0, so\n",
    "\n",
    "$$H_0: \\beta_j = 0$$\n",
    "\n",
    "$$H_1: \\beta_j \\neq 0$$\n",
    "\n",
    "$$t_{\\hat{\\beta}_j} = \\frac{\\hat{\\beta}_j}{se(\\hat{\\beta}_j)}$$\n",
    "\n",
    "The subscript on the *t* statistic indicates that this is **the** *t* value for $\\hat{\\beta}_j$ for this frequent version of the test. Under $H_0$, it has the *t* distribution with *n - k - 1* degree of freedom implying that the probability that $|t_{\\hat{\\beta}_j}| > c$ is equal to $\\alpha$ if *c* is the $1 - \\frac{\\alpha}{2}$ quantile of this distribution. If $\\alpha$ is our significance level (e.g. $\\alpha = 5\\%$), then we reject $H_0$ if $|t_{\\hat{\\beta}_j}| > c$ in our sample. For the typical significance level $\\alpha = 5\\%$, the critical value *c* will be around 2 for reasonable large degrees of freedom and approach the counterpart of 1.96 from the standard normal distribution in very large samples.\n",
    "\n",
    "The *p* value indicates the smallest value of the significance level $\\alpha$ for which we would still reject $H_0$ using our sample. So it is the probability for a random variable *T* with the respective *t* distribution that |*T*| > $|t_{\\hat{\\beta}_j}|$ where $t_{\\hat{\\beta}_j}$ is the value of the *t* statistic in our particular sample. In our two-tailed test, it can be calculated as\n",
    "\n",
    "$$p_{\\hat{\\beta}_j} = 2 \\cdot F_{t_{n-k-1}} \\cdot (-|t_{\\hat{\\beta}_j}|)$$\n",
    "\n",
    "where $F_{t_{n-k-1}} (\\cdot)$ is the CDF of the *t* distribution with *n - k - 1* degree of freedom. If our software provides us with the relevant *p* values, they are easy to use: We reject $H_0$ if $p_{\\hat{\\beta}_j} \\leq \\alpha$.\n",
    "\n",
    "Since this standard case of a *t* test is so common, **statsmodels** provides us with the relevant *t* and *p* values directly in the **summary** of the estimation results we already saw in the previous notebooks. The regression table includes for all regressors and the intercept:\n",
    "\n",
    "- parameter estimates and standard errors\n",
    "- test statistics $t_{\\hat{\\beta}_j}$ in column **t**\n",
    "- respective *p* values $p_{\\hat{\\beta}_j}$ in the colmun **P>|t|**\n",
    "- respective 95% confidence interval in columns **[0.025 and 0.975]**\n",
    "\n",
    "#### Wooldridge, Example 4.3: Determinants of College GPA\n",
    "\n",
    "We have repeatedly used the data set *GPA1* in the previous notebooks. This example uses three regressors and estimates a regression model of the form.\n",
    "\n",
    "$$colGPA = \\beta_0 + \\beta_1 \\cdot hsGPA + \\beta_2 \\cdot ACT + \\beta_3 \\cdot skipped + u$$\n",
    "\n",
    "For the critical values of the *t* test, using the normal approximation instead of the exact *t* distribution with *n - k - 1* = 137 d.f. doesn't make much of a difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ahead-mongolia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Modules\n",
    "import scipy.stats as stats\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "smart-condition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Critical Values by t Distribution: [1.97743121 2.61219198]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CV for alpha = 5% and 1% using the t distribution with 137 d.f.\n",
    "alpha = np.array([0.05, 0.01])\n",
    "cv_t = stats.t.ppf(1 - alpha / 2, 137)\n",
    "print(f'Critical Values by t Distribution: {cv_t}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "indoor-headquarters",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Critical Values by Normal Distribution: [1.95996398 2.5758293 ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CV for alpha = 5% and 1% using the normal approximation\n",
    "cv_n = stats.norm.ppf(1 - alpha / 2)\n",
    "print(f'Critical Values by Normal Distribution: {cv_n}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respected-sympathy",
   "metadata": {},
   "source": [
    "We presents the standard **summary** which directly contains all the information to test the hypotheses for all parameters. The *t* statistics for all coefficients excepts $\\beta_2$ are larger in absolute value than the *critical value* c = 2.61 (or c = 2.58 using the normal approximation) for $\\alpha$ = 1%. So we would reject $H_0$ for all usual significance levels. By construction, we draw the same conclusions from the *p* values.\n",
    "\n",
    "In order to confirm that **statsmodels** is exactly using the formulas of Wooldridge (2019). We next reconstruct the *t* and *p* values manually. We extract the coefficients (**params**) and standard errors (**bse**) from the regression results, and simply apply the $t_{\\hat{\\beta}_j}$ and $p_{\\hat{\\beta}_j}$ equations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "exotic-bruce",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wooldridged'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-94c69015c970>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Import Modules\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mwooldridged\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mwoo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstats\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'wooldridged'"
     ]
    }
   ],
   "source": [
    "# Import Modules\n",
    "import wooldridge as woo\n",
    "import scipy.stats as stats\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compressed-longitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data set gpa1\n",
    "gpa1 = woo.dataWoo('gpa1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flying-irish",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store and display results:\n",
    "reg = smf.ols(formula = 'colGPA ~ hsGPA + ACT + skipped', data = gpa1)\n",
    "results = reg.fit()\n",
    "print(f'Regression Summary: \\n{results.summary()}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neither-visitor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually confirm the formulas:\n",
    "\n",
    "# Extract coefficients and SE\n",
    "b = results.params\n",
    "se = results.bse\n",
    "\n",
    "# Reproduce t statistic\n",
    "tstat = b / se\n",
    "print(f't Statistics: \\n{tstatat}\\n')\n",
    "\n",
    "# Reproduce p value\n",
    "pval = 2 * stats.t.cdf(-abs(tstat), 137)\n",
    "print(f'P Value: \\n{pval}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binding-burden",
   "metadata": {},
   "source": [
    "#### Other Hypotheses\n",
    "\n",
    "For a one-tailed test, the critical value *c* of the *t* test and the *p* values have to be adjsuted appropriately Wooldridge (2019) provides a general discussion in Section 4.2. For testing the null hypothesis $H_0: \\beta_j = a_j$, the tests for the three common alternative hypotheses are summarized in the following table.\n",
    "\n",
    "**One- and Two-tailed *t* Tests for $H_0: \\beta_j = a_j$**\n",
    "\n",
    "|  $H_1$:  |  $\\beta_j \\neq a_j$  |  $\\beta_j > a_j$  |  $\\beta_j < a_j$  |\n",
    "|  :---:  |  :---:  |  :---:  |  :---:  |\n",
    "|  *c* = quantile  |  $1 - \\frac{\\alpha}{2}$  |  $1 - \\alpha$  |  $1 - \\alpha$  |\n",
    "|  Reject $H_0$ if  |  $|t_{\\hat{\\beta}_j}| > c$  |  $\\hat{\\beta}_j > c$  |  $\\hat{\\beta}_j > c$  |\n",
    "|  *p* value  |  $2 \\cdot F_{t_{n - k - 1}} \\cdot (-|t_{\\hat{\\beta}_j}|)$  |  $F_{t_{n - k - 1}} \\cdot (-t_{\\hat{\\beta}_j})$  |  $F_{t_{n - k - 1}} \\cdot (-t_{\\hat{\\beta}_j})$  |\n",
    "\n",
    "Given the stardard regerssion output including the *p* value for two-sided tests $p_{\\hat{\\beta}_j}$, we can easily do one-sided *t* tests for the null hypothesis $H_0: \\beta_j = 0$ in two steps:\n",
    "\n",
    "* Is $\\hat{\\beta}_j$ positive (if $H_1: \\beta_j > 0$) or negative (if $H_1: \\beta_j < 0$)?\n",
    "- No -> Do not reject $H_0$ since this cannot be evidence against $H_0$.\n",
    "- Yes -> The relevent *p* value is half of the reported $p_{\\hat{\\beta}_j}$.\n",
    "- Reject $H_0$ if $p = \\frac{1}{2} p_{\\hat{\\beta}_j} < \\alpha$.\n",
    "\n",
    "#### Wooldridge, Example 4.1: Hourly Wage Equation\n",
    "\n",
    "We have already estimated the wage equation\n",
    "\n",
    "$$log(wage) = \\beta_0 + \\beta_1 \\cdot educ + \\beta_2 \\cdot exper + \\beta_3 \\cdot tenure + u$$\n",
    "\n",
    "Now we are ready to test $H_0: \\beta_2 > 0$. For the critical values of the *t* test, using the normal approximation instead of the exact *t* distribution with *n - k - 1* = 522 d.f. doesn't make any relevant difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranking-associate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Modules\n",
    "import scipy.stats as stats\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "previous-reasoning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV for alpha = 5% and 1% using the t distribution with 522 d.f.\n",
    "alpha = np.array([0.05, 0.01])\n",
    "cv_t = stats.t.ppf(1 - alpha, 522)\n",
    "print(f'Critical Values by t Distribution: {cv_t}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dependent-minimum",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV for alpha = 5% and 1% using the normal approximation\n",
    "cv_n = stats.norm.ppf(1 - alpha)\n",
    "print(f'Critical Values by Normal Distribution: {cv_n}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "auburn-covering",
   "metadata": {},
   "source": [
    "In this example, we show the standard regression output. The reported *t* statistic for the parameter of *exper* is $t_{\\hat{\\beta}_2}$ = 2.391 which is larger than the critical value *c* = 2.33 for the significance level $\\alpha$ = 1%, so we reject $H_0$. By construction, we get the same answer from looking at the *p* value. Like always, the reported $p_{\\hat{\\beta}_j}$ value is for a two-sided test, so we have to divide it by 2. The resulting value $p = \\frac{0.017}{2} = 0.0085 < 0.01$, so we reject $H-0$ using an $\\alpha$ = 1% significance level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finite-governor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Modules\n",
    "import wooldridge as woo\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raised-kidney",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data set 'wage1'\n",
    "wage1 = woo.dataWoo('wage1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "single-england",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the regression model and print the model summary\n",
    "reg = smf.ols(formula = 'np.log(wage) ~ educ + exper + tenure', data = wage1)\n",
    "results = reg.fit()\n",
    "print(f'Regression Summary Output: \\n{results}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convenient-wagner",
   "metadata": {},
   "source": [
    "### 2. Confidence Intervals\n",
    "\n",
    "We have already looked at confidence intervals (CI) for the mean of a normally distributed random variabel in the previous notebook. CI for the regression parameters are equally easy to construct and closely related to *t* test. Wooldridge (2019, Section 4.3) provides a succinct discussion. The 95% confidence interval for parameter $\\beta_j$ is simply.\n",
    "\n",
    "$$\\hat{\\beta}_j \\pm c \\cdot se(\\hat{\\beta}_j)$$\n",
    "\n",
    "where *c* is the same critical value for the two-sided *t* test using a significance level $\\alpha$ = 5%. Wooldridge (2019) shows examples of how to manually construct these CI.\n",
    "\n",
    "**statsmodels** provides the 95% confdience intervals for all parameters in the regression table. If you use the method **conf_int()** on the object with the regression results, you can compute other significance levels. Below example demonstrates the procedure.\n",
    "\n",
    "#### Wooldridge, Example 4.8: Model of R&D Expenditures\n",
    "\n",
    "We study the relationship between the R&D expenditures of a firm, its size, and the profit margin for a sample of 32 firms in the chemical industry. The regression equation is\n",
    "\n",
    "$$log(rd) = \\beta_0 +\\beta_1 \\cdot log(sales) + \\beta_2 \\cdot profmarg + u$$\n",
    "\n",
    "Here, we present the regression results as well as the 95% and 99% CI. See Wooldridge (2019) for the manual calculation of the CI and comments on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governing-stamp",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Modules\n",
    "import wooldridge as woo\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "modern-excerpt",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'woo' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-54cdca9dea81>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Import data set 'rdchem'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mrdchem\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwoo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataWoo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'rdchem'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'woo' is not defined"
     ]
    }
   ],
   "source": [
    "# Import data set 'rdchem'\n",
    "rdchem = woo.dataWoo('rdchem')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "individual-saint",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLS regression\n",
    "reg = smf.ols(formula = 'np.log(rd) ~ np.log(sales) + profmarg', data = rdchem)\n",
    "results = reg.fit()\n",
    "print(f'Regression Summary Output: \\n{results}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "objective-performance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 95% and 99% Confidence Interval:\n",
    "ci95 = results.conf_int(0.05)\n",
    "ci99 = results.conf_int(0.01)\n",
    "\n",
    "print(f'95% Confidence Interval: {ci95}\\n')\n",
    "print(f'99% Confidence Interval: {ci99}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saving-height",
   "metadata": {},
   "source": [
    "### 3. Linear Restrictions: *F* Tests\n",
    "\n",
    "Wooldridge (2019, Sections 4.4 and 4.5) discusses more general tests than those for the null hypotheses for individual estimated parameter. They can involve one or more hypotheses involving one or more population parameters in a linear fashion.\n",
    "\n",
    "We follow the illustrative example of Wooldridge (2019, Section 4.5) and analyze major league baseball players' salaries using the data set *MLB1* and the regression model\n",
    "\n",
    "$$log(salary) = \\beta_0 + \\beta_1 \\cdot years + \\beta_2 \\cdot gamesyr + \\beta_3 \\cdot bavg + \\beta_4 \\cdot hrunsyr + \\beta_5 \\cdot rbisyr + u$$\n",
    "\n",
    "We want to test whether the performance measures batting average (*bavg*), home runs per year (*hrunsyr*), and runs batted in per year (*rbisyr*) have an impact on the salary once we control for the number of years as an active player (*years*) and the number of games played per year (*gamesyr*). So we state our null hypothesis as $H_0: \\beta_3 = \\beta_4 = \\beta_5 = 0$ versus $H_1: H_0$ is false, i.e. at least one of the performance measures matters.\n",
    "\n",
    "The test statistic of the *F* test is based on the relative difference between the sum of squared residuals in the general (unrestricted) model and a restricted model in which the hypotheses are imposed $SSR_{ur}$ and $SSR_{r}$, respectively. In our example, the restricted model is one in which *bavg*, *hrunsyr*, and *rbisyr* are excluded as regressors. If both models involve the same dependent variable, it can also be written in terms of the coefficient of determination in the unrestricted and the restricted model $R_{ur}^2$ and $R_{r}^2$, respectively:\n",
    "\n",
    "$$F = \\frac{SSR_{r} - SSR_{ur}}{SSR_{ur}} \\cdot \\frac{n - k - 1}{q} = \\frac{R_{ur}^2 - R_{r}^2}{R_{ur}^2} \\cdot \\frac{n - k - 1}{q}$$\n",
    "\n",
    "where *q* is the number of restrictions (in our example, *q* = 3). Intuitively, if the null hypothesis is correct, then imposing it as a restriction will not lead to a significant drop in the model fit and the *F* test statistic should be relatively small. It can be shown that under the CLM assumptions and the null hypothesis, the statistic has an *F* distribution with the numerator degrees of freedom equal to *q* if *F* > *c*, where critical value *c* is the 1 - $\\alpha$ quantile of the relevant $F_{q, n-k-1}$ distribution. In our example, *n* = 353, *k* = 5, *q* = 3. So with $\\alpha$ = 1%, the critical value is 3.84 and can be calculated using the **f.ppf()** function in **scipy.stats** as\n",
    "\n",
    "``` Python\n",
    "f.ppf(1 - 0.01, 3, 347)\n",
    "```\n",
    "\n",
    "Here, we show the calculation for this example. The result is *F* = 9.55 > 3.84, so we clearly reject $H_0$. We also calculate the *p* value for this test. It is $p = 4.47 \\cdot 10^{-06} = 0.00000447$, so we reject $H_0$ for any reasonable significance level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "distant-leadership",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Modules\n",
    "import wooldridge as woo\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "irish-combining",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data set \"mlb1\"\n",
    "mlb1 = woo.dataWoo('mlb1')\n",
    "n = mlb1.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "integral-lottery",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R Square for Unrestricted Model: 0.6278028485187443\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Unrestricted OLS Regression\n",
    "reg_ur = smf.ols(\n",
    "    formula = 'np.log(salary) ~ years + gamesyr + bavg + hrunsyr + rbisyr',\n",
    "    data = mlb1)\n",
    "fit_ur = reg_ur.fit()\n",
    "r2_ur = fit_ur.rsquared\n",
    "print(f'R Square for Unrestricted Model: {r2_ur}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "declared-compound",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R Square for Restricted Model: 0.5970716339066895\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Restricted OLS Regression\n",
    "reg_r = smf.ols(\n",
    "    formula = 'np.log(salary) ~ years + gamesyr',\n",
    "    data = mlb1)\n",
    "fit_r = reg_r.fit()\n",
    "r2_r = fit_r.rsquared\n",
    "print(f'R Square for Restricted Model: {r2_r}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "static-launch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F Statistic: 9.55025352195195\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# F statistic\n",
    "fstat = (r2_ur - r2_r) / (1 - r2_ur) * (n - 6) / 3\n",
    "print(f'F Statistic: {fstat}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "logical-article",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Critical Value at 1% Significant Level: 3.838520048496057\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Critical Value for alpha = 1%\n",
    "cv = stats.f.ppf(1 - 0.01, 3, 347)\n",
    "print(f'Critical Value at 1% Significant Level: {cv}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "brave-inflation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value for the F Test: 4.473708139829391e-06\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# p value = 1 - cdf of the appropriate F distribution\n",
    "fpval = 1 - stats.f.cdf(fstat, 3, 347)\n",
    "print(f'p-value for the F Test: {fpval}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handled-manufacturer",
   "metadata": {},
   "source": [
    "It should not be surprising that there is more convenient way to do this. The module **statsmodels** provides a command **f_test()** which is well suited for these kinds of tests. Given the object with regression results, for example **results**, an *F* test is conducted with\n",
    "\n",
    "``` Python\n",
    "hypotheses = ['var_name1 = 0', 'var_name2 = 0']\n",
    "ftest = results.f_test(hypotheses)\n",
    "```\n",
    "\n",
    "where **hypotheses** collects null hypothesis to be tested. It is a list of length *q* where each restriction is described as a text in which the variable name takes the place of its parameter. In our example, $H_0$ is that the three parameters of *bavg*, *hrunsyr*, and *rbisyr* are all equal to zero, which translates as **hypotheses = ['bavg = 0', 'hrunsyr = 0', 'rbisyr = 0']**. We implement this for the same test as the manual calculations done in the previous example and results in exactly the same *F* statistic and *p* value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "expected-dispatch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Modules\n",
    "import wooldridge as woo\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "sonic-thunder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data set \"mlb1\"\n",
    "mlb1 = woo.dataWoo('mlb1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "tender-invasion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLS Regression\n",
    "reg = smf.ols(\n",
    "    formula = 'np.log(salary) ~ years + gamesyr + bavg + hrunsyr + rbisyr',\n",
    "    data = mlb1)\n",
    "results = reg.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "verbal-management",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F Statistic: 9.55025352195186\n",
      "\n",
      "p value for the F Test: 4.4737081398390565e-06\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Automate F test\n",
    "hypotheses = ['bavg = 0', 'hrunsyr = 0', 'rbisyr = 0']\n",
    "ftest = results.f_test(hypotheses)\n",
    "fstat = ftest.statistic[0][0]\n",
    "fpval = ftest.pvalue\n",
    "\n",
    "print(f'F Statistic: {fstat}\\n')\n",
    "print(f'p value for the F Test: {fpval}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educational-windows",
   "metadata": {},
   "source": [
    "This function can also be used to test more complicated null hypotheses. For example, suppose a sport reporter claims that the batting average plays no role and that the number of home runs has twice the impact as the number of runs batted in. This translates (using variable names instead of numbers as subscripts) as $H_0: \\beta_{bavg} = 0, \\beta_{hrunsyr} = 2 \\cdot \\beta_{rbisyr}$. For *Python* we translate it as **hypotheses = ['bavg = 0', 'hrunsyr = 2 * rbisyr']**. The output shows the results of this test. The *p* value is 0.6, so we cannot reject $H_0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "endless-wells",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F Statistic: 0.5117822576247739\n",
      "\n",
      "p value for the F Test: 0.5998780329146338\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Automate F test\n",
    "hypotheses = ['bavg = 0', 'hrunsyr = 2 * rbisyr']\n",
    "ftest = results.f_test(hypotheses)\n",
    "fstat = ftest.statistic[0][0]\n",
    "fpval = ftest.pvalue\n",
    "\n",
    "print(f'F Statistic: {fstat}\\n')\n",
    "print(f'p value for the F Test: {fpval}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "posted-magnet",
   "metadata": {},
   "source": [
    "Both the most important and the most straightforward *F* test is the one for **overall significance**. The null hypothesis is that all parameters except for the constant are equal to zero. If this null hypothesis holds, the regressors do not have any joint explanatory power for *y*. The result of such a test are automatically included in the upper part of the **summary** output as **F-statistic** (F statistic) and **Prob(F-statistic)** (*p* value).\n",
    "\n",
    "******\n",
    "\n",
    "Asymptotic theory allows us to relax some assumptions needed to derive the sampling distribution of estimators if the sample size is large enough. For running a regression in a software package, it does not matter whether we rely on stronger assumptions or on asymptotic arguments. So we don't have to learn anything new regarding the implementation. \n",
    "\n",
    "Instead, we aim to imporve our intuition regarding the working of asymptotics by looking at some simulation exercises briefly discusses the implementation of the regression-based Lagrange multiplier (LM) test presented by Wooldridge (2019, Section 5.2).\n",
    "\n",
    "### 4. Simulation Exercises\n",
    "\n",
    "In the previous notebook, we already used Monte Carlo Simulation methods to study the mean and variance of OLS estimators under the assumptions SLR.1 -SLR.5. Here, we will conduct similar experiments but will look at the whole sampling distribution of OLS estimators. Remember that the sampling distribution is important since confidence intervals, *t* and *F* tests and other tools of inference rely on it. \n",
    "\n",
    "Theorem 4.1 of Wooldridge (2019) gives the normal distribution of the OLS estimators (conditional on the regressors) based on assumptions MLR.1 through MLR.6. In contrast, Theorem 5.2 states that *asymptotically*, the distribution is normal by assumptions MLR.1 through MLR.5 only. Assumption MRL.6 - the normal distribution of the error terms - is not required if the sample is large enough to justify asymptotic argument.\n",
    "\n",
    "In other words: In small samples, the parameter estimates have a normal sampling distriubtion only if\n",
    "\n",
    "- the error terms are normally distributed and\n",
    "- we condition on the regressors\n",
    "\n",
    "To see how this works out in practice, we set up a series of simulation experiments. The first case simulates a model consistent with MLR.1 through MLR.6 and keeps the regressors fixed. Theory suggests that the sampling distribution of $\\hat{\\beta}$ is normal, independent of the sample size. The second case simulates a violation of assumption MLR.6. Normality of $\\hat{\\beta}$ only holds asymptotically, so for small sample size we suspect a violation. Finally, we will look closer into what \"conditional on the regressors\" means and simulate a (very plausible) violation of this in the last case.\n",
    "\n",
    "#### Normally Distributed Error Terms (Case 1)\n",
    "\n",
    "Here, we draws 10,000 samples of a given size (which has to be stored in variable *n* before) from a population that is consistent with assumption MLR.1 through MLR.6. The error terms are specified to be standard normal. The slope estimate $\\hat{\\beta}_1$ is stored for each of the generated samples in the array **b1**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "union-isolation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "greek-history",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed\n",
    "np.random.seed(835742)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "further-delta",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set sample size and number of simulations\n",
    "n = 100\n",
    "r = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "instrumental-surname",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set true parameters\n",
    "beta0 = 1\n",
    "beta1 = 0.5\n",
    "sx = 1\n",
    "ex = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "duplicate-bankruptcy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize b1 to store results later\n",
    "b1 = np.empty(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "american-civilization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw a sample of x, fixed over replications\n",
    "x = stats.norm.rvs(ex, sx, size = n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "blond-recovery",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat the similation r times\n",
    "for i in range(r):\n",
    "    # draw a saimple of u (std. normal)\n",
    "    u = stats.norm.rvs(0, 1, size = n)\n",
    "    y = beta0 + beta1 * x + u\n",
    "    df = pd.DataFrame({\"y\": y, \"x\": x})\n",
    "    \n",
    "    # estimate conditional OLS\n",
    "    reg = smf.ols(formula = 'y ~ x', data = df)\n",
    "    results = reg.fit()\n",
    "    b1[i] = results.params['x']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chubby-former",
   "metadata": {},
   "source": [
    "The code was run for different sample sizes. The density estimate together with the corresponding normal density are shown in Figure 5.1. Not surprisingly, all distributions look very similar to the normal distribution - that is what Theorem 4.1 predicted. Note that the fact that the sampling variance decreases as *n* rises is only obvious if we pay attention to the different scales of the axes.\n",
    "\n",
    "**Figure 5.1:** Density of $\\hat{\\beta}_1$ with Different Sample Sizes: Normal Error Terms\n",
    "\n",
    "|  |  |\n",
    "|  :---:  |  :---:  |\n",
    "|  n = 5  |  n = 10  |\n",
    "|  ![alt](images/MCSim-olsasy-norm-n5.png)  |  ![alt](images/MCSim-olsasy-norm-n10.png)  |\n",
    "|  n = 100  |  n = 1000  |\n",
    "|  ![alt](images/MCSim-olsasy-norm-n100.png)  |  ![alt](images/MCSim-olsasy-norm-n1000.png)  |\n",
    "\n",
    "#### Non-Normal Error Terms (Case 2)\n",
    "\n",
    "The next step is to simulate a violation of assumtpion MLR.6. In order to implement a rather drastic violation of the normality assumtpion, we implement a \"standardized\" $\\chi^2$ distribution with one degree of freedom. More specifically, let *v* be distributed as $\\chi_{[1]}^2$. Because this distribution has a mean of 1 and a variance of 2, the error term $u = \\frac{v - 1}{\\sqrt{2}}$ has a mean of 0 and a variance of 1. This simplifies the comparison to the exercise with the standard normal errors above. Figure 5.2 plots the density functions fo the standard normal distribution used above and the \"standardized\" $\\chi^2$ distribution. Both have a mean of 0 and a variance of 1 but very different shapes. The only line of code we changed compared to the previous is the sampling of *u* where we replace drawing from a standard normal distribution using **u = stats.norm.rvs(0, 1, size = n)** with sampling from the standardized $\\chi_{[1]}^2$ distribution with \n",
    "\n",
    "``` Python\n",
    "u = (stats.chi2.rvs(1, size = n) - 1) / np.sqrt(2)\n",
    "```\n",
    "\n",
    "**Figure 5.2:** Density Functions of the Simulated Error Terms\n",
    "![alt](images/MCSim-olsasy-stdchisq.png)\n",
    "\n",
    "For each of the same sample sizes used above, we again estimate the slope parameter for 10,000 samples. The densities of $\\hat{\\beta}_1$ are plotted in Figure 5.3 together with the respective normal distributions with the corresponding variances. For the small sample sizes, the deviation from the normal distribution is strong. Note that the dashed normal distributions have the same mean and variance. The main difference is the kurtosis which is larger than 8 in the simulations for n = 5 compared to the normal distribution for which the kurtosis is equal to 3.\n",
    "\n",
    "For larger sample size, the sampling distribution of $\\hat{\\beta}_1$ coverges to the normal distribution. For *n* = 100, the difference is much smaller but still discernible. For *n* = 1,000, it cannot be detected anymore in our simulation exerice. How large the sample needs to be depends among other things on the severity of the violations of MLR.6. If the distribution of the error terms is not as extremely non-normal as in our simulation, smaller sample sizes like the rule of thumb *n* = 30 might suffice for valid asymptotics.\n",
    "\n",
    "**Figure 5.3:** Density of $\\hat{\\beta}_1$ with Different Sample Sizes: Non-Normal Error Terms\n",
    "\n",
    "|  |  |\n",
    "|  :---:  |  :---:  |\n",
    "|  n = 5  |  n = 10  |\n",
    "|  ![alt](images/MCSim-olsasy-chisq-n5.png)  |  ![alt](images/MCSim-olsasy-chisq-n10.png)  |\n",
    "|  n = 100  |  n = 1000  |\n",
    "|  ![alt](images/MCSim-olsasy-chisq-n100.png)  |  ![alt](images/MCSim-olsasy-chisq-n1000.png)  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "threatened-deficit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "preliminary-optimization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed\n",
    "np.random.seed(835742)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "chief-daisy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set sample size and number of simulations\n",
    "n = 100\n",
    "r = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "opposed-radar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set true parameters\n",
    "beta0 = 1\n",
    "beta1 = 0.5\n",
    "sx = 1\n",
    "ex = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "computational-solid",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize b1 to store results later\n",
    "b1 = np.empty(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "russian-spiritual",
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw a sample of x, fixed over replications\n",
    "x = stats.norm.rvs(ex, sx, size = n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "radical-preserve",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat the similation r times\n",
    "for i in range(r):\n",
    "    # draw a saimple of u (std. normal)\n",
    "    u = (stats.chi2.rvs(1, size = n) - 1) / np.sqrt(2)\n",
    "    y = beta0 + beta1 * x + u\n",
    "    df = pd.DataFrame({\"y\": y, \"x\": x})\n",
    "    \n",
    "    # estimate conditional OLS\n",
    "    reg = smf.ols(formula = 'y ~ x', data = df)\n",
    "    results = reg.fit()\n",
    "    b1[i] = results.params['x']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loaded-filter",
   "metadata": {},
   "source": [
    "#### Unconditioning on the Regressors (Case 3)\n",
    "\n",
    "There is more subtle difference between the finite-sample results regarding the variance (Theorem 3.2) and distribution (Theorem 4.1) on one hand and the corresponding asymptotic results (Theorem 5.2). The former results describe the sampling distribution \"conditional on the sample values of the independent variables\". This implies that as we draw different samples, the values of the regressors $x_1, x_2, x_3, ... x_k$ remain the same and only the error terms and dependent varialbes change.\n",
    "\n",
    "In our previous simulation exerices, this is implemented by making random draws of *x* outside of the simulation loop. This is a realistic description of how data is generated only in some simple experiments: The experimenter chooses the regressors for sample, conducts the experiement and measures the dependent variable.\n",
    "\n",
    "In most applications we are concerned with, this is an unrealistic description of how we obtain our data. If we draw a sample of individuals, both their dependent and independent variables differ across samples. In these cases, the distribution \"conditional on the sample values of the independent variables\" can only serve as an approximation of the actual distribution with varying regressors. For large samples, this distinction is irrelevant and the asymptotic distribution is the same. \n",
    "\n",
    "Let's see how this plays out in an example. The code in this example differs from case 1 only by moving the generation of the regressors into the loop in which the 10,000 samples are generated. This is inconsistent with Theorem 4.1, so for small samples, we don't know the distribution of $\\hat{\\beta}_1$. Theorem 5.2 is applicable, so for (very) large sample, we know that the estimator is normally distributed.\n",
    "\n",
    "Figure 5.4 shows the distribution of the 10,000 estimates generated for *n* = 5, 10, 100, and 1,000. As we expected from theory, the distribution is (close to) normal for large samples. For small samples, it deviates quite a bit. The kurtosis is 8.7 for a sample size of *n* = 5 which is far away from the kurtosis of 3 of a normal distribution.\n",
    "\n",
    "**Figure 5.4:** Density of $\\hat{\\beta}_1$ with Different Sample Size: Varying Regressors\n",
    "\n",
    "|  |  |\n",
    "|  :---:  |  :---:  |\n",
    "|  n = 5  |  n = 10  |\n",
    "|  ![alt](images/MCSim-olsasy-uncond-n5.png)  |  ![alt](images/MCSim-olsasy-uncond-n10.png)  |\n",
    "|  n = 100  |  n = 1000  |\n",
    "|  ![alt](images/MCSim-olsasy-uncond-n100.png)  |  ![alt](images/MCSim-olsasy-uncond-n1000.png)  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "effective-mississippi",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "complicated-borough",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed\n",
    "np.random.seed(835742)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "beginning-technician",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set sample size and number of simulations\n",
    "n = 100\n",
    "r = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "instrumental-album",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set true parameters\n",
    "beta0 = 1\n",
    "beta1 = 0.5\n",
    "sx = 1\n",
    "ex = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "covered-serial",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize b1 to store results later\n",
    "b1 = np.empty(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "necessary-directive",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat the similation r times\n",
    "for i in range(r):\n",
    "    # draw a sample of x, varying over replications\n",
    "    x = stats.norm.rvs(ex, sx, size = n)\n",
    "    \n",
    "    # draw a saimple of u (std. normal)\n",
    "    u = stats.norm.rvs(0, 1, size = n)\n",
    "    y = beta0 + beta1 * x + u\n",
    "    df = pd.DataFrame({\"y\": y, \"x\": x})\n",
    "    \n",
    "    # estimate conditional OLS\n",
    "    reg = smf.ols(formula = 'y ~ x', data = df)\n",
    "    results = reg.fit()\n",
    "    b1[i] = results.params['x']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gorgeous-certification",
   "metadata": {},
   "source": [
    "### 5. LM Test\n",
    "\n",
    "As an alternative to the *F* tests discussed previously, *LM* tests for the same sort of hypotheses can be very useful with large samples. In the linear regression setup, the test statistic is \n",
    "\n",
    "$$LM = n \\cdot R_\\bar{u}^2$$\n",
    "\n",
    "where *n* is the sample size and $R_\\bar{u}^2$ is the usual $R^2$ statistic in a regression of the residual $\\bar{u}$ from the restricted model on the unrestricted set of regressors. Under the null hypothesis, it is asymptotically distributed as $\\chi_{q}^2$ with *q* denoting number of restrictions. Details are given in Wooldridge (2019, Section 5.2).\n",
    "\n",
    "The implementation in **statsmodels** is straightforward if we remember that the residuals can be obtained with the **resid** attribute.\n",
    "\n",
    "#### Wooldridge, Example 5.3: Economic Model of Crime\n",
    "\n",
    "We analyze the same data on the number of arrests as in the crime example. The unrestricted regression model is\n",
    "\n",
    "$$narr86 = \\beta_0 + \\beta_1pcnv + \\beta_2avgsen + \\beta_3tottime + \\beta_4ptime86 + \\beta_5qemp86 + u$$\n",
    "\n",
    "The dependent variable narr86 reflects the number of times a man was arrested and is explained by the proportion of prior arrests (*pcnv*), previous average sentences (*avgsen*), the time spend in prison before 1986 (*tottime*), the number of months in prison in 1986 (*ptime86*), and the number of quarters unemployed in 1986 (*qemp86*).\n",
    "\n",
    "The joint null hypothesis is \n",
    "\n",
    "$$H_0: \\beta_2 = \\beta_3 = 0$$\n",
    "\n",
    "so the restricted set of regressors excludes *avgsen* and *tottime*. In this example, we show an implmentation of this *LM* test. The restricted model is estimated and its residuals **utilde =** $\\tilde{u}$ are calculated. They are regressed on the unrestricted set of regressors. The $R^2$ from this regression is 0.001494, so the *LM* test statistic is calculated to be around *LM* = 0.001494 $\\cdot$ 2725 = 4.071. This is smaller than the critical value for a significance level of $\\alpha$ = 10%, so we do not reject the null hypothesis. We can also easily calculate the *p* value using the $\\chi^2$ CDF **chi2.cdf()**. It turns out to be 0.1306. \n",
    "\n",
    "The same hypothesis can be tested using the F test using the command **f_test()**. In this example, it delivers the same *p* value up to three digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "resistant-tuning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Modules\n",
    "import wooldridge as woo\n",
    "import statsmodels.formula.api as smf\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "lyric-secondary",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the crime1 data set\n",
    "crime1 = woo.dataWoo('crime1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "million-canal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 for Restricted Model: 0.04132330770123094\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Estimate restricted model\n",
    "reg_r = smf.ols(formula = 'narr86 ~ pcnv + ptime86 + qemp86', data = crime1)\n",
    "fit_r = reg_r.fit()\n",
    "r2_r = fit_r.rsquared\n",
    "print(f'R^2 for Restricted Model: {r2_r}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "integral-prediction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 for Residual from Restricted Model: 0.0014938456737877415\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. Regression of residuals from restricted model\n",
    "crime1['utilde'] = fit_r.resid\n",
    "reg_LM = smf.ols(formula = 'utilde ~ pcnv + ptime86 + qemp86 + avgsen + tottime', data = crime1)\n",
    "fit_LM = reg_LM.fit()\n",
    "r2_LM = fit_LM.rsquared\n",
    "print(f'R^2 for Residual from Restricted Model: {r2_LM}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "hawaiian-surname",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LM Test Statistic: 4.070729461071595\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. calculation of LM test statistic\n",
    "LM = r2_LM * fit_LM.nobs\n",
    "print(f'LM Test Statistic: {LM}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "polyphonic-disaster",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Critical Value at 10% Significance Level: 4.605170185988092\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#4 Critical Value from chi-squared distribution, alpha = 10%\n",
    "cv = stats.chi2.ppf(1 - 0.10, 2)\n",
    "print(f'Critical Value at 10% Significance Level: {cv}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "missing-dancing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p value for the Test: 0.13063282803267184\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5. p value (alternative to critical value)\n",
    "pval = 1 - stats.chi2.cdf(LM, 2)\n",
    "print(f'p value for the Test: {pval}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "numerous-power",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F Statistic: 2.033921558435096\n",
      "\n",
      "p value: 0.13102048172760739\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6. Compare to F-test\n",
    "reg = smf.ols(formula = ' narr86 ~ pcnv + ptime86 + qemp86 + avgsen + tottime', data = crime1)\n",
    "results = reg.fit()\n",
    "hypotheses = ['avgsen = 0', 'tottime = 0']\n",
    "ftest = results.f_test(hypotheses)\n",
    "fstat = ftest.statistic[0][0]\n",
    "fpval = ftest.pvalue\n",
    "print(f'F Statistic: {fstat}\\n')\n",
    "print(f'p value: {fpval}\\n')"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
